{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width  species\n",
      "0           5.1          3.5           1.4          0.2        0\n",
      "1           4.9          3.0           1.4          0.2        0\n",
      "2           4.7          3.2           1.3          0.2        0\n",
      "3           4.6          3.1           1.5          0.2        0\n",
      "4           5.0          3.6           1.4          0.2        0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset=pd.read_csv('iris.csv')\n",
    "print(dataset.head())\n",
    "dataset=dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset[:,0:4]\n",
    "target=dataset[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: six~=1.15.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py~=3.1.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard~=2.5 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-nightly~=2.5.0.dev in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: grpcio~=1.34.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.4.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel~=0.35 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.32.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (57.0.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<5,>=3.0.2 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: keras in c:\\users\\udara vimukthi\\appdata\\roaming\\python\\python38\\site-packages (2.4.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from keras) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: h5py in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from keras) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#categorical encoding\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "target_new=to_categorical(target)\n",
    "print(target_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 17,091\n",
      "Trainable params: 17,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model=Sequential()\n",
    "#an empty NN created\n",
    "\n",
    "model.add(Dense(64,input_dim=4,activation='relu')) #first hidden layer\n",
    "model.add(Dense(128,activation='relu'))  #2nd hidden layer\n",
    "model.add(Dense(64,activation='relu'))  #3rd hidden layer\n",
    "model.add(Dense(3,activation='softmax')) #output layer\n",
    "\n",
    "#because of classification problem, we have to use categorical crossentropy \n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense [array([[ 0.11913955, -0.25299722,  0.17426616,  0.06412461, -0.2450199 ,\n",
      "        -0.26783967,  0.15985906, -0.03783625, -0.00048569,  0.09727213,\n",
      "        -0.2391701 , -0.29252478,  0.04894233,  0.01775962,  0.14531925,\n",
      "         0.07147086, -0.24950413, -0.2918427 ,  0.26530862,  0.23143297,\n",
      "        -0.07005361,  0.19751984, -0.03415045, -0.17956987,  0.12350333,\n",
      "         0.08795506, -0.09980454,  0.0742164 ,  0.1931434 ,  0.05933762,\n",
      "        -0.28632945,  0.00911048,  0.1484946 ,  0.08713228, -0.02348086,\n",
      "         0.02339724,  0.00489032, -0.07643463,  0.27650332,  0.13137513,\n",
      "        -0.2430217 ,  0.11317304, -0.23327944, -0.08744006,  0.00533727,\n",
      "         0.19399437,  0.10393381, -0.02606237,  0.10141996, -0.25074407,\n",
      "         0.20675349, -0.01093742, -0.09391125, -0.10850948,  0.10098979,\n",
      "         0.19106737,  0.13904092,  0.07902744,  0.02412817, -0.18918926,\n",
      "         0.27935964,  0.0645293 , -0.07751197,  0.02132004],\n",
      "       [ 0.21519232, -0.07832895,  0.155438  , -0.139638  ,  0.01569292,\n",
      "         0.02117488,  0.05476812,  0.10892412,  0.08881399,  0.11123508,\n",
      "         0.02015653,  0.13626206,  0.16588762,  0.2814517 , -0.25901404,\n",
      "        -0.10386448, -0.06268319, -0.2922671 , -0.03988612,  0.27787602,\n",
      "        -0.1967105 , -0.05236892,  0.28793192, -0.2807608 ,  0.10473672,\n",
      "         0.13455889,  0.03039455, -0.25991014,  0.2833215 ,  0.15840575,\n",
      "        -0.06211635, -0.20924771,  0.26928306, -0.20791161,  0.01103517,\n",
      "         0.15678218,  0.11814925,  0.15485564, -0.01551169, -0.05794777,\n",
      "        -0.17606735, -0.25256637,  0.11219451, -0.24439546,  0.03433034,\n",
      "        -0.2626507 , -0.2910485 , -0.06784731, -0.07905239,  0.16112116,\n",
      "         0.03850353, -0.02431962,  0.20860273, -0.02217153,  0.07549518,\n",
      "         0.25717437, -0.20432374, -0.04555157,  0.05423093,  0.1269593 ,\n",
      "        -0.03120375, -0.21764812, -0.00381634,  0.2809115 ],\n",
      "       [-0.10727777, -0.22376557,  0.24907547, -0.02275106, -0.2796826 ,\n",
      "         0.15183839, -0.1242242 ,  0.17384464, -0.13549103, -0.10206726,\n",
      "        -0.06373595,  0.03794584, -0.21702738, -0.26360896, -0.24635161,\n",
      "        -0.19165078,  0.26732945,  0.24823278,  0.28992862, -0.26078093,\n",
      "        -0.19842622,  0.24313551,  0.15413561,  0.02158612,  0.02342337,\n",
      "         0.0578202 , -0.09549692,  0.13249749,  0.08738872,  0.18948138,\n",
      "         0.19619367,  0.17773795, -0.24931236,  0.07246405,  0.01095939,\n",
      "         0.11816299, -0.16324425,  0.15714097,  0.13882774, -0.28010562,\n",
      "        -0.01276198,  0.09395224, -0.17777732, -0.00404876, -0.03084689,\n",
      "        -0.25106964, -0.11079904,  0.25225735, -0.25410366,  0.11551982,\n",
      "         0.11452147, -0.24369775, -0.0261322 ,  0.15610662,  0.2029705 ,\n",
      "        -0.00607318,  0.28949016,  0.05926162, -0.14533165, -0.12191148,\n",
      "         0.04904118,  0.16724682, -0.26576325, -0.151625  ],\n",
      "       [-0.22803444, -0.28176814,  0.17486638, -0.21639445, -0.27593598,\n",
      "        -0.1757738 , -0.09868003, -0.20190975, -0.2631972 , -0.18967253,\n",
      "         0.14582145, -0.04303217, -0.02443582, -0.03994751,  0.23053461,\n",
      "         0.12537241,  0.20510185, -0.2248283 ,  0.10482234,  0.03258702,\n",
      "        -0.01290655, -0.07935366, -0.03176785, -0.13883172, -0.25303173,\n",
      "        -0.18328819,  0.2601229 , -0.23121834, -0.1048855 ,  0.13059142,\n",
      "         0.28523195,  0.18663219, -0.29110616, -0.27504802, -0.1915665 ,\n",
      "         0.2419256 , -0.0573902 , -0.14988452, -0.14022392,  0.14553592,\n",
      "         0.12211189, -0.06545831, -0.05552973,  0.12817162,  0.28881317,\n",
      "        -0.20080438, -0.09794442,  0.239514  ,  0.1898394 ,  0.2896133 ,\n",
      "        -0.14579158,  0.06928214, -0.05931588, -0.14575198,  0.05494645,\n",
      "         0.2633919 ,  0.15670165, -0.12172423,  0.15861121, -0.14347827,\n",
      "         0.19353291, -0.14199975, -0.20362636,  0.09884936]],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "dense_1 [array([[-0.0254886 , -0.01596831,  0.04521877, ...,  0.12423794,\n",
      "        -0.04081033, -0.14618959],\n",
      "       [-0.11336546,  0.09616761, -0.08687169, ...,  0.17452838,\n",
      "         0.13274343,  0.10744156],\n",
      "       [-0.04634611, -0.09606392,  0.00358273, ..., -0.03995243,\n",
      "         0.01929677, -0.10786419],\n",
      "       ...,\n",
      "       [ 0.04102798,  0.1066177 ,  0.04762438, ...,  0.14123045,\n",
      "         0.03055044,  0.1318071 ],\n",
      "       [-0.10535296, -0.13124561, -0.12810041, ...,  0.04631348,\n",
      "        -0.11104114, -0.04089876],\n",
      "       [-0.17342295,  0.11702128,  0.13108821, ...,  0.10516681,\n",
      "        -0.17573798, -0.15862206]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "dense_2 [array([[-0.03961289,  0.16805448, -0.03389297, ...,  0.13574447,\n",
      "         0.07021256, -0.14995578],\n",
      "       [ 0.05898397, -0.15647417,  0.00752814, ..., -0.09644999,\n",
      "         0.1289068 ,  0.15939479],\n",
      "       [-0.08958507, -0.15752418,  0.14393456, ...,  0.08029042,\n",
      "        -0.14371498,  0.13695805],\n",
      "       ...,\n",
      "       [ 0.16226874, -0.03117627,  0.09170209, ..., -0.16349567,\n",
      "         0.1699978 , -0.03317247],\n",
      "       [-0.13403384, -0.10767314,  0.08500169, ...,  0.07288298,\n",
      "         0.12165748, -0.09823908],\n",
      "       [-0.16635154, -0.08446258,  0.145678  , ..., -0.07792185,\n",
      "         0.1742499 ,  0.1229596 ]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "dense_3 [array([[ 0.11276668,  0.23382998, -0.29525635],\n",
      "       [ 0.06484583, -0.11577526, -0.2700807 ],\n",
      "       [ 0.28109843, -0.23045099,  0.11080199],\n",
      "       [ 0.2967931 , -0.11379302,  0.07994786],\n",
      "       [ 0.24407977,  0.1130228 , -0.26167333],\n",
      "       [ 0.01383469, -0.08032472, -0.18792579],\n",
      "       [ 0.1938596 , -0.17063423,  0.03539819],\n",
      "       [ 0.03525171, -0.18334535, -0.0896942 ],\n",
      "       [ 0.08041334, -0.25065386, -0.2590696 ],\n",
      "       [-0.22806092,  0.28419036, -0.21942154],\n",
      "       [-0.00977945, -0.2910632 , -0.02215838],\n",
      "       [ 0.08544111, -0.24741569,  0.09236988],\n",
      "       [ 0.21348405, -0.24647048, -0.09788761],\n",
      "       [-0.2342426 ,  0.0870657 ,  0.14490357],\n",
      "       [-0.12016797,  0.07325363, -0.20927292],\n",
      "       [-0.26232466, -0.26624465,  0.20665032],\n",
      "       [ 0.09610927, -0.06950317,  0.16237506],\n",
      "       [-0.14724675, -0.2445949 , -0.12878267],\n",
      "       [-0.04953024, -0.11431535,  0.06400529],\n",
      "       [ 0.16191772, -0.23932388, -0.2858196 ],\n",
      "       [-0.09476843,  0.03957763,  0.14081842],\n",
      "       [-0.20557284, -0.06234024,  0.0225136 ],\n",
      "       [-0.16409895, -0.12802333, -0.21654023],\n",
      "       [-0.09105287, -0.14143936,  0.19026497],\n",
      "       [ 0.15131885, -0.11945949, -0.2983387 ],\n",
      "       [-0.04893228, -0.06221296,  0.10468122],\n",
      "       [ 0.1976884 , -0.01750788, -0.09688546],\n",
      "       [-0.08086789,  0.1417067 , -0.11335465],\n",
      "       [-0.12730479, -0.22003448, -0.13580926],\n",
      "       [ 0.18915138,  0.23650157, -0.18220586],\n",
      "       [-0.10982981,  0.1266382 , -0.16620055],\n",
      "       [-0.17709774, -0.08922745,  0.23238009],\n",
      "       [ 0.13565359, -0.13378556,  0.27106607],\n",
      "       [-0.0711672 ,  0.25825977, -0.07171494],\n",
      "       [ 0.24466753, -0.15847263,  0.21034557],\n",
      "       [ 0.23005849,  0.15498418, -0.2289437 ],\n",
      "       [-0.03669754, -0.03429478, -0.27699998],\n",
      "       [-0.13666992,  0.02763179, -0.013188  ],\n",
      "       [-0.27541077, -0.07157846,  0.01331791],\n",
      "       [ 0.14819053,  0.2556237 ,  0.07819751],\n",
      "       [ 0.2974797 ,  0.11256739, -0.19490284],\n",
      "       [-0.13496172, -0.07712907,  0.14717135],\n",
      "       [-0.01776144,  0.24589801,  0.21882278],\n",
      "       [-0.03791824,  0.13856462, -0.09804614],\n",
      "       [ 0.24464232,  0.19200414,  0.05744895],\n",
      "       [-0.14415784,  0.2645902 , -0.16712928],\n",
      "       [-0.14211772,  0.28726745, -0.22486326],\n",
      "       [-0.25893438,  0.00492132, -0.0957917 ],\n",
      "       [-0.26758718, -0.23628935,  0.18866986],\n",
      "       [ 0.17578423, -0.20983064,  0.28819865],\n",
      "       [ 0.00413415,  0.05609235,  0.10180238],\n",
      "       [ 0.04153734, -0.12354399,  0.2959702 ],\n",
      "       [ 0.02439454, -0.10433805,  0.12706912],\n",
      "       [-0.23850432, -0.01141724,  0.2338317 ],\n",
      "       [-0.29768294, -0.24347517,  0.10359025],\n",
      "       [ 0.24173492,  0.04777259, -0.04215527],\n",
      "       [ 0.12163225,  0.18655604,  0.16450989],\n",
      "       [ 0.14481574,  0.03200504, -0.13456126],\n",
      "       [-0.0881028 ,  0.03773859,  0.2755052 ],\n",
      "       [ 0.25729012, -0.02421883,  0.26172048],\n",
      "       [-0.10044676, -0.18705569, -0.05767748],\n",
      "       [-0.14405617, -0.10845259,  0.26375097],\n",
      "       [-0.14901988, -0.25888437,  0.11579639],\n",
      "       [ 0.00536227,  0.09827188,  0.00319231]], dtype=float32), array([0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#to see the randomly adjusted weights\n",
    "\n",
    "for layer in model.layers:\n",
    "    parameters= layer.get_weights()\n",
    "    name=layer.name\n",
    "    \n",
    "    print(name, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.6917 - val_loss: 1.1976 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2731 - accuracy: 0.9083 - val_loss: 1.7428 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1703 - accuracy: 0.9250 - val_loss: 1.6373 - val_accuracy: 0.1000\n",
      "Epoch 4/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2051 - accuracy: 0.8750 - val_loss: 3.2771 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.9333 - val_loss: 2.3453 - val_accuracy: 0.1000\n",
      "Epoch 6/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9583 - val_loss: 0.1187 - val_accuracy: 1.0000\n",
      "Epoch 7/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9583 - val_loss: 1.0046 - val_accuracy: 0.5333\n",
      "Epoch 8/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9833 - val_loss: 0.8172 - val_accuracy: 0.5667\n",
      "Epoch 9/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9417 - val_loss: 5.6854 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9333 - val_loss: 0.3169 - val_accuracy: 0.8000\n",
      "Epoch 11/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.9750 - val_loss: 0.2550 - val_accuracy: 0.8333\n",
      "Epoch 12/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0710 - accuracy: 0.9750 - val_loss: 0.2160 - val_accuracy: 0.8667\n",
      "Epoch 13/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1223 - accuracy: 0.9500 - val_loss: 0.0924 - val_accuracy: 1.0000\n",
      "Epoch 14/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9750 - val_loss: 0.2968 - val_accuracy: 0.8000\n",
      "Epoch 15/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9750 - val_loss: 0.0360 - val_accuracy: 1.0000\n",
      "Epoch 16/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0745 - accuracy: 0.9667 - val_loss: 0.9190 - val_accuracy: 0.5667\n",
      "Epoch 17/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0809 - accuracy: 0.9667 - val_loss: 0.1993 - val_accuracy: 0.9000\n",
      "Epoch 18/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0735 - accuracy: 0.9750 - val_loss: 0.5831 - val_accuracy: 0.7333\n",
      "Epoch 19/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9583 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
      "Epoch 20/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0884 - accuracy: 0.9667 - val_loss: 0.0875 - val_accuracy: 1.0000\n",
      "Epoch 21/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0453 - accuracy: 0.9833 - val_loss: 0.9690 - val_accuracy: 0.5667\n",
      "Epoch 22/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.9833 - val_loss: 0.6565 - val_accuracy: 0.7333\n",
      "Epoch 23/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9750 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 24/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9500 - val_loss: 0.3361 - val_accuracy: 0.8000\n",
      "Epoch 25/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0748 - accuracy: 0.9500 - val_loss: 0.0705 - val_accuracy: 1.0000\n",
      "Epoch 26/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0432 - accuracy: 0.9833 - val_loss: 0.1957 - val_accuracy: 0.9000\n",
      "Epoch 27/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.2037 - val_accuracy: 0.9000\n",
      "Epoch 28/1000\n",
      "120/120 [==============================] - 0s 975us/step - loss: 0.0347 - accuracy: 0.9833 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
      "Epoch 29/1000\n",
      "120/120 [==============================] - 0s 965us/step - loss: 0.0934 - accuracy: 0.9583 - val_loss: 0.2188 - val_accuracy: 0.9000\n",
      "Epoch 30/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0378 - accuracy: 0.9833 - val_loss: 2.3670 - val_accuracy: 0.4000\n",
      "Epoch 31/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.9417 - val_loss: 0.0926 - val_accuracy: 0.9667\n",
      "Epoch 32/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9583 - val_loss: 0.2022 - val_accuracy: 0.9000\n",
      "Epoch 33/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0346 - accuracy: 0.9833 - val_loss: 3.9256 - val_accuracy: 0.0333\n",
      "Epoch 34/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.9750 - val_loss: 0.4282 - val_accuracy: 0.7667\n",
      "Epoch 35/1000\n",
      "120/120 [==============================] - 0s 982us/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 2.5149 - val_accuracy: 0.3000\n",
      "Epoch 36/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.9750 - val_loss: 0.4312 - val_accuracy: 0.7667\n",
      "Epoch 37/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0483 - accuracy: 0.9833 - val_loss: 0.3205 - val_accuracy: 0.8000\n",
      "Epoch 38/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0370 - accuracy: 0.9833 - val_loss: 0.8318 - val_accuracy: 0.7000\n",
      "Epoch 39/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9667 - val_loss: 0.3630 - val_accuracy: 0.8000\n",
      "Epoch 40/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0490 - accuracy: 0.9750 - val_loss: 0.1778 - val_accuracy: 0.9000\n",
      "Epoch 41/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0456 - accuracy: 0.9833 - val_loss: 1.1206 - val_accuracy: 0.5333\n",
      "Epoch 42/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0520 - accuracy: 0.9833 - val_loss: 0.2011 - val_accuracy: 0.9000\n",
      "Epoch 43/1000\n",
      "120/120 [==============================] - 0s 936us/step - loss: 0.0506 - accuracy: 0.9750 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 44/1000\n",
      "120/120 [==============================] - 0s 938us/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.0745 - val_accuracy: 1.0000\n",
      "Epoch 45/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9583 - val_loss: 0.4746 - val_accuracy: 0.7667\n",
      "Epoch 46/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0493 - val_accuracy: 1.0000\n",
      "Epoch 47/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0539 - accuracy: 0.9917 - val_loss: 0.0682 - val_accuracy: 1.0000\n",
      "Epoch 48/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0534 - accuracy: 0.9917 - val_loss: 0.8150 - val_accuracy: 0.7333\n",
      "Epoch 49/1000\n",
      "120/120 [==============================] - 0s 987us/step - loss: 0.0665 - accuracy: 0.9750 - val_loss: 0.5079 - val_accuracy: 0.8000\n",
      "Epoch 50/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.9750 - val_loss: 0.4577 - val_accuracy: 0.8000\n",
      "Epoch 51/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9750 - val_loss: 0.2242 - val_accuracy: 0.9000\n",
      "Epoch 52/1000\n",
      "120/120 [==============================] - 0s 993us/step - loss: 0.0809 - accuracy: 0.9833 - val_loss: 0.1156 - val_accuracy: 0.9667\n",
      "Epoch 53/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0376 - accuracy: 0.9917 - val_loss: 0.9947 - val_accuracy: 0.6667\n",
      "Epoch 54/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0452 - accuracy: 0.9833 - val_loss: 0.0514 - val_accuracy: 1.0000\n",
      "Epoch 55/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0634 - accuracy: 0.9833 - val_loss: 0.7127 - val_accuracy: 0.7333\n",
      "Epoch 56/1000\n",
      "120/120 [==============================] - 0s 959us/step - loss: 0.0620 - accuracy: 0.9750 - val_loss: 0.3478 - val_accuracy: 0.8000\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 947us/step - loss: 0.0479 - accuracy: 0.9833 - val_loss: 0.2186 - val_accuracy: 0.9000\n",
      "Epoch 58/1000\n",
      "120/120 [==============================] - 0s 983us/step - loss: 0.0638 - accuracy: 0.9750 - val_loss: 0.9549 - val_accuracy: 0.5667\n",
      "Epoch 59/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0592 - accuracy: 0.9750 - val_loss: 0.4100 - val_accuracy: 0.8000\n",
      "Epoch 60/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.6526 - val_accuracy: 0.7333\n",
      "Epoch 61/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0412 - accuracy: 0.9833 - val_loss: 0.3182 - val_accuracy: 0.8333\n",
      "Epoch 62/1000\n",
      "120/120 [==============================] - 0s 946us/step - loss: 0.0479 - accuracy: 0.9750 - val_loss: 0.5429 - val_accuracy: 0.7333\n",
      "Epoch 63/1000\n",
      "120/120 [==============================] - 0s 985us/step - loss: 0.0386 - accuracy: 0.9667 - val_loss: 0.0932 - val_accuracy: 0.9333\n",
      "Epoch 64/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0481 - accuracy: 0.9750 - val_loss: 0.1963 - val_accuracy: 0.9000\n",
      "Epoch 65/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0468 - accuracy: 0.9833 - val_loss: 0.4186 - val_accuracy: 0.8000\n",
      "Epoch 66/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.3364 - val_accuracy: 0.8333\n",
      "Epoch 67/1000\n",
      "120/120 [==============================] - 0s 998us/step - loss: 0.0432 - accuracy: 0.9833 - val_loss: 0.7960 - val_accuracy: 0.7000\n",
      "Epoch 68/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.2240 - val_accuracy: 0.9000\n",
      "Epoch 69/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0298 - accuracy: 0.9833 - val_loss: 0.6474 - val_accuracy: 0.7333\n",
      "Epoch 70/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0414 - accuracy: 0.9833 - val_loss: 0.5216 - val_accuracy: 0.7667\n",
      "Epoch 71/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9750 - val_loss: 0.9051 - val_accuracy: 0.6667\n",
      "Epoch 72/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.5331 - val_accuracy: 0.7333\n",
      "Epoch 73/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0283 - accuracy: 0.9833 - val_loss: 0.1558 - val_accuracy: 0.9333\n",
      "Epoch 74/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.9750 - val_loss: 0.9357 - val_accuracy: 0.7333\n",
      "Epoch 75/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9667 - val_loss: 1.0920 - val_accuracy: 0.6333\n",
      "Epoch 76/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9750 - val_loss: 0.3180 - val_accuracy: 0.8333\n",
      "Epoch 77/1000\n",
      "120/120 [==============================] - 0s 939us/step - loss: 0.0303 - accuracy: 0.9917 - val_loss: 0.3760 - val_accuracy: 0.8000\n",
      "Epoch 78/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9750 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 79/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.9583 - val_loss: 0.6502 - val_accuracy: 0.7333\n",
      "Epoch 80/1000\n",
      "120/120 [==============================] - 0s 980us/step - loss: 0.0508 - accuracy: 0.9750 - val_loss: 0.1428 - val_accuracy: 0.9333\n",
      "Epoch 81/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0479 - accuracy: 0.9833 - val_loss: 0.0987 - val_accuracy: 0.9333\n",
      "Epoch 82/1000\n",
      "120/120 [==============================] - 0s 942us/step - loss: 0.0515 - accuracy: 0.9750 - val_loss: 0.4364 - val_accuracy: 0.7667\n",
      "Epoch 83/1000\n",
      "120/120 [==============================] - 0s 943us/step - loss: 0.0391 - accuracy: 0.9833 - val_loss: 0.4984 - val_accuracy: 0.7667\n",
      "Epoch 84/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0347 - accuracy: 0.9667 - val_loss: 0.4250 - val_accuracy: 0.8000\n",
      "Epoch 85/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0532 - accuracy: 0.9833 - val_loss: 0.3367 - val_accuracy: 0.8000\n",
      "Epoch 86/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0402 - accuracy: 0.9833 - val_loss: 1.5139 - val_accuracy: 0.5000\n",
      "Epoch 87/1000\n",
      "120/120 [==============================] - 0s 949us/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 88/1000\n",
      "120/120 [==============================] - 0s 962us/step - loss: 0.0363 - accuracy: 0.9917 - val_loss: 0.3646 - val_accuracy: 0.8333\n",
      "Epoch 89/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0371 - accuracy: 0.9750 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 90/1000\n",
      "120/120 [==============================] - 0s 949us/step - loss: 0.0613 - accuracy: 0.9750 - val_loss: 1.8450 - val_accuracy: 0.5000\n",
      "Epoch 91/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0267 - accuracy: 0.9833 - val_loss: 0.5280 - val_accuracy: 0.8000\n",
      "Epoch 92/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0680 - accuracy: 0.9750 - val_loss: 0.1381 - val_accuracy: 0.9333\n",
      "Epoch 93/1000\n",
      "120/120 [==============================] - 0s 968us/step - loss: 0.0547 - accuracy: 0.9667 - val_loss: 0.2765 - val_accuracy: 0.8667\n",
      "Epoch 94/1000\n",
      "120/120 [==============================] - 0s 950us/step - loss: 0.0319 - accuracy: 0.9833 - val_loss: 1.0741 - val_accuracy: 0.6333\n",
      "Epoch 95/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0339 - accuracy: 0.9833 - val_loss: 0.0514 - val_accuracy: 0.9667\n",
      "Epoch 96/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0528 - accuracy: 0.9833 - val_loss: 0.2813 - val_accuracy: 0.9000\n",
      "Epoch 97/1000\n",
      "120/120 [==============================] - 0s 932us/step - loss: 0.0445 - accuracy: 0.9833 - val_loss: 0.4232 - val_accuracy: 0.7667\n",
      "Epoch 98/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0312 - accuracy: 0.9917 - val_loss: 0.7569 - val_accuracy: 0.7000\n",
      "Epoch 99/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0379 - accuracy: 0.9833 - val_loss: 0.6194 - val_accuracy: 0.7333\n",
      "Epoch 100/1000\n",
      "120/120 [==============================] - 0s 946us/step - loss: 0.0577 - accuracy: 0.9917 - val_loss: 0.6623 - val_accuracy: 0.7333\n",
      "Epoch 101/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0218 - accuracy: 0.9917 - val_loss: 4.0590 - val_accuracy: 0.4333\n",
      "Epoch 102/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9500 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
      "Epoch 103/1000\n",
      "120/120 [==============================] - 0s 934us/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.9086 - val_accuracy: 0.6667\n",
      "Epoch 104/1000\n",
      "120/120 [==============================] - 0s 952us/step - loss: 0.0240 - accuracy: 0.9833 - val_loss: 1.2028 - val_accuracy: 0.6667\n",
      "Epoch 105/1000\n",
      "120/120 [==============================] - 0s 958us/step - loss: 0.0833 - accuracy: 0.9750 - val_loss: 0.5253 - val_accuracy: 0.7333\n",
      "Epoch 106/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0544 - accuracy: 0.9750 - val_loss: 0.1471 - val_accuracy: 0.9333\n",
      "Epoch 107/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0387 - accuracy: 0.9917 - val_loss: 1.0167 - val_accuracy: 0.7333\n",
      "Epoch 108/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0846 - accuracy: 0.9833 - val_loss: 0.2465 - val_accuracy: 0.9000\n",
      "Epoch 109/1000\n",
      "120/120 [==============================] - 0s 931us/step - loss: 0.0297 - accuracy: 0.9833 - val_loss: 0.6467 - val_accuracy: 0.7667\n",
      "Epoch 110/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0321 - accuracy: 0.9917 - val_loss: 0.6583 - val_accuracy: 0.7333\n",
      "Epoch 111/1000\n",
      "120/120 [==============================] - 0s 942us/step - loss: 0.0419 - accuracy: 0.9833 - val_loss: 0.2089 - val_accuracy: 0.9000\n",
      "Epoch 112/1000\n",
      "120/120 [==============================] - 0s 958us/step - loss: 0.0286 - accuracy: 0.9917 - val_loss: 0.1126 - val_accuracy: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/1000\n",
      "120/120 [==============================] - 0s 944us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.3888 - val_accuracy: 0.7667\n",
      "Epoch 114/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0464 - accuracy: 0.9833 - val_loss: 0.2741 - val_accuracy: 0.8667\n",
      "Epoch 115/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0191 - accuracy: 0.9917 - val_loss: 2.1790 - val_accuracy: 0.4667\n",
      "Epoch 116/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9833 - val_loss: 0.9498 - val_accuracy: 0.6333\n",
      "Epoch 117/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0422 - accuracy: 0.9833 - val_loss: 0.4310 - val_accuracy: 0.7667\n",
      "Epoch 118/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0521 - accuracy: 0.9750 - val_loss: 0.4986 - val_accuracy: 0.7333\n",
      "Epoch 119/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0328 - accuracy: 0.9750 - val_loss: 1.0654 - val_accuracy: 0.6000\n",
      "Epoch 120/1000\n",
      "120/120 [==============================] - 0s 942us/step - loss: 0.0401 - accuracy: 0.9917 - val_loss: 0.5386 - val_accuracy: 0.7667\n",
      "Epoch 121/1000\n",
      "120/120 [==============================] - 0s 928us/step - loss: 0.0516 - accuracy: 0.9833 - val_loss: 0.4305 - val_accuracy: 0.7667\n",
      "Epoch 122/1000\n",
      "120/120 [==============================] - 0s 920us/step - loss: 0.0416 - accuracy: 0.9750 - val_loss: 0.2792 - val_accuracy: 0.8667\n",
      "Epoch 123/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0493 - accuracy: 0.9917 - val_loss: 1.2537 - val_accuracy: 0.6000\n",
      "Epoch 124/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0291 - accuracy: 0.9917 - val_loss: 0.0670 - val_accuracy: 0.9667\n",
      "Epoch 125/1000\n",
      "120/120 [==============================] - 0s 952us/step - loss: 0.0128 - accuracy: 0.9917 - val_loss: 1.4403 - val_accuracy: 0.6333\n",
      "Epoch 126/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0597 - accuracy: 0.9667 - val_loss: 1.2383 - val_accuracy: 0.6333\n",
      "Epoch 127/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0343 - accuracy: 0.9833 - val_loss: 0.2602 - val_accuracy: 0.9000\n",
      "Epoch 128/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0659 - accuracy: 0.9667 - val_loss: 0.1884 - val_accuracy: 0.9333\n",
      "Epoch 129/1000\n",
      "120/120 [==============================] - 0s 945us/step - loss: 0.0302 - accuracy: 0.9833 - val_loss: 0.6233 - val_accuracy: 0.7333\n",
      "Epoch 130/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0463 - accuracy: 0.9833 - val_loss: 0.0521 - val_accuracy: 0.9667\n",
      "Epoch 131/1000\n",
      "120/120 [==============================] - 0s 936us/step - loss: 0.0375 - accuracy: 0.9833 - val_loss: 0.1307 - val_accuracy: 0.9333\n",
      "Epoch 132/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0396 - accuracy: 0.9833 - val_loss: 0.1270 - val_accuracy: 0.9333\n",
      "Epoch 133/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0408 - accuracy: 0.9750 - val_loss: 0.5000 - val_accuracy: 0.7667\n",
      "Epoch 134/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0218 - accuracy: 0.9917 - val_loss: 0.4287 - val_accuracy: 0.8000\n",
      "Epoch 135/1000\n",
      "120/120 [==============================] - 0s 951us/step - loss: 0.0402 - accuracy: 0.9750 - val_loss: 0.7009 - val_accuracy: 0.7333\n",
      "Epoch 136/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.9750 - val_loss: 0.0491 - val_accuracy: 1.0000\n",
      "Epoch 137/1000\n",
      "120/120 [==============================] - 0s 950us/step - loss: 0.0372 - accuracy: 0.9833 - val_loss: 0.6700 - val_accuracy: 0.6667\n",
      "Epoch 138/1000\n",
      "120/120 [==============================] - 0s 958us/step - loss: 0.0380 - accuracy: 0.9833 - val_loss: 0.2185 - val_accuracy: 0.9000\n",
      "Epoch 139/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0256 - accuracy: 0.9917 - val_loss: 0.9973 - val_accuracy: 0.6333\n",
      "Epoch 140/1000\n",
      "120/120 [==============================] - 0s 938us/step - loss: 0.0487 - accuracy: 0.9833 - val_loss: 1.2189 - val_accuracy: 0.6333\n",
      "Epoch 141/1000\n",
      "120/120 [==============================] - 0s 942us/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.3481 - val_accuracy: 0.8667\n",
      "Epoch 142/1000\n",
      "120/120 [==============================] - 0s 951us/step - loss: 0.0172 - accuracy: 0.9917 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 143/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9583 - val_loss: 0.1058 - val_accuracy: 0.9667\n",
      "Epoch 144/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0292 - accuracy: 0.9917 - val_loss: 1.5357 - val_accuracy: 0.6333\n",
      "Epoch 145/1000\n",
      "120/120 [==============================] - 0s 952us/step - loss: 0.0159 - accuracy: 0.9917 - val_loss: 1.0046 - val_accuracy: 0.7333\n",
      "Epoch 146/1000\n",
      "120/120 [==============================] - 0s 937us/step - loss: 0.0368 - accuracy: 0.9833 - val_loss: 0.4941 - val_accuracy: 0.8333\n",
      "Epoch 147/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9750 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 148/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0705 - accuracy: 0.9833 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
      "Epoch 149/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.9833 - val_loss: 0.2055 - val_accuracy: 0.9333\n",
      "Epoch 150/1000\n",
      "120/120 [==============================] - 0s 939us/step - loss: 0.0204 - accuracy: 0.9917 - val_loss: 1.3955 - val_accuracy: 0.6000\n",
      "Epoch 151/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.9917 - val_loss: 0.5297 - val_accuracy: 0.8000\n",
      "Epoch 152/1000\n",
      "120/120 [==============================] - 0s 933us/step - loss: 0.0301 - accuracy: 0.9833 - val_loss: 0.1650 - val_accuracy: 0.9333\n",
      "Epoch 153/1000\n",
      "120/120 [==============================] - 0s 945us/step - loss: 0.0328 - accuracy: 0.9917 - val_loss: 0.2915 - val_accuracy: 0.9000\n",
      "Epoch 154/1000\n",
      "120/120 [==============================] - 0s 951us/step - loss: 0.0295 - accuracy: 0.9917 - val_loss: 1.7084 - val_accuracy: 0.6333\n",
      "Epoch 155/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0617 - accuracy: 0.9833 - val_loss: 0.5608 - val_accuracy: 0.8000\n",
      "Epoch 156/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0291 - accuracy: 0.9917 - val_loss: 0.5360 - val_accuracy: 0.7667\n",
      "Epoch 157/1000\n",
      "120/120 [==============================] - 0s 984us/step - loss: 0.0156 - accuracy: 0.9917 - val_loss: 0.1034 - val_accuracy: 0.9333\n",
      "Epoch 158/1000\n",
      "120/120 [==============================] - 0s 960us/step - loss: 0.0609 - accuracy: 0.9750 - val_loss: 0.2819 - val_accuracy: 0.8667\n",
      "Epoch 159/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0303 - accuracy: 0.9833 - val_loss: 1.0472 - val_accuracy: 0.6333\n",
      "Epoch 160/1000\n",
      "120/120 [==============================] - 0s 938us/step - loss: 0.0310 - accuracy: 0.9833 - val_loss: 0.3561 - val_accuracy: 0.8667\n",
      "Epoch 161/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 162/1000\n",
      "120/120 [==============================] - 0s 965us/step - loss: 0.0161 - accuracy: 0.9917 - val_loss: 1.6757 - val_accuracy: 0.6333\n",
      "Epoch 163/1000\n",
      "120/120 [==============================] - 0s 940us/step - loss: 0.0527 - accuracy: 0.9583 - val_loss: 2.5420 - val_accuracy: 0.5000\n",
      "Epoch 164/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0659 - accuracy: 0.9750 - val_loss: 0.5227 - val_accuracy: 0.7667\n",
      "Epoch 165/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0304 - accuracy: 0.9750 - val_loss: 0.3521 - val_accuracy: 0.8667\n",
      "Epoch 166/1000\n",
      "120/120 [==============================] - 0s 938us/step - loss: 0.0291 - accuracy: 0.9917 - val_loss: 0.0863 - val_accuracy: 0.9333\n",
      "Epoch 167/1000\n",
      "120/120 [==============================] - 0s 943us/step - loss: 0.0205 - accuracy: 0.9917 - val_loss: 3.1002 - val_accuracy: 0.4333\n",
      "Epoch 168/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 938us/step - loss: 0.0255 - accuracy: 0.9833 - val_loss: 0.5800 - val_accuracy: 0.8000\n",
      "Epoch 169/1000\n",
      "120/120 [==============================] - 0s 952us/step - loss: 0.0453 - accuracy: 0.9750 - val_loss: 0.3186 - val_accuracy: 0.8333\n",
      "Epoch 170/1000\n",
      "120/120 [==============================] - 0s 938us/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 0.6041 - val_accuracy: 0.8000\n",
      "Epoch 171/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.0175 - accuracy: 0.9917 - val_loss: 0.2071 - val_accuracy: 0.9333\n",
      "Epoch 172/1000\n",
      "120/120 [==============================] - 0s 959us/step - loss: 0.0734 - accuracy: 0.9833 - val_loss: 0.6012 - val_accuracy: 0.8000\n",
      "Epoch 173/1000\n",
      "120/120 [==============================] - 0s 942us/step - loss: 0.0675 - accuracy: 0.9833 - val_loss: 0.3587 - val_accuracy: 0.8333\n",
      "Epoch 174/1000\n",
      "120/120 [==============================] - 0s 934us/step - loss: 0.0194 - accuracy: 0.9917 - val_loss: 0.2313 - val_accuracy: 0.9333\n",
      "Epoch 175/1000\n",
      "120/120 [==============================] - 0s 943us/step - loss: 0.1204 - accuracy: 0.9583 - val_loss: 0.3318 - val_accuracy: 0.8000\n",
      "Epoch 176/1000\n",
      "120/120 [==============================] - 0s 962us/step - loss: 0.0320 - accuracy: 0.9917 - val_loss: 0.7766 - val_accuracy: 0.6333\n",
      "Epoch 177/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0491 - accuracy: 0.9833 - val_loss: 0.7028 - val_accuracy: 0.6667\n",
      "Epoch 178/1000\n",
      "120/120 [==============================] - 0s 945us/step - loss: 0.0364 - accuracy: 0.9750 - val_loss: 0.2527 - val_accuracy: 0.9333\n",
      "Epoch 179/1000\n",
      "120/120 [==============================] - 0s 938us/step - loss: 0.0589 - accuracy: 0.9917 - val_loss: 0.6183 - val_accuracy: 0.8000\n",
      "Epoch 180/1000\n",
      "120/120 [==============================] - 0s 935us/step - loss: 0.0595 - accuracy: 0.9917 - val_loss: 0.4046 - val_accuracy: 0.8000\n",
      "Epoch 181/1000\n",
      "120/120 [==============================] - 0s 952us/step - loss: 0.0299 - accuracy: 0.9917 - val_loss: 0.4755 - val_accuracy: 0.7667\n",
      "Epoch 182/1000\n",
      "120/120 [==============================] - 0s 950us/step - loss: 0.0301 - accuracy: 0.9917 - val_loss: 1.1337 - val_accuracy: 0.6333\n",
      "Epoch 183/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0567 - accuracy: 0.9667 - val_loss: 0.1257 - val_accuracy: 0.9333\n",
      "Epoch 184/1000\n",
      "120/120 [==============================] - 0s 957us/step - loss: 0.0225 - accuracy: 0.9917 - val_loss: 0.7014 - val_accuracy: 0.7667\n",
      "Epoch 185/1000\n",
      "120/120 [==============================] - 0s 962us/step - loss: 0.0233 - accuracy: 0.9833 - val_loss: 0.3532 - val_accuracy: 0.8667\n",
      "Epoch 186/1000\n",
      "120/120 [==============================] - 0s 928us/step - loss: 0.0458 - accuracy: 0.9750 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 187/1000\n",
      "120/120 [==============================] - 0s 950us/step - loss: 0.0584 - accuracy: 0.9750 - val_loss: 0.2060 - val_accuracy: 0.9333\n",
      "Epoch 188/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0370 - accuracy: 0.9750 - val_loss: 0.4411 - val_accuracy: 0.8000\n",
      "Epoch 189/1000\n",
      "120/120 [==============================] - 0s 952us/step - loss: 0.0396 - accuracy: 0.9750 - val_loss: 0.0872 - val_accuracy: 0.9667\n",
      "Epoch 190/1000\n",
      "120/120 [==============================] - 0s 950us/step - loss: 0.0390 - accuracy: 0.9833 - val_loss: 0.2657 - val_accuracy: 0.8667\n",
      "Epoch 191/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0286 - accuracy: 0.9917 - val_loss: 1.0129 - val_accuracy: 0.6667\n",
      "Epoch 192/1000\n",
      "120/120 [==============================] - 0s 937us/step - loss: 0.0459 - accuracy: 0.9833 - val_loss: 0.1548 - val_accuracy: 0.9333\n",
      "Epoch 193/1000\n",
      "120/120 [==============================] - 0s 935us/step - loss: 0.0285 - accuracy: 0.9917 - val_loss: 0.2487 - val_accuracy: 0.8667\n",
      "Epoch 194/1000\n",
      "120/120 [==============================] - 0s 949us/step - loss: 0.0440 - accuracy: 0.9667 - val_loss: 0.6215 - val_accuracy: 0.7000\n",
      "Epoch 195/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0353 - accuracy: 0.9833 - val_loss: 0.4499 - val_accuracy: 0.8000\n",
      "Epoch 196/1000\n",
      "120/120 [==============================] - 0s 946us/step - loss: 0.0224 - accuracy: 0.9917 - val_loss: 0.2053 - val_accuracy: 0.9333\n",
      "Epoch 197/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0304 - accuracy: 0.9917 - val_loss: 0.2277 - val_accuracy: 0.9333\n",
      "Epoch 198/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 199/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0551 - accuracy: 0.9750 - val_loss: 0.3178 - val_accuracy: 0.8000\n",
      "Epoch 200/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0268 - accuracy: 0.9917 - val_loss: 0.9049 - val_accuracy: 0.6333\n",
      "Epoch 201/1000\n",
      "120/120 [==============================] - 0s 932us/step - loss: 0.0392 - accuracy: 0.9833 - val_loss: 0.8456 - val_accuracy: 0.6333\n",
      "Epoch 202/1000\n",
      "120/120 [==============================] - 0s 939us/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.3736 - val_accuracy: 0.8000\n",
      "Epoch 203/1000\n",
      "120/120 [==============================] - 0s 933us/step - loss: 0.0291 - accuracy: 0.9917 - val_loss: 1.1552 - val_accuracy: 0.6333\n",
      "Epoch 204/1000\n",
      "120/120 [==============================] - 0s 941us/step - loss: 0.0323 - accuracy: 0.9917 - val_loss: 0.4186 - val_accuracy: 0.8333\n",
      "Epoch 205/1000\n",
      "120/120 [==============================] - 0s 960us/step - loss: 0.0218 - accuracy: 0.9917 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
      "Epoch 206/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0368 - accuracy: 0.9833 - val_loss: 0.1198 - val_accuracy: 0.9333\n",
      "Epoch 207/1000\n",
      "120/120 [==============================] - 0s 938us/step - loss: 0.0275 - accuracy: 0.9833 - val_loss: 0.2210 - val_accuracy: 0.9333\n",
      "Epoch 208/1000\n",
      "120/120 [==============================] - 0s 923us/step - loss: 0.0344 - accuracy: 0.9833 - val_loss: 0.1687 - val_accuracy: 0.9333\n",
      "Epoch 209/1000\n",
      "120/120 [==============================] - 0s 983us/step - loss: 0.0372 - accuracy: 0.9833 - val_loss: 0.0417 - val_accuracy: 0.9667\n",
      "Epoch 210/1000\n",
      "120/120 [==============================] - 0s 932us/step - loss: 0.0513 - accuracy: 0.9833 - val_loss: 0.4755 - val_accuracy: 0.8000\n",
      "Epoch 211/1000\n",
      "120/120 [==============================] - 0s 936us/step - loss: 0.0409 - accuracy: 0.9917 - val_loss: 0.0200 - val_accuracy: 1.0000\n",
      "Epoch 212/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.3747 - val_accuracy: 0.8333\n",
      "Epoch 213/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0250 - accuracy: 0.9917 - val_loss: 0.6925 - val_accuracy: 0.7667\n",
      "Epoch 214/1000\n",
      "120/120 [==============================] - 0s 941us/step - loss: 0.0313 - accuracy: 0.9917 - val_loss: 0.7828 - val_accuracy: 0.7667\n",
      "Epoch 215/1000\n",
      "120/120 [==============================] - 0s 952us/step - loss: 0.0250 - accuracy: 0.9917 - val_loss: 1.4756 - val_accuracy: 0.6667\n",
      "Epoch 216/1000\n",
      "120/120 [==============================] - 0s 943us/step - loss: 0.0084 - accuracy: 0.9917 - val_loss: 0.1201 - val_accuracy: 0.9333\n",
      "Epoch 217/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0200 - accuracy: 0.9917 - val_loss: 1.0861 - val_accuracy: 0.7333\n",
      "Epoch 218/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0384 - accuracy: 0.9833 - val_loss: 0.1264 - val_accuracy: 0.9333\n",
      "Epoch 219/1000\n",
      "120/120 [==============================] - 0s 940us/step - loss: 0.0394 - accuracy: 0.9750 - val_loss: 0.3747 - val_accuracy: 0.8333\n",
      "Epoch 220/1000\n",
      "120/120 [==============================] - 0s 932us/step - loss: 0.0210 - accuracy: 0.9917 - val_loss: 0.9863 - val_accuracy: 0.7000\n",
      "Epoch 221/1000\n",
      "120/120 [==============================] - 0s 938us/step - loss: 0.0445 - accuracy: 0.9833 - val_loss: 0.2215 - val_accuracy: 0.9333\n",
      "Epoch 222/1000\n",
      "120/120 [==============================] - 0s 958us/step - loss: 0.0315 - accuracy: 0.9917 - val_loss: 0.3962 - val_accuracy: 0.8333\n",
      "Epoch 223/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 946us/step - loss: 0.0226 - accuracy: 0.9917 - val_loss: 1.6742 - val_accuracy: 0.6333\n",
      "Epoch 224/1000\n",
      "120/120 [==============================] - 0s 936us/step - loss: 0.0609 - accuracy: 0.9667 - val_loss: 0.6591 - val_accuracy: 0.7000\n",
      "Epoch 225/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0279 - accuracy: 0.9833 - val_loss: 0.3471 - val_accuracy: 0.8333\n",
      "Epoch 226/1000\n",
      "120/120 [==============================] - 0s 937us/step - loss: 0.0319 - accuracy: 0.9833 - val_loss: 0.6363 - val_accuracy: 0.7667\n",
      "Epoch 227/1000\n",
      "120/120 [==============================] - 0s 933us/step - loss: 0.0238 - accuracy: 0.9917 - val_loss: 1.5556 - val_accuracy: 0.6333\n",
      "Epoch 228/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0246 - accuracy: 0.9833 - val_loss: 1.0592 - val_accuracy: 0.7667\n",
      "Epoch 229/1000\n",
      "120/120 [==============================] - 0s 938us/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0558 - val_accuracy: 0.9667\n",
      "Epoch 230/1000\n",
      "120/120 [==============================] - 0s 962us/step - loss: 0.0445 - accuracy: 0.9667 - val_loss: 0.2491 - val_accuracy: 0.8333\n",
      "Epoch 231/1000\n",
      "120/120 [==============================] - 0s 937us/step - loss: 0.0299 - accuracy: 0.9833 - val_loss: 0.3703 - val_accuracy: 0.8000\n",
      "Epoch 232/1000\n",
      "120/120 [==============================] - 0s 938us/step - loss: 0.0365 - accuracy: 0.9833 - val_loss: 0.2208 - val_accuracy: 0.8667\n",
      "Epoch 233/1000\n",
      "120/120 [==============================] - 0s 965us/step - loss: 0.0391 - accuracy: 0.9833 - val_loss: 2.1421 - val_accuracy: 0.5667\n",
      "Epoch 234/1000\n",
      "120/120 [==============================] - 0s 959us/step - loss: 0.0341 - accuracy: 0.9917 - val_loss: 0.5724 - val_accuracy: 0.7000\n",
      "Epoch 235/1000\n",
      "120/120 [==============================] - 0s 929us/step - loss: 0.0295 - accuracy: 0.9750 - val_loss: 1.3004 - val_accuracy: 0.6333\n",
      "Epoch 236/1000\n",
      "120/120 [==============================] - 0s 933us/step - loss: 0.0295 - accuracy: 0.9833 - val_loss: 0.5294 - val_accuracy: 0.8000\n",
      "Epoch 237/1000\n",
      "120/120 [==============================] - 0s 943us/step - loss: 0.0307 - accuracy: 0.9750 - val_loss: 0.6198 - val_accuracy: 0.7667\n",
      "Epoch 238/1000\n",
      "120/120 [==============================] - 0s 946us/step - loss: 0.1422 - accuracy: 0.9750 - val_loss: 1.3555e-04 - val_accuracy: 1.0000\n",
      "Epoch 239/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0508 - accuracy: 0.9833 - val_loss: 1.0428 - val_accuracy: 0.6333\n",
      "Epoch 240/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0382 - accuracy: 0.9750 - val_loss: 0.1967 - val_accuracy: 0.9333\n",
      "Epoch 241/1000\n",
      "120/120 [==============================] - 0s 937us/step - loss: 0.0590 - accuracy: 0.9833 - val_loss: 0.1371 - val_accuracy: 0.9333\n",
      "Epoch 242/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0367 - accuracy: 0.9750 - val_loss: 0.3100 - val_accuracy: 0.8667\n",
      "Epoch 243/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0311 - accuracy: 0.9917 - val_loss: 0.7946 - val_accuracy: 0.7000\n",
      "Epoch 244/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0228 - accuracy: 0.9917 - val_loss: 0.2500 - val_accuracy: 0.9333\n",
      "Epoch 245/1000\n",
      "120/120 [==============================] - 0s 959us/step - loss: 0.0249 - accuracy: 0.9833 - val_loss: 0.4061 - val_accuracy: 0.9000\n",
      "Epoch 246/1000\n",
      "120/120 [==============================] - 0s 939us/step - loss: 0.0340 - accuracy: 0.9750 - val_loss: 1.5339 - val_accuracy: 0.6333\n",
      "Epoch 247/1000\n",
      "120/120 [==============================] - 0s 946us/step - loss: 0.0751 - accuracy: 0.9750 - val_loss: 0.3905 - val_accuracy: 0.8667\n",
      "Epoch 248/1000\n",
      "120/120 [==============================] - 0s 960us/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 0.4201 - val_accuracy: 0.9000\n",
      "Epoch 249/1000\n",
      "120/120 [==============================] - 0s 951us/step - loss: 0.0192 - accuracy: 0.9917 - val_loss: 1.5035 - val_accuracy: 0.6667\n",
      "Epoch 250/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0193 - accuracy: 0.9833 - val_loss: 0.6174 - val_accuracy: 0.8333\n",
      "Epoch 251/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0449 - accuracy: 0.9917 - val_loss: 0.1832 - val_accuracy: 0.9333\n",
      "Epoch 252/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0289 - accuracy: 0.9917 - val_loss: 0.3791 - val_accuracy: 0.9000\n",
      "Epoch 253/1000\n",
      "120/120 [==============================] - 0s 945us/step - loss: 0.0141 - accuracy: 0.9917 - val_loss: 1.4205 - val_accuracy: 0.6667\n",
      "Epoch 254/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0254 - accuracy: 0.9833 - val_loss: 0.2506 - val_accuracy: 0.9333\n",
      "Epoch 255/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 1.9746 - val_accuracy: 0.6333\n",
      "Epoch 256/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9750 - val_loss: 0.7569 - val_accuracy: 0.7000\n",
      "Epoch 257/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 0.9917 - val_loss: 0.8615 - val_accuracy: 0.7000\n",
      "Epoch 258/1000\n",
      "120/120 [==============================] - 0s 942us/step - loss: 0.0290 - accuracy: 0.9750 - val_loss: 0.2750 - val_accuracy: 0.9333\n",
      "Epoch 259/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0359 - accuracy: 0.9750 - val_loss: 0.2648 - val_accuracy: 0.9333\n",
      "Epoch 260/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0221 - accuracy: 0.9833 - val_loss: 0.4103 - val_accuracy: 0.8333\n",
      "Epoch 261/1000\n",
      "120/120 [==============================] - 0s 960us/step - loss: 0.0497 - accuracy: 0.9833 - val_loss: 1.3812 - val_accuracy: 0.6333\n",
      "Epoch 262/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0168 - accuracy: 0.9917 - val_loss: 0.7535 - val_accuracy: 0.7000\n",
      "Epoch 263/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9917 - val_loss: 0.4553 - val_accuracy: 0.8667\n",
      "Epoch 264/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0544 - accuracy: 0.9667 - val_loss: 0.1243 - val_accuracy: 0.9333\n",
      "Epoch 265/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0358 - accuracy: 0.9833 - val_loss: 0.5743 - val_accuracy: 0.7333\n",
      "Epoch 266/1000\n",
      "120/120 [==============================] - 0s 974us/step - loss: 0.0311 - accuracy: 0.9750 - val_loss: 0.3325 - val_accuracy: 0.8333\n",
      "Epoch 267/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0590 - accuracy: 0.9667 - val_loss: 0.7985 - val_accuracy: 0.6333\n",
      "Epoch 268/1000\n",
      "120/120 [==============================] - 0s 950us/step - loss: 0.0318 - accuracy: 0.9833 - val_loss: 0.2685 - val_accuracy: 0.9000\n",
      "Epoch 269/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0309 - accuracy: 0.9750 - val_loss: 0.9617 - val_accuracy: 0.7000\n",
      "Epoch 270/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 0.9917 - val_loss: 0.4086 - val_accuracy: 0.8667\n",
      "Epoch 271/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0367 - accuracy: 0.9750 - val_loss: 0.4608 - val_accuracy: 0.8333\n",
      "Epoch 272/1000\n",
      "120/120 [==============================] - 0s 949us/step - loss: 0.0393 - accuracy: 0.9833 - val_loss: 0.1128 - val_accuracy: 0.9667\n",
      "Epoch 273/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0567 - accuracy: 0.9667 - val_loss: 0.0902 - val_accuracy: 0.9667\n",
      "Epoch 274/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0248 - accuracy: 0.9833 - val_loss: 0.7459 - val_accuracy: 0.7000\n",
      "Epoch 275/1000\n",
      "120/120 [==============================] - 0s 945us/step - loss: 0.0233 - accuracy: 0.9917 - val_loss: 0.4299 - val_accuracy: 0.9333\n",
      "Epoch 276/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0931 - accuracy: 0.9833 - val_loss: 0.4739 - val_accuracy: 0.8000\n",
      "Epoch 277/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0266 - accuracy: 0.9917 - val_loss: 0.6257 - val_accuracy: 0.8000\n",
      "Epoch 278/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 954us/step - loss: 0.0197 - accuracy: 0.9917 - val_loss: 0.6782 - val_accuracy: 0.8333\n",
      "Epoch 279/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 0.9917 - val_loss: 8.4581 - val_accuracy: 0.2333\n",
      "Epoch 280/1000\n",
      "120/120 [==============================] - 0s 982us/step - loss: 0.1848 - accuracy: 0.9667 - val_loss: 0.5273 - val_accuracy: 0.6333\n",
      "Epoch 281/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0359 - accuracy: 0.9833 - val_loss: 0.5028 - val_accuracy: 0.7667\n",
      "Epoch 282/1000\n",
      "120/120 [==============================] - 0s 991us/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.6608 - val_accuracy: 0.7667\n",
      "Epoch 283/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.4605 - val_accuracy: 0.7667\n",
      "Epoch 284/1000\n",
      "120/120 [==============================] - 0s 943us/step - loss: 0.0204 - accuracy: 0.9917 - val_loss: 1.8161 - val_accuracy: 0.5667\n",
      "Epoch 285/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0811 - accuracy: 0.9667 - val_loss: 0.2805 - val_accuracy: 0.8333\n",
      "Epoch 286/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0401 - accuracy: 0.9833 - val_loss: 0.1777 - val_accuracy: 0.9000\n",
      "Epoch 287/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0308 - accuracy: 0.9917 - val_loss: 0.8044 - val_accuracy: 0.7000\n",
      "Epoch 288/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0413 - accuracy: 0.9917 - val_loss: 7.0538e-04 - val_accuracy: 1.0000\n",
      "Epoch 289/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.9750 - val_loss: 0.1831 - val_accuracy: 0.9333\n",
      "Epoch 290/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0301 - accuracy: 0.9833 - val_loss: 0.5664 - val_accuracy: 0.8000\n",
      "Epoch 291/1000\n",
      "120/120 [==============================] - 0s 949us/step - loss: 0.0215 - accuracy: 0.9917 - val_loss: 0.6458 - val_accuracy: 0.8000\n",
      "Epoch 292/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0555 - accuracy: 0.9750 - val_loss: 0.7264 - val_accuracy: 0.6333\n",
      "Epoch 293/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0291 - accuracy: 0.9833 - val_loss: 0.3732 - val_accuracy: 0.8333\n",
      "Epoch 294/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0185 - accuracy: 0.9917 - val_loss: 1.3649 - val_accuracy: 0.6333\n",
      "Epoch 295/1000\n",
      "120/120 [==============================] - 0s 984us/step - loss: 0.0248 - accuracy: 0.9833 - val_loss: 1.1140 - val_accuracy: 0.6333\n",
      "Epoch 296/1000\n",
      "120/120 [==============================] - 0s 932us/step - loss: 0.0650 - accuracy: 0.9750 - val_loss: 0.9109 - val_accuracy: 0.6333\n",
      "Epoch 297/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0249 - accuracy: 0.9917 - val_loss: 0.3951 - val_accuracy: 0.8667\n",
      "Epoch 298/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0330 - accuracy: 0.9833 - val_loss: 0.1329 - val_accuracy: 0.9333\n",
      "Epoch 299/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9667 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 300/1000\n",
      "120/120 [==============================] - 0s 958us/step - loss: 0.0390 - accuracy: 0.9750 - val_loss: 0.6286 - val_accuracy: 0.6333\n",
      "Epoch 301/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0309 - accuracy: 0.9917 - val_loss: 0.4351 - val_accuracy: 0.8000\n",
      "Epoch 302/1000\n",
      "120/120 [==============================] - 0s 984us/step - loss: 0.0276 - accuracy: 0.9833 - val_loss: 0.4246 - val_accuracy: 0.8000\n",
      "Epoch 303/1000\n",
      "120/120 [==============================] - 0s 941us/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.2724 - val_accuracy: 0.9333\n",
      "Epoch 304/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0289 - accuracy: 0.9833 - val_loss: 0.1406 - val_accuracy: 0.9333\n",
      "Epoch 305/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0516 - accuracy: 0.9833 - val_loss: 0.4530 - val_accuracy: 0.8333\n",
      "Epoch 306/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0174 - accuracy: 0.9917 - val_loss: 0.4016 - val_accuracy: 0.8667\n",
      "Epoch 307/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0233 - accuracy: 0.9833 - val_loss: 0.6366 - val_accuracy: 0.8000\n",
      "Epoch 308/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0152 - accuracy: 0.9917 - val_loss: 1.9779 - val_accuracy: 0.6333\n",
      "Epoch 309/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0420 - accuracy: 0.9750 - val_loss: 0.1304 - val_accuracy: 0.9333\n",
      "Epoch 310/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.8835 - val_accuracy: 0.7333\n",
      "Epoch 311/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0143 - accuracy: 0.9917 - val_loss: 2.1575 - val_accuracy: 0.6333\n",
      "Epoch 312/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9667 - val_loss: 0.5836 - val_accuracy: 0.7667\n",
      "Epoch 313/1000\n",
      "120/120 [==============================] - 0s 962us/step - loss: 0.0183 - accuracy: 0.9917 - val_loss: 0.3617 - val_accuracy: 0.9000\n",
      "Epoch 314/1000\n",
      "120/120 [==============================] - 0s 952us/step - loss: 0.0276 - accuracy: 0.9833 - val_loss: 0.8248 - val_accuracy: 0.7333\n",
      "Epoch 315/1000\n",
      "120/120 [==============================] - 0s 962us/step - loss: 0.0198 - accuracy: 0.9917 - val_loss: 1.7858 - val_accuracy: 0.6333\n",
      "Epoch 316/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 0.9833 - val_loss: 1.0722 - val_accuracy: 0.7000\n",
      "Epoch 317/1000\n",
      "120/120 [==============================] - 0s 978us/step - loss: 0.0244 - accuracy: 0.9917 - val_loss: 0.7922 - val_accuracy: 0.8000\n",
      "Epoch 318/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.2094 - val_accuracy: 0.9333\n",
      "Epoch 319/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.0250 - accuracy: 0.9833 - val_loss: 0.2649 - val_accuracy: 0.9333\n",
      "Epoch 320/1000\n",
      "120/120 [==============================] - 0s 957us/step - loss: 0.0179 - accuracy: 0.9917 - val_loss: 0.6499 - val_accuracy: 0.8000\n",
      "Epoch 321/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0176 - accuracy: 0.9917 - val_loss: 1.5481 - val_accuracy: 0.6667\n",
      "Epoch 322/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0626 - accuracy: 0.9833 - val_loss: 1.2908 - val_accuracy: 0.6333\n",
      "Epoch 323/1000\n",
      "120/120 [==============================] - 0s 967us/step - loss: 0.0363 - accuracy: 0.9833 - val_loss: 0.2262 - val_accuracy: 0.9333\n",
      "Epoch 324/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.9833 - val_loss: 0.6602 - val_accuracy: 0.8000\n",
      "Epoch 325/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0217 - accuracy: 0.9917 - val_loss: 0.9327 - val_accuracy: 0.7000\n",
      "Epoch 326/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 2.0598 - val_accuracy: 0.6333\n",
      "Epoch 327/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 0.9833 - val_loss: 0.4115 - val_accuracy: 0.9333\n",
      "Epoch 328/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.9917 - val_loss: 3.0602 - val_accuracy: 0.6000\n",
      "Epoch 329/1000\n",
      "120/120 [==============================] - 0s 992us/step - loss: 0.0361 - accuracy: 0.9833 - val_loss: 0.5930 - val_accuracy: 0.8667\n",
      "Epoch 330/1000\n",
      "120/120 [==============================] - 0s 928us/step - loss: 0.0174 - accuracy: 0.9917 - val_loss: 0.3218 - val_accuracy: 0.9333\n",
      "Epoch 331/1000\n",
      "120/120 [==============================] - 0s 920us/step - loss: 0.0182 - accuracy: 0.9917 - val_loss: 2.4365 - val_accuracy: 0.6333\n",
      "Epoch 332/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0519 - accuracy: 0.9667 - val_loss: 0.5290 - val_accuracy: 0.8000\n",
      "Epoch 333/1000\n",
      "120/120 [==============================] - 0s 907us/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 0.7164 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 334/1000\n",
      "120/120 [==============================] - 0s 984us/step - loss: 0.0216 - accuracy: 0.9917 - val_loss: 0.9910 - val_accuracy: 0.7333\n",
      "Epoch 335/1000\n",
      "120/120 [==============================] - 0s 943us/step - loss: 0.0173 - accuracy: 0.9917 - val_loss: 1.7443 - val_accuracy: 0.6333\n",
      "Epoch 336/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9833 - val_loss: 0.7604 - val_accuracy: 0.8000\n",
      "Epoch 337/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0240 - accuracy: 0.9833 - val_loss: 1.0374 - val_accuracy: 0.7667\n",
      "Epoch 338/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.0187 - accuracy: 0.9917 - val_loss: 1.0357 - val_accuracy: 0.8000\n",
      "Epoch 339/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0831 - accuracy: 0.9833 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
      "Epoch 340/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0599 - accuracy: 0.9667 - val_loss: 0.2684 - val_accuracy: 0.9333\n",
      "Epoch 341/1000\n",
      "120/120 [==============================] - 0s 973us/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 0.6374 - val_accuracy: 0.8000\n",
      "Epoch 342/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0238 - accuracy: 0.9833 - val_loss: 0.2148 - val_accuracy: 0.9333\n",
      "Epoch 343/1000\n",
      "120/120 [==============================] - 0s 967us/step - loss: 0.0192 - accuracy: 0.9917 - val_loss: 1.1792 - val_accuracy: 0.7000\n",
      "Epoch 344/1000\n",
      "120/120 [==============================] - 0s 958us/step - loss: 0.0439 - accuracy: 0.9833 - val_loss: 0.2355 - val_accuracy: 0.9333\n",
      "Epoch 345/1000\n",
      "120/120 [==============================] - 0s 983us/step - loss: 0.0195 - accuracy: 0.9917 - val_loss: 0.3912 - val_accuracy: 0.9333\n",
      "Epoch 346/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0213 - accuracy: 0.9917 - val_loss: 1.4714 - val_accuracy: 0.6667\n",
      "Epoch 347/1000\n",
      "120/120 [==============================] - 0s 958us/step - loss: 0.0196 - accuracy: 0.9833 - val_loss: 0.2590 - val_accuracy: 0.9333\n",
      "Epoch 348/1000\n",
      "120/120 [==============================] - 0s 943us/step - loss: 0.0274 - accuracy: 0.9917 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
      "Epoch 349/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0498 - accuracy: 0.9750 - val_loss: 0.5053 - val_accuracy: 0.8000\n",
      "Epoch 350/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0206 - accuracy: 0.9917 - val_loss: 0.4408 - val_accuracy: 0.8667\n",
      "Epoch 351/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 1.8094 - val_accuracy: 0.6333\n",
      "Epoch 352/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.0519 - accuracy: 0.9833 - val_loss: 0.6260 - val_accuracy: 0.7333\n",
      "Epoch 353/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0199 - accuracy: 0.9917 - val_loss: 0.3130 - val_accuracy: 0.9333\n",
      "Epoch 354/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0270 - accuracy: 0.9833 - val_loss: 0.9354 - val_accuracy: 0.7000\n",
      "Epoch 355/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.0233 - accuracy: 0.9833 - val_loss: 0.1071 - val_accuracy: 0.9667\n",
      "Epoch 356/1000\n",
      "120/120 [==============================] - 0s 934us/step - loss: 0.0504 - accuracy: 0.9833 - val_loss: 0.3803 - val_accuracy: 0.8333\n",
      "Epoch 357/1000\n",
      "120/120 [==============================] - 0s 992us/step - loss: 0.0191 - accuracy: 0.9917 - val_loss: 0.4443 - val_accuracy: 0.8333\n",
      "Epoch 358/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.9833 - val_loss: 0.7817 - val_accuracy: 0.7000\n",
      "Epoch 359/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.0153 - accuracy: 0.9917 - val_loss: 0.1934 - val_accuracy: 0.9333\n",
      "Epoch 360/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0440 - accuracy: 0.9833 - val_loss: 0.2812 - val_accuracy: 0.9333\n",
      "Epoch 361/1000\n",
      "120/120 [==============================] - 0s 950us/step - loss: 0.0289 - accuracy: 0.9833 - val_loss: 0.4448 - val_accuracy: 0.8333\n",
      "Epoch 362/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0246 - accuracy: 0.9833 - val_loss: 0.4437 - val_accuracy: 0.8333\n",
      "Epoch 363/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0228 - accuracy: 0.9917 - val_loss: 1.3605 - val_accuracy: 0.6667\n",
      "Epoch 364/1000\n",
      "120/120 [==============================] - 0s 932us/step - loss: 0.0149 - accuracy: 0.9917 - val_loss: 1.0555 - val_accuracy: 0.7333\n",
      "Epoch 365/1000\n",
      "120/120 [==============================] - 0s 942us/step - loss: 0.0272 - accuracy: 0.9917 - val_loss: 1.6793 - val_accuracy: 0.6667\n",
      "Epoch 366/1000\n",
      "120/120 [==============================] - 0s 945us/step - loss: 0.0213 - accuracy: 0.9917 - val_loss: 0.9896 - val_accuracy: 0.7667\n",
      "Epoch 367/1000\n",
      "120/120 [==============================] - 0s 942us/step - loss: 0.0181 - accuracy: 0.9917 - val_loss: 0.2249 - val_accuracy: 0.9333\n",
      "Epoch 368/1000\n",
      "120/120 [==============================] - 0s 941us/step - loss: 0.0826 - accuracy: 0.9750 - val_loss: 0.2342 - val_accuracy: 0.9333\n",
      "Epoch 369/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 0.5060 - val_accuracy: 0.8000\n",
      "Epoch 370/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0265 - accuracy: 0.9833 - val_loss: 0.5709 - val_accuracy: 0.8000\n",
      "Epoch 371/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0310 - accuracy: 0.9750 - val_loss: 0.3351 - val_accuracy: 0.8667\n",
      "Epoch 372/1000\n",
      "120/120 [==============================] - 0s 993us/step - loss: 0.0230 - accuracy: 0.9917 - val_loss: 0.3245 - val_accuracy: 0.8667\n",
      "Epoch 373/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9917 - val_loss: 0.5575 - val_accuracy: 0.8000\n",
      "Epoch 374/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0344 - accuracy: 0.9833 - val_loss: 0.8360 - val_accuracy: 0.8000\n",
      "Epoch 375/1000\n",
      "120/120 [==============================] - 0s 940us/step - loss: 0.0377 - accuracy: 0.9833 - val_loss: 0.3049 - val_accuracy: 0.9000\n",
      "Epoch 376/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0243 - accuracy: 0.9833 - val_loss: 0.5107 - val_accuracy: 0.8000\n",
      "Epoch 377/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0256 - accuracy: 0.9833 - val_loss: 0.6259 - val_accuracy: 0.8000\n",
      "Epoch 378/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9833 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
      "Epoch 379/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0469 - accuracy: 0.9750 - val_loss: 0.4350 - val_accuracy: 0.9000\n",
      "Epoch 380/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 0.9917 - val_loss: 0.3401 - val_accuracy: 0.9333\n",
      "Epoch 381/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0388 - accuracy: 0.9833 - val_loss: 2.3820 - val_accuracy: 0.6333\n",
      "Epoch 382/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0405 - accuracy: 0.9833 - val_loss: 1.8296 - val_accuracy: 0.6333\n",
      "Epoch 383/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 0.4824 - val_accuracy: 0.9333\n",
      "Epoch 384/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0235 - accuracy: 0.9833 - val_loss: 0.7441 - val_accuracy: 0.8000\n",
      "Epoch 385/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0283 - accuracy: 0.9833 - val_loss: 0.0998 - val_accuracy: 0.9667\n",
      "Epoch 386/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9833 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
      "Epoch 387/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.5210 - val_accuracy: 0.8000\n",
      "Epoch 388/1000\n",
      "120/120 [==============================] - 0s 943us/step - loss: 0.0218 - accuracy: 0.9917 - val_loss: 0.6728 - val_accuracy: 0.8000\n",
      "Epoch 389/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 975us/step - loss: 0.0216 - accuracy: 0.9917 - val_loss: 0.7127 - val_accuracy: 0.8000\n",
      "Epoch 390/1000\n",
      "120/120 [==============================] - 0s 985us/step - loss: 0.0358 - accuracy: 0.9833 - val_loss: 0.1772 - val_accuracy: 0.9333\n",
      "Epoch 391/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0207 - accuracy: 0.9917 - val_loss: 0.3599 - val_accuracy: 0.9333\n",
      "Epoch 392/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0205 - accuracy: 0.9917 - val_loss: 0.6334 - val_accuracy: 0.8000\n",
      "Epoch 393/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.9833 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
      "Epoch 394/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0366 - accuracy: 0.9833 - val_loss: 0.4929 - val_accuracy: 0.8000\n",
      "Epoch 395/1000\n",
      "120/120 [==============================] - 0s 968us/step - loss: 0.0246 - accuracy: 0.9917 - val_loss: 0.7339 - val_accuracy: 0.7333\n",
      "Epoch 396/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0227 - accuracy: 0.9917 - val_loss: 0.4237 - val_accuracy: 0.8333\n",
      "Epoch 397/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 0.9833 - val_loss: 0.4612 - val_accuracy: 0.8333\n",
      "Epoch 398/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0186 - accuracy: 0.9917 - val_loss: 1.1027 - val_accuracy: 0.6667\n",
      "Epoch 399/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0603 - accuracy: 0.9833 - val_loss: 0.0741 - val_accuracy: 0.9667\n",
      "Epoch 400/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 0.9917 - val_loss: 0.9328 - val_accuracy: 0.6333\n",
      "Epoch 401/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0347 - accuracy: 0.9750 - val_loss: 0.1821 - val_accuracy: 0.9333\n",
      "Epoch 402/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0212 - accuracy: 0.9833 - val_loss: 0.3421 - val_accuracy: 0.8333\n",
      "Epoch 403/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9917 - val_loss: 1.0576 - val_accuracy: 0.6333\n",
      "Epoch 404/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0262 - accuracy: 0.9750 - val_loss: 0.0887 - val_accuracy: 0.9667\n",
      "Epoch 405/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 0.1209 - val_accuracy: 0.9333\n",
      "Epoch 406/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.1880 - val_accuracy: 0.9333\n",
      "Epoch 407/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9750 - val_loss: 0.4939 - val_accuracy: 0.8333\n",
      "Epoch 408/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9583 - val_loss: 0.1044 - val_accuracy: 0.9667\n",
      "Epoch 409/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.9583 - val_loss: 0.0840 - val_accuracy: 0.9667\n",
      "Epoch 410/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0262 - accuracy: 0.9917 - val_loss: 0.4282 - val_accuracy: 0.8333\n",
      "Epoch 411/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.3153 - val_accuracy: 0.9333\n",
      "Epoch 412/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0218 - accuracy: 0.9917 - val_loss: 0.3133 - val_accuracy: 0.9333\n",
      "Epoch 413/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0268 - accuracy: 0.9833 - val_loss: 0.3185 - val_accuracy: 0.9333\n",
      "Epoch 414/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0243 - accuracy: 0.9833 - val_loss: 0.4265 - val_accuracy: 0.9000\n",
      "Epoch 415/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9917 - val_loss: 0.2208 - val_accuracy: 0.9333\n",
      "Epoch 416/1000\n",
      "120/120 [==============================] - 0s 988us/step - loss: 0.0765 - accuracy: 0.9750 - val_loss: 0.4188 - val_accuracy: 0.9333\n",
      "Epoch 417/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0226 - accuracy: 0.9917 - val_loss: 0.9132 - val_accuracy: 0.7333\n",
      "Epoch 418/1000\n",
      "120/120 [==============================] - 0s 1000us/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.4062 - val_accuracy: 0.9333\n",
      "Epoch 419/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0194 - accuracy: 0.9917 - val_loss: 1.0705 - val_accuracy: 0.7333\n",
      "Epoch 420/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0793 - accuracy: 0.9500 - val_loss: 0.2145 - val_accuracy: 0.9333\n",
      "Epoch 421/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0293 - accuracy: 0.9833 - val_loss: 0.4680 - val_accuracy: 0.8333\n",
      "Epoch 422/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 0.9917 - val_loss: 0.6197 - val_accuracy: 0.8000\n",
      "Epoch 423/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0189 - accuracy: 0.9917 - val_loss: 0.3630 - val_accuracy: 0.9333\n",
      "Epoch 424/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.5567 - val_accuracy: 0.8667\n",
      "Epoch 425/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0169 - accuracy: 0.9917 - val_loss: 0.7368 - val_accuracy: 0.8333\n",
      "Epoch 426/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0381 - accuracy: 0.9833 - val_loss: 0.5929 - val_accuracy: 0.8000\n",
      "Epoch 427/1000\n",
      "120/120 [==============================] - 0s 987us/step - loss: 0.0217 - accuracy: 0.9917 - val_loss: 0.9032 - val_accuracy: 0.7667\n",
      "Epoch 428/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9750 - val_loss: 0.8809 - val_accuracy: 0.6333\n",
      "Epoch 429/1000\n",
      "120/120 [==============================] - 0s 996us/step - loss: 0.0424 - accuracy: 0.9833 - val_loss: 0.4091 - val_accuracy: 0.8333\n",
      "Epoch 430/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0247 - accuracy: 0.9917 - val_loss: 0.7933 - val_accuracy: 0.7333\n",
      "Epoch 431/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 0.9917 - val_loss: 0.8300 - val_accuracy: 0.7667\n",
      "Epoch 432/1000\n",
      "120/120 [==============================] - 0s 988us/step - loss: 0.0182 - accuracy: 0.9917 - val_loss: 0.6354 - val_accuracy: 0.8333\n",
      "Epoch 433/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0433 - accuracy: 0.9833 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
      "Epoch 434/1000\n",
      "120/120 [==============================] - 0s 987us/step - loss: 0.0449 - accuracy: 0.9750 - val_loss: 0.7106 - val_accuracy: 0.7333\n",
      "Epoch 435/1000\n",
      "120/120 [==============================] - 0s 968us/step - loss: 0.0171 - accuracy: 0.9917 - val_loss: 1.7443 - val_accuracy: 0.6333\n",
      "Epoch 436/1000\n",
      "120/120 [==============================] - 0s 994us/step - loss: 0.0233 - accuracy: 0.9917 - val_loss: 0.4503 - val_accuracy: 0.9333\n",
      "Epoch 437/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0205 - accuracy: 0.9833 - val_loss: 0.6604 - val_accuracy: 0.8333\n",
      "Epoch 438/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.1146 - accuracy: 0.9750 - val_loss: 0.0203 - val_accuracy: 1.0000\n",
      "Epoch 439/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.2278 - val_accuracy: 0.9333\n",
      "Epoch 440/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0287 - accuracy: 0.9917 - val_loss: 0.3058 - val_accuracy: 0.8667\n",
      "Epoch 441/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0270 - accuracy: 0.9917 - val_loss: 0.4087 - val_accuracy: 0.8000\n",
      "Epoch 442/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 1.5693 - val_accuracy: 0.6333\n",
      "Epoch 443/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0271 - accuracy: 0.9833 - val_loss: 0.1586 - val_accuracy: 0.9333\n",
      "Epoch 444/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0297 - accuracy: 0.9917 - val_loss: 0.5103 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 445/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 1.6688 - val_accuracy: 0.6333\n",
      "Epoch 446/1000\n",
      "120/120 [==============================] - 0s 982us/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.4626 - val_accuracy: 0.9333\n",
      "Epoch 447/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0567 - accuracy: 0.9667 - val_loss: 0.2227 - val_accuracy: 0.9333\n",
      "Epoch 448/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0186 - accuracy: 0.9917 - val_loss: 0.9943 - val_accuracy: 0.7000\n",
      "Epoch 449/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0217 - accuracy: 0.9833 - val_loss: 1.8078 - val_accuracy: 0.6333\n",
      "Epoch 450/1000\n",
      "120/120 [==============================] - 0s 992us/step - loss: 0.0299 - accuracy: 0.9833 - val_loss: 0.2601 - val_accuracy: 0.9333\n",
      "Epoch 451/1000\n",
      "120/120 [==============================] - 0s 988us/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.9680 - val_accuracy: 0.7000\n",
      "Epoch 452/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0302 - accuracy: 0.9750 - val_loss: 0.4217 - val_accuracy: 0.8333\n",
      "Epoch 453/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0252 - accuracy: 0.9833 - val_loss: 0.6620 - val_accuracy: 0.8000\n",
      "Epoch 454/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 4.1569 - val_accuracy: 0.5667\n",
      "Epoch 455/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.1237 - accuracy: 0.9750 - val_loss: 0.4577 - val_accuracy: 0.8000\n",
      "Epoch 456/1000\n",
      "120/120 [==============================] - 0s 994us/step - loss: 0.0244 - accuracy: 0.9917 - val_loss: 0.5852 - val_accuracy: 0.7667\n",
      "Epoch 457/1000\n",
      "120/120 [==============================] - 0s 989us/step - loss: 0.0211 - accuracy: 0.9917 - val_loss: 0.2881 - val_accuracy: 0.9333\n",
      "Epoch 458/1000\n",
      "120/120 [==============================] - 0s 998us/step - loss: 0.0146 - accuracy: 0.9917 - val_loss: 0.2007 - val_accuracy: 0.9333\n",
      "Epoch 459/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0475 - accuracy: 0.9917 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 460/1000\n",
      "120/120 [==============================] - 0s 978us/step - loss: 0.0245 - accuracy: 0.9833 - val_loss: 1.1762 - val_accuracy: 0.6333\n",
      "Epoch 461/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0106 - accuracy: 0.9917 - val_loss: 0.2198 - val_accuracy: 0.9333\n",
      "Epoch 462/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0334 - accuracy: 0.9917 - val_loss: 0.7430 - val_accuracy: 0.7667\n",
      "Epoch 463/1000\n",
      "120/120 [==============================] - 0s 984us/step - loss: 0.0213 - accuracy: 0.9917 - val_loss: 1.4584 - val_accuracy: 0.6333\n",
      "Epoch 464/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.9917 - val_loss: 0.1834 - val_accuracy: 0.9333\n",
      "Epoch 465/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.3172 - val_accuracy: 0.9333\n",
      "Epoch 466/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0297 - accuracy: 0.9833 - val_loss: 0.3764 - val_accuracy: 0.9333\n",
      "Epoch 467/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0233 - accuracy: 0.9917 - val_loss: 2.4137 - val_accuracy: 0.6333\n",
      "Epoch 468/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0189 - accuracy: 0.9833 - val_loss: 0.1023 - val_accuracy: 0.9667\n",
      "Epoch 469/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.3803 - val_accuracy: 0.9333\n",
      "Epoch 470/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.7463 - val_accuracy: 0.8333\n",
      "Epoch 471/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0380 - accuracy: 0.9750 - val_loss: 0.2662 - val_accuracy: 0.9333\n",
      "Epoch 472/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0405 - accuracy: 0.9750 - val_loss: 0.0565 - val_accuracy: 0.9667\n",
      "Epoch 473/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0310 - accuracy: 0.9833 - val_loss: 0.4193 - val_accuracy: 0.8333\n",
      "Epoch 474/1000\n",
      "120/120 [==============================] - 0s 984us/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.4148 - val_accuracy: 0.9333\n",
      "Epoch 475/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0221 - accuracy: 0.9917 - val_loss: 0.5594 - val_accuracy: 0.8333\n",
      "Epoch 476/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0222 - accuracy: 0.9833 - val_loss: 0.4137 - val_accuracy: 0.9333\n",
      "Epoch 477/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0244 - accuracy: 0.9833 - val_loss: 0.7797 - val_accuracy: 0.8000\n",
      "Epoch 478/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0151 - accuracy: 0.9917 - val_loss: 0.5879 - val_accuracy: 0.8333\n",
      "Epoch 479/1000\n",
      "120/120 [==============================] - 0s 998us/step - loss: 0.0771 - accuracy: 0.9583 - val_loss: 0.0893 - val_accuracy: 0.9667\n",
      "Epoch 480/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0377 - accuracy: 0.9917 - val_loss: 0.2826 - val_accuracy: 0.9000\n",
      "Epoch 481/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0210 - accuracy: 0.9917 - val_loss: 0.2349 - val_accuracy: 0.9333\n",
      "Epoch 482/1000\n",
      "120/120 [==============================] - 0s 968us/step - loss: 0.0196 - accuracy: 0.9917 - val_loss: 0.5538 - val_accuracy: 0.8000\n",
      "Epoch 483/1000\n",
      "120/120 [==============================] - 0s 983us/step - loss: 0.0243 - accuracy: 0.9833 - val_loss: 0.9055 - val_accuracy: 0.7000\n",
      "Epoch 484/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.5454 - val_accuracy: 0.8000\n",
      "Epoch 485/1000\n",
      "120/120 [==============================] - 0s 988us/step - loss: 0.0214 - accuracy: 0.9917 - val_loss: 0.5240 - val_accuracy: 0.8000\n",
      "Epoch 486/1000\n",
      "120/120 [==============================] - 0s 957us/step - loss: 0.0199 - accuracy: 0.9917 - val_loss: 0.7032 - val_accuracy: 0.7667\n",
      "Epoch 487/1000\n",
      "120/120 [==============================] - 0s 988us/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.2572 - val_accuracy: 0.9333\n",
      "Epoch 488/1000\n",
      "120/120 [==============================] - 0s 987us/step - loss: 0.0243 - accuracy: 0.9833 - val_loss: 0.2037 - val_accuracy: 0.9333\n",
      "Epoch 489/1000\n",
      "120/120 [==============================] - 0s 998us/step - loss: 0.0201 - accuracy: 0.9917 - val_loss: 0.6695 - val_accuracy: 0.8000\n",
      "Epoch 490/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0199 - accuracy: 0.9917 - val_loss: 3.1475 - val_accuracy: 0.5667\n",
      "Epoch 491/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0153 - accuracy: 0.9917 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
      "Epoch 492/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9667 - val_loss: 0.0658 - val_accuracy: 0.9667\n",
      "Epoch 493/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 0.9833 - val_loss: 0.5409 - val_accuracy: 0.8000\n",
      "Epoch 494/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 0.9000\n",
      "Epoch 495/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0257 - accuracy: 0.9917 - val_loss: 0.2769 - val_accuracy: 0.9333\n",
      "Epoch 496/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9833 - val_loss: 0.2057 - val_accuracy: 0.9333\n",
      "Epoch 497/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 0.9917 - val_loss: 0.7741 - val_accuracy: 0.6667\n",
      "Epoch 498/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0232 - accuracy: 0.9833 - val_loss: 0.5327 - val_accuracy: 0.7667\n",
      "Epoch 499/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9833 - val_loss: 0.7430 - val_accuracy: 0.7000\n",
      "Epoch 500/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0396 - accuracy: 0.9833 - val_loss: 0.2356 - val_accuracy: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 501/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 0.9750 - val_loss: 0.4195 - val_accuracy: 0.8000\n",
      "Epoch 502/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 0.4247 - val_accuracy: 0.8333\n",
      "Epoch 503/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0479 - accuracy: 0.9833 - val_loss: 0.5789 - val_accuracy: 0.7333\n",
      "Epoch 504/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0215 - accuracy: 0.9917 - val_loss: 0.5259 - val_accuracy: 0.7333\n",
      "Epoch 505/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0217 - accuracy: 0.9917 - val_loss: 0.4634 - val_accuracy: 0.8000\n",
      "Epoch 506/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0169 - accuracy: 0.9917 - val_loss: 0.2986 - val_accuracy: 0.9333\n",
      "Epoch 507/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0590 - accuracy: 0.9667 - val_loss: 0.3431 - val_accuracy: 0.8333\n",
      "Epoch 508/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.9917 - val_loss: 0.8449 - val_accuracy: 0.7333\n",
      "Epoch 509/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9750 - val_loss: 0.1576 - val_accuracy: 0.9333\n",
      "Epoch 510/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0206 - accuracy: 0.9917 - val_loss: 0.0917 - val_accuracy: 0.9667\n",
      "Epoch 511/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0315 - accuracy: 0.9833 - val_loss: 0.8307 - val_accuracy: 0.7000\n",
      "Epoch 512/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0462 - accuracy: 0.9750 - val_loss: 0.9828 - val_accuracy: 0.6333\n",
      "Epoch 513/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0328 - accuracy: 0.9917 - val_loss: 0.4837 - val_accuracy: 0.7667\n",
      "Epoch 514/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 1.6720 - val_accuracy: 0.6333\n",
      "Epoch 515/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0267 - accuracy: 0.9917 - val_loss: 0.7025 - val_accuracy: 0.7667\n",
      "Epoch 516/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.9833 - val_loss: 0.4394 - val_accuracy: 0.8667\n",
      "Epoch 517/1000\n",
      "120/120 [==============================] - 0s 975us/step - loss: 0.0285 - accuracy: 0.9833 - val_loss: 0.4104 - val_accuracy: 0.8667\n",
      "Epoch 518/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0191 - accuracy: 0.9917 - val_loss: 0.6741 - val_accuracy: 0.7667\n",
      "Epoch 519/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.5424 - val_accuracy: 0.8333\n",
      "Epoch 520/1000\n",
      "120/120 [==============================] - 0s 994us/step - loss: 0.0290 - accuracy: 0.9833 - val_loss: 0.2756 - val_accuracy: 0.9333\n",
      "Epoch 521/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0589 - accuracy: 0.9917 - val_loss: 0.2845 - val_accuracy: 0.8667\n",
      "Epoch 522/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.7813 - val_accuracy: 0.7000\n",
      "Epoch 523/1000\n",
      "120/120 [==============================] - 0s 987us/step - loss: 0.0234 - accuracy: 0.9833 - val_loss: 1.1139 - val_accuracy: 0.6333\n",
      "Epoch 524/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0214 - accuracy: 0.9833 - val_loss: 0.6448 - val_accuracy: 0.8000\n",
      "Epoch 525/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0442 - accuracy: 0.9750 - val_loss: 0.3486 - val_accuracy: 0.8000\n",
      "Epoch 526/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0235 - accuracy: 0.9917 - val_loss: 0.9448 - val_accuracy: 0.6333\n",
      "Epoch 527/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0378 - accuracy: 0.9833 - val_loss: 0.2664 - val_accuracy: 0.9000\n",
      "Epoch 528/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.9917 - val_loss: 0.1958 - val_accuracy: 0.9333\n",
      "Epoch 529/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0333 - accuracy: 0.9750 - val_loss: 0.2023 - val_accuracy: 0.9333\n",
      "Epoch 530/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.6002 - accuracy: 0.9417 - val_loss: 0.4178 - val_accuracy: 0.7667\n",
      "Epoch 531/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 0.9917 - val_loss: 0.3126 - val_accuracy: 0.8667\n",
      "Epoch 532/1000\n",
      "120/120 [==============================] - 0s 989us/step - loss: 0.0260 - accuracy: 0.9917 - val_loss: 0.4337 - val_accuracy: 0.8000\n",
      "Epoch 533/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0204 - accuracy: 0.9917 - val_loss: 0.4970 - val_accuracy: 0.8000\n",
      "Epoch 534/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 0.9917 - val_loss: 0.5312 - val_accuracy: 0.8000\n",
      "Epoch 535/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 1.9923 - val_accuracy: 0.6000\n",
      "Epoch 536/1000\n",
      "120/120 [==============================] - 0s 993us/step - loss: 0.0358 - accuracy: 0.9750 - val_loss: 0.9305 - val_accuracy: 0.6667\n",
      "Epoch 537/1000\n",
      "120/120 [==============================] - 0s 980us/step - loss: 0.0288 - accuracy: 0.9833 - val_loss: 0.2939 - val_accuracy: 0.9000\n",
      "Epoch 538/1000\n",
      "120/120 [==============================] - 0s 957us/step - loss: 0.0170 - accuracy: 0.9917 - val_loss: 0.2450 - val_accuracy: 0.9333\n",
      "Epoch 539/1000\n",
      "120/120 [==============================] - 0s 991us/step - loss: 0.0193 - accuracy: 0.9917 - val_loss: 0.6613 - val_accuracy: 0.7667\n",
      "Epoch 540/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0131 - accuracy: 0.9917 - val_loss: 0.0869 - val_accuracy: 0.9667\n",
      "Epoch 541/1000\n",
      "120/120 [==============================] - 0s 994us/step - loss: 0.0384 - accuracy: 0.9833 - val_loss: 0.3263 - val_accuracy: 0.9333\n",
      "Epoch 542/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0158 - accuracy: 0.9917 - val_loss: 0.6951 - val_accuracy: 0.7667\n",
      "Epoch 543/1000\n",
      "120/120 [==============================] - 0s 968us/step - loss: 0.0198 - accuracy: 0.9917 - val_loss: 0.3891 - val_accuracy: 0.9000\n",
      "Epoch 544/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0170 - accuracy: 0.9917 - val_loss: 0.6619 - val_accuracy: 0.7667\n",
      "Epoch 545/1000\n",
      "120/120 [==============================] - 0s 973us/step - loss: 0.0304 - accuracy: 0.9833 - val_loss: 0.3137 - val_accuracy: 0.9333\n",
      "Epoch 546/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0323 - accuracy: 0.9833 - val_loss: 0.5128 - val_accuracy: 0.8000\n",
      "Epoch 547/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0168 - accuracy: 0.9917 - val_loss: 0.1474 - val_accuracy: 0.9333\n",
      "Epoch 548/1000\n",
      "120/120 [==============================] - 0s 990us/step - loss: 0.0345 - accuracy: 0.9750 - val_loss: 0.3089 - val_accuracy: 0.9000\n",
      "Epoch 549/1000\n",
      "120/120 [==============================] - 0s 967us/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.5710 - val_accuracy: 0.8000\n",
      "Epoch 550/1000\n",
      "120/120 [==============================] - 0s 987us/step - loss: 0.0264 - accuracy: 0.9833 - val_loss: 0.5757 - val_accuracy: 0.8000\n",
      "Epoch 551/1000\n",
      "120/120 [==============================] - 0s 983us/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 1.5865 - val_accuracy: 0.6333\n",
      "Epoch 552/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2131 - val_accuracy: 0.9333\n",
      "Epoch 553/1000\n",
      "120/120 [==============================] - 0s 995us/step - loss: 0.0406 - accuracy: 0.9667 - val_loss: 0.3949 - val_accuracy: 0.9000\n",
      "Epoch 554/1000\n",
      "120/120 [==============================] - 0s 995us/step - loss: 0.0210 - accuracy: 0.9917 - val_loss: 0.3589 - val_accuracy: 0.9000\n",
      "Epoch 555/1000\n",
      "120/120 [==============================] - 0s 975us/step - loss: 0.0112 - accuracy: 0.9917 - val_loss: 2.2419 - val_accuracy: 0.6333\n",
      "Epoch 556/1000\n",
      "120/120 [==============================] - 0s 998us/step - loss: 0.0624 - accuracy: 0.9917 - val_loss: 0.4409 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 557/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0121 - accuracy: 0.9917 - val_loss: 1.8953 - val_accuracy: 0.6000\n",
      "Epoch 558/1000\n",
      "120/120 [==============================] - 0s 990us/step - loss: 0.0642 - accuracy: 0.9917 - val_loss: 2.3212 - val_accuracy: 0.5667\n",
      "Epoch 559/1000\n",
      "120/120 [==============================] - 0s 959us/step - loss: 0.0389 - accuracy: 0.9833 - val_loss: 0.3345 - val_accuracy: 0.9000\n",
      "Epoch 560/1000\n",
      "120/120 [==============================] - 0s 989us/step - loss: 0.0174 - accuracy: 0.9917 - val_loss: 0.2872 - val_accuracy: 0.9333\n",
      "Epoch 561/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0204 - accuracy: 0.9833 - val_loss: 0.0984 - val_accuracy: 0.9667\n",
      "Epoch 562/1000\n",
      "120/120 [==============================] - 0s 967us/step - loss: 0.0178 - accuracy: 0.9917 - val_loss: 1.0904 - val_accuracy: 0.6333\n",
      "Epoch 563/1000\n",
      "120/120 [==============================] - 0s 975us/step - loss: 0.0457 - accuracy: 0.9750 - val_loss: 0.3106 - val_accuracy: 0.9000\n",
      "Epoch 564/1000\n",
      "120/120 [==============================] - 0s 987us/step - loss: 0.0178 - accuracy: 0.9833 - val_loss: 0.3107 - val_accuracy: 0.9333\n",
      "Epoch 565/1000\n",
      "120/120 [==============================] - 0s 982us/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.3226 - val_accuracy: 0.9333\n",
      "Epoch 566/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0182 - accuracy: 0.9917 - val_loss: 0.9929 - val_accuracy: 0.7000\n",
      "Epoch 567/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.0265 - accuracy: 0.9833 - val_loss: 0.4841 - val_accuracy: 0.8333\n",
      "Epoch 568/1000\n",
      "120/120 [==============================] - 0s 958us/step - loss: 0.0162 - accuracy: 0.9917 - val_loss: 1.0743 - val_accuracy: 0.7000\n",
      "Epoch 569/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0254 - accuracy: 0.9750 - val_loss: 1.0755 - val_accuracy: 0.7000\n",
      "Epoch 570/1000\n",
      "120/120 [==============================] - 0s 988us/step - loss: 0.0327 - accuracy: 0.9833 - val_loss: 0.9124 - val_accuracy: 0.7000\n",
      "Epoch 571/1000\n",
      "120/120 [==============================] - 0s 998us/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.9439 - val_accuracy: 0.7667\n",
      "Epoch 572/1000\n",
      "120/120 [==============================] - 0s 946us/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 0.5674 - val_accuracy: 0.8333\n",
      "Epoch 573/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0466 - accuracy: 0.9750 - val_loss: 0.3292 - val_accuracy: 0.9333\n",
      "Epoch 574/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0174 - accuracy: 0.9917 - val_loss: 0.4679 - val_accuracy: 0.8667\n",
      "Epoch 575/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0215 - accuracy: 0.9917 - val_loss: 0.2552 - val_accuracy: 0.9333\n",
      "Epoch 576/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.5167 - val_accuracy: 0.8333\n",
      "Epoch 577/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 0.2985 - val_accuracy: 0.9333\n",
      "Epoch 578/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0189 - accuracy: 0.9917 - val_loss: 0.3591 - val_accuracy: 0.9000\n",
      "Epoch 579/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9917 - val_loss: 0.6913 - val_accuracy: 0.8000\n",
      "Epoch 580/1000\n",
      "120/120 [==============================] - 0s 978us/step - loss: 0.0156 - accuracy: 0.9917 - val_loss: 1.9818 - val_accuracy: 0.6333\n",
      "Epoch 581/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.5862 - val_accuracy: 0.8000\n",
      "Epoch 582/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0175 - accuracy: 0.9917 - val_loss: 1.0669 - val_accuracy: 0.7000\n",
      "Epoch 583/1000\n",
      "120/120 [==============================] - 0s 949us/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 0.6513 - val_accuracy: 0.8000\n",
      "Epoch 584/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0222 - accuracy: 0.9917 - val_loss: 0.1462 - val_accuracy: 0.9667\n",
      "Epoch 585/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0410 - accuracy: 0.9750 - val_loss: 0.3486 - val_accuracy: 0.9333\n",
      "Epoch 586/1000\n",
      "120/120 [==============================] - 0s 965us/step - loss: 0.0385 - accuracy: 0.9833 - val_loss: 0.2268 - val_accuracy: 0.9333\n",
      "Epoch 587/1000\n",
      "120/120 [==============================] - 0s 983us/step - loss: 0.0206 - accuracy: 0.9917 - val_loss: 0.4683 - val_accuracy: 0.8000\n",
      "Epoch 588/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0138 - accuracy: 0.9917 - val_loss: 0.3601 - val_accuracy: 0.9000\n",
      "Epoch 589/1000\n",
      "120/120 [==============================] - 0s 992us/step - loss: 0.0204 - accuracy: 0.9917 - val_loss: 0.4180 - val_accuracy: 0.9000\n",
      "Epoch 590/1000\n",
      "120/120 [==============================] - 0s 978us/step - loss: 0.0191 - accuracy: 0.9917 - val_loss: 0.9188 - val_accuracy: 0.7667\n",
      "Epoch 591/1000\n",
      "120/120 [==============================] - 0s 974us/step - loss: 0.0238 - accuracy: 0.9917 - val_loss: 1.6091 - val_accuracy: 0.6333\n",
      "Epoch 592/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0705 - accuracy: 0.9667 - val_loss: 0.0975 - val_accuracy: 0.9667\n",
      "Epoch 593/1000\n",
      "120/120 [==============================] - 0s 968us/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.4552 - val_accuracy: 0.8000\n",
      "Epoch 594/1000\n",
      "120/120 [==============================] - 0s 992us/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.4731 - val_accuracy: 0.8667\n",
      "Epoch 595/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0247 - accuracy: 0.9833 - val_loss: 0.5395 - val_accuracy: 0.8000\n",
      "Epoch 596/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 0.9917 - val_loss: 0.5279 - val_accuracy: 0.8667\n",
      "Epoch 597/1000\n",
      "120/120 [==============================] - 0s 999us/step - loss: 0.0118 - accuracy: 0.9917 - val_loss: 1.7475 - val_accuracy: 0.6333\n",
      "Epoch 598/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0403 - accuracy: 0.9833 - val_loss: 0.5675 - val_accuracy: 0.8000\n",
      "Epoch 599/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0178 - accuracy: 0.9917 - val_loss: 0.3843 - val_accuracy: 0.9333\n",
      "Epoch 600/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 0.9917 - val_loss: 0.3560 - val_accuracy: 0.9333\n",
      "Epoch 601/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 1.4366 - val_accuracy: 0.7000\n",
      "Epoch 602/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0273 - accuracy: 0.9833 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 603/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0355 - accuracy: 0.9833 - val_loss: 0.5634 - val_accuracy: 0.7667\n",
      "Epoch 604/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0163 - accuracy: 0.9917 - val_loss: 0.2750 - val_accuracy: 0.9333\n",
      "Epoch 605/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0351 - accuracy: 0.9833 - val_loss: 0.3365 - val_accuracy: 0.9000\n",
      "Epoch 606/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0139 - accuracy: 0.9917 - val_loss: 0.5545 - val_accuracy: 0.8000\n",
      "Epoch 607/1000\n",
      "120/120 [==============================] - 0s 989us/step - loss: 0.0190 - accuracy: 0.9833 - val_loss: 0.8467 - val_accuracy: 0.7667\n",
      "Epoch 608/1000\n",
      "120/120 [==============================] - 0s 984us/step - loss: 0.0216 - accuracy: 0.9917 - val_loss: 0.7448 - val_accuracy: 0.7667\n",
      "Epoch 609/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0312 - accuracy: 0.9833 - val_loss: 0.3962 - val_accuracy: 0.9000\n",
      "Epoch 610/1000\n",
      "120/120 [==============================] - 0s 990us/step - loss: 0.0110 - accuracy: 0.9917 - val_loss: 1.0214 - val_accuracy: 0.7333\n",
      "Epoch 611/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.9917 - val_loss: 0.7406 - val_accuracy: 0.7667\n",
      "Epoch 612/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0176 - accuracy: 0.9917 - val_loss: 0.6405 - val_accuracy: 0.8667\n",
      "Epoch 613/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0187 - accuracy: 0.9917 - val_loss: 1.8962 - val_accuracy: 0.6667\n",
      "Epoch 614/1000\n",
      "120/120 [==============================] - 0s 984us/step - loss: 0.0199 - accuracy: 0.9917 - val_loss: 1.3128 - val_accuracy: 0.7667\n",
      "Epoch 615/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.9667 - val_loss: 3.0804 - val_accuracy: 0.6333\n",
      "Epoch 616/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9667 - val_loss: 0.3627 - val_accuracy: 0.8667\n",
      "Epoch 617/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.5464 - val_accuracy: 0.7667\n",
      "Epoch 618/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0158 - accuracy: 0.9917 - val_loss: 0.3223 - val_accuracy: 0.9000\n",
      "Epoch 619/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0164 - accuracy: 0.9917 - val_loss: 0.5694 - val_accuracy: 0.8000\n",
      "Epoch 620/1000\n",
      "120/120 [==============================] - 0s 988us/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 1.1958 - val_accuracy: 0.7000\n",
      "Epoch 621/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0122 - accuracy: 0.9917 - val_loss: 0.3752 - val_accuracy: 0.9333\n",
      "Epoch 622/1000\n",
      "120/120 [==============================] - 0s 975us/step - loss: 0.0189 - accuracy: 0.9917 - val_loss: 0.3564 - val_accuracy: 0.9333\n",
      "Epoch 623/1000\n",
      "120/120 [==============================] - 0s 975us/step - loss: 0.0247 - accuracy: 0.9917 - val_loss: 0.9836 - val_accuracy: 0.7667\n",
      "Epoch 624/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 0.9917 - val_loss: 3.0152 - val_accuracy: 0.6333\n",
      "Epoch 625/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9750 - val_loss: 0.7319 - val_accuracy: 0.8000\n",
      "Epoch 626/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0155 - accuracy: 0.9917 - val_loss: 0.9115 - val_accuracy: 0.7667\n",
      "Epoch 627/1000\n",
      "120/120 [==============================] - 0s 995us/step - loss: 0.0133 - accuracy: 0.9917 - val_loss: 0.4776 - val_accuracy: 0.9000\n",
      "Epoch 628/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0208 - accuracy: 0.9833 - val_loss: 0.5876 - val_accuracy: 0.8667\n",
      "Epoch 629/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 0.9917 - val_loss: 0.4316 - val_accuracy: 0.9333\n",
      "Epoch 630/1000\n",
      "120/120 [==============================] - 0s 982us/step - loss: 0.0209 - accuracy: 0.9917 - val_loss: 0.4927 - val_accuracy: 0.9000\n",
      "Epoch 631/1000\n",
      "120/120 [==============================] - 0s 987us/step - loss: 0.0151 - accuracy: 0.9917 - val_loss: 0.4581 - val_accuracy: 0.9333\n",
      "Epoch 632/1000\n",
      "120/120 [==============================] - 0s 965us/step - loss: 0.0142 - accuracy: 0.9917 - val_loss: 0.3783 - val_accuracy: 0.9333\n",
      "Epoch 633/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.9917 - val_loss: 1.0228 - val_accuracy: 0.7667\n",
      "Epoch 634/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0167 - accuracy: 0.9917 - val_loss: 1.2149 - val_accuracy: 0.7667\n",
      "Epoch 635/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.4601 - val_accuracy: 0.8667\n",
      "Epoch 636/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0160 - accuracy: 0.9917 - val_loss: 0.8018 - val_accuracy: 0.7667\n",
      "Epoch 637/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.7230 - val_accuracy: 0.8000\n",
      "Epoch 638/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0205 - accuracy: 0.9833 - val_loss: 1.7673 - val_accuracy: 0.6333\n",
      "Epoch 639/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0128 - accuracy: 0.9917 - val_loss: 0.4318 - val_accuracy: 0.9333\n",
      "Epoch 640/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0112 - accuracy: 0.9917 - val_loss: 0.9086 - val_accuracy: 0.8333\n",
      "Epoch 641/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0212 - accuracy: 0.9833 - val_loss: 0.2596 - val_accuracy: 0.9333\n",
      "Epoch 642/1000\n",
      "120/120 [==============================] - 0s 996us/step - loss: 0.0323 - accuracy: 0.9917 - val_loss: 0.6089 - val_accuracy: 0.9000\n",
      "Epoch 643/1000\n",
      "120/120 [==============================] - 0s 959us/step - loss: 0.0279 - accuracy: 0.9750 - val_loss: 0.4813 - val_accuracy: 0.9333\n",
      "Epoch 644/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.1013 - accuracy: 0.9833 - val_loss: 0.1375 - val_accuracy: 0.9667\n",
      "Epoch 645/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 1.0920 - val_accuracy: 0.7667\n",
      "Epoch 646/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0185 - accuracy: 0.9917 - val_loss: 3.8363 - val_accuracy: 0.6000\n",
      "Epoch 647/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0212 - accuracy: 0.9833 - val_loss: 0.0317 - val_accuracy: 0.9667\n",
      "Epoch 648/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9500 - val_loss: 0.1000 - val_accuracy: 0.9667\n",
      "Epoch 649/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0312 - accuracy: 0.9917 - val_loss: 0.2869 - val_accuracy: 0.8667\n",
      "Epoch 650/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0239 - accuracy: 0.9917 - val_loss: 0.5936 - val_accuracy: 0.7667\n",
      "Epoch 651/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0206 - accuracy: 0.9917 - val_loss: 0.3960 - val_accuracy: 0.8667\n",
      "Epoch 652/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0206 - accuracy: 0.9917 - val_loss: 0.9181 - val_accuracy: 0.7667\n",
      "Epoch 653/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0139 - accuracy: 0.9917 - val_loss: 1.2154 - val_accuracy: 0.7333\n",
      "Epoch 654/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0290 - accuracy: 0.9833 - val_loss: 1.2358 - val_accuracy: 0.7000\n",
      "Epoch 655/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 1.0117 - val_accuracy: 0.7667\n",
      "Epoch 656/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0426 - accuracy: 0.9833 - val_loss: 0.0864 - val_accuracy: 0.9667\n",
      "Epoch 657/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0273 - accuracy: 0.9917 - val_loss: 0.9535 - val_accuracy: 0.7667\n",
      "Epoch 658/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9917 - val_loss: 2.8041 - val_accuracy: 0.6333\n",
      "Epoch 659/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0230 - accuracy: 0.9917 - val_loss: 0.4552 - val_accuracy: 0.9000\n",
      "Epoch 660/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0154 - accuracy: 0.9917 - val_loss: 1.0121 - val_accuracy: 0.7667\n",
      "Epoch 661/1000\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9855   - 0s 973us/step - loss: 0.0290 - accuracy: 0.9833 - val_loss: 0.4524 - val_accuracy: 0.9333\n",
      "Epoch 662/1000\n",
      "120/120 [==============================] - 0s 995us/step - loss: 0.0179 - accuracy: 0.9917 - val_loss: 0.8096 - val_accuracy: 0.8333\n",
      "Epoch 663/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0164 - accuracy: 0.9917 - val_loss: 1.4679 - val_accuracy: 0.7333\n",
      "Epoch 664/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0494 - accuracy: 0.9750 - val_loss: 0.1958 - val_accuracy: 0.9333\n",
      "Epoch 665/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0254 - accuracy: 0.9917 - val_loss: 0.5822 - val_accuracy: 0.8667\n",
      "Epoch 666/1000\n",
      "120/120 [==============================] - 0s 980us/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 2.9818 - val_accuracy: 0.6333\n",
      "Epoch 667/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 982us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.4468 - val_accuracy: 0.9000\n",
      "Epoch 668/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0181 - accuracy: 0.9917 - val_loss: 0.3916 - val_accuracy: 0.9333\n",
      "Epoch 669/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0178 - accuracy: 0.9917 - val_loss: 0.5766 - val_accuracy: 0.9000\n",
      "Epoch 670/1000\n",
      "120/120 [==============================] - 0s 985us/step - loss: 0.0183 - accuracy: 0.9917 - val_loss: 0.3490 - val_accuracy: 0.9333\n",
      "Epoch 671/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0141 - accuracy: 0.9917 - val_loss: 0.8856 - val_accuracy: 0.8333\n",
      "Epoch 672/1000\n",
      "120/120 [==============================] - 0s 988us/step - loss: 0.0275 - accuracy: 0.9833 - val_loss: 0.0199 - val_accuracy: 1.0000\n",
      "Epoch 673/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0418 - accuracy: 0.9833 - val_loss: 1.0126 - val_accuracy: 0.7333\n",
      "Epoch 674/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 0.9917 - val_loss: 1.0795 - val_accuracy: 0.7667\n",
      "Epoch 675/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 2.6525 - val_accuracy: 0.6333\n",
      "Epoch 676/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0335 - accuracy: 0.9750 - val_loss: 0.2649 - val_accuracy: 0.9333\n",
      "Epoch 677/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0252 - accuracy: 0.9833 - val_loss: 1.4921 - val_accuracy: 0.7000\n",
      "Epoch 678/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9833 - val_loss: 0.4763 - val_accuracy: 0.9333\n",
      "Epoch 679/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0260 - accuracy: 0.9833 - val_loss: 0.4592 - val_accuracy: 0.9333\n",
      "Epoch 680/1000\n",
      "120/120 [==============================] - 0s 974us/step - loss: 0.0327 - accuracy: 0.9750 - val_loss: 0.0440 - val_accuracy: 0.9667\n",
      "Epoch 681/1000\n",
      "120/120 [==============================] - 0s 967us/step - loss: 0.0608 - accuracy: 0.9917 - val_loss: 0.4782 - val_accuracy: 0.8667\n",
      "Epoch 682/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.0215 - accuracy: 0.9917 - val_loss: 1.5086 - val_accuracy: 0.6333\n",
      "Epoch 683/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0308 - accuracy: 0.9750 - val_loss: 1.3218 - val_accuracy: 0.6667\n",
      "Epoch 684/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0254 - accuracy: 0.9833 - val_loss: 0.9301 - val_accuracy: 0.7667\n",
      "Epoch 685/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.8277 - val_accuracy: 0.8000\n",
      "Epoch 686/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0249 - accuracy: 0.9833 - val_loss: 0.3193 - val_accuracy: 0.9333\n",
      "Epoch 687/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0164 - accuracy: 0.9917 - val_loss: 2.0183 - val_accuracy: 0.6333\n",
      "Epoch 688/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0340 - accuracy: 0.9750 - val_loss: 0.7923 - val_accuracy: 0.7667\n",
      "Epoch 689/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0255 - accuracy: 0.9833 - val_loss: 0.4960 - val_accuracy: 0.9000\n",
      "Epoch 690/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0206 - accuracy: 0.9833 - val_loss: 0.3951 - val_accuracy: 0.9333\n",
      "Epoch 691/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0138 - accuracy: 0.9917 - val_loss: 0.6120 - val_accuracy: 0.9333\n",
      "Epoch 692/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.9774 - val_accuracy: 0.9000\n",
      "Epoch 693/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9750 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
      "Epoch 694/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0514 - accuracy: 0.9750 - val_loss: 0.4824 - val_accuracy: 0.7667\n",
      "Epoch 695/1000\n",
      "120/120 [==============================] - 0s 985us/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.4858 - val_accuracy: 0.8333\n",
      "Epoch 696/1000\n",
      "120/120 [==============================] - 0s 952us/step - loss: 0.0135 - accuracy: 0.9917 - val_loss: 0.6767 - val_accuracy: 0.9000\n",
      "Epoch 697/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
      "Epoch 698/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0780 - accuracy: 0.9833 - val_loss: 0.2161 - val_accuracy: 0.9000\n",
      "Epoch 699/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0226 - accuracy: 0.9917 - val_loss: 0.9380 - val_accuracy: 0.6333\n",
      "Epoch 700/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0212 - accuracy: 0.9833 - val_loss: 0.3560 - val_accuracy: 0.9000\n",
      "Epoch 701/1000\n",
      "120/120 [==============================] - 0s 980us/step - loss: 0.0185 - accuracy: 0.9833 - val_loss: 0.8004 - val_accuracy: 0.7667\n",
      "Epoch 702/1000\n",
      "120/120 [==============================] - 0s 985us/step - loss: 0.0471 - accuracy: 0.9833 - val_loss: 0.4807 - val_accuracy: 0.8333\n",
      "Epoch 703/1000\n",
      "120/120 [==============================] - 0s 983us/step - loss: 0.0170 - accuracy: 0.9917 - val_loss: 0.4585 - val_accuracy: 0.8667\n",
      "Epoch 704/1000\n",
      "120/120 [==============================] - 0s 959us/step - loss: 0.0371 - accuracy: 0.9833 - val_loss: 0.8927 - val_accuracy: 0.7333\n",
      "Epoch 705/1000\n",
      "120/120 [==============================] - 0s 999us/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 1.4737 - val_accuracy: 0.6333\n",
      "Epoch 706/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0205 - accuracy: 0.9917 - val_loss: 0.3291 - val_accuracy: 0.9000\n",
      "Epoch 707/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0103 - accuracy: 0.9917 - val_loss: 2.7687 - val_accuracy: 0.6333\n",
      "Epoch 708/1000\n",
      "120/120 [==============================] - 0s 991us/step - loss: 0.0254 - accuracy: 0.9917 - val_loss: 0.4643 - val_accuracy: 0.8667\n",
      "Epoch 709/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0185 - accuracy: 0.9833 - val_loss: 0.8134 - val_accuracy: 0.7667\n",
      "Epoch 710/1000\n",
      "120/120 [==============================] - 0s 992us/step - loss: 0.0259 - accuracy: 0.9750 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
      "Epoch 711/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0334 - accuracy: 0.9833 - val_loss: 3.0092 - val_accuracy: 0.6333\n",
      "Epoch 712/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9750 - val_loss: 0.1257 - val_accuracy: 0.9667\n",
      "Epoch 713/1000\n",
      "120/120 [==============================] - 0s 999us/step - loss: 0.0243 - accuracy: 0.9917 - val_loss: 0.3526 - val_accuracy: 0.8333\n",
      "Epoch 714/1000\n",
      "120/120 [==============================] - 0s 992us/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.2987 - val_accuracy: 0.8667\n",
      "Epoch 715/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0183 - accuracy: 0.9917 - val_loss: 1.6279 - val_accuracy: 0.6333\n",
      "Epoch 716/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0145 - accuracy: 0.9917 - val_loss: 0.1806 - val_accuracy: 0.9333\n",
      "Epoch 717/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0410 - accuracy: 0.9833 - val_loss: 1.0132 - val_accuracy: 0.6333\n",
      "Epoch 718/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0241 - accuracy: 0.9833 - val_loss: 0.4281 - val_accuracy: 0.7667\n",
      "Epoch 719/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0178 - accuracy: 0.9917 - val_loss: 0.1683 - val_accuracy: 0.9333\n",
      "Epoch 720/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0437 - accuracy: 0.9917 - val_loss: 0.2961 - val_accuracy: 0.9000\n",
      "Epoch 721/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0314 - accuracy: 0.9833 - val_loss: 1.0736 - val_accuracy: 0.6333\n",
      "Epoch 722/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0136 - accuracy: 0.9917 - val_loss: 0.3609 - val_accuracy: 0.8667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 723/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0239 - accuracy: 0.9833 - val_loss: 0.5778 - val_accuracy: 0.7667\n",
      "Epoch 724/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0197 - accuracy: 0.9917 - val_loss: 0.5628 - val_accuracy: 0.7667\n",
      "Epoch 725/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0275 - accuracy: 0.9917 - val_loss: 0.2294 - val_accuracy: 0.9333\n",
      "Epoch 726/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0088 - accuracy: 0.9917 - val_loss: 2.3341 - val_accuracy: 0.6333\n",
      "Epoch 727/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0133 - accuracy: 0.9917 - val_loss: 0.3777 - val_accuracy: 0.8667\n",
      "Epoch 728/1000\n",
      "120/120 [==============================] - 0s 968us/step - loss: 0.0219 - accuracy: 0.9833 - val_loss: 0.3566 - val_accuracy: 0.9000\n",
      "Epoch 729/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0289 - accuracy: 0.9750 - val_loss: 0.5292 - val_accuracy: 0.8667\n",
      "Epoch 730/1000\n",
      "120/120 [==============================] - 0s 973us/step - loss: 0.0170 - accuracy: 0.9917 - val_loss: 0.3212 - val_accuracy: 0.9333\n",
      "Epoch 731/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.3454 - val_accuracy: 0.9000\n",
      "Epoch 732/1000\n",
      "120/120 [==============================] - 0s 975us/step - loss: 0.0125 - accuracy: 0.9917 - val_loss: 2.6753 - val_accuracy: 0.6333\n",
      "Epoch 733/1000\n",
      "120/120 [==============================] - 0s 982us/step - loss: 0.0678 - accuracy: 0.9833 - val_loss: 1.4209 - val_accuracy: 0.6333\n",
      "Epoch 734/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0169 - accuracy: 0.9917 - val_loss: 0.2156 - val_accuracy: 0.9333\n",
      "Epoch 735/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.8232 - val_accuracy: 0.7333\n",
      "Epoch 736/1000\n",
      "120/120 [==============================] - 0s 949us/step - loss: 0.0148 - accuracy: 0.9917 - val_loss: 0.4038 - val_accuracy: 0.9000\n",
      "Epoch 737/1000\n",
      "120/120 [==============================] - 0s 959us/step - loss: 0.0108 - accuracy: 0.9917 - val_loss: 1.0985 - val_accuracy: 0.7333\n",
      "Epoch 738/1000\n",
      "120/120 [==============================] - 0s 990us/step - loss: 0.0341 - accuracy: 0.9833 - val_loss: 0.9393 - val_accuracy: 0.7333\n",
      "Epoch 739/1000\n",
      "120/120 [==============================] - 0s 962us/step - loss: 0.0099 - accuracy: 0.9917 - val_loss: 0.2315 - val_accuracy: 0.9333\n",
      "Epoch 740/1000\n",
      "120/120 [==============================] - 0s 951us/step - loss: 0.0272 - accuracy: 0.9917 - val_loss: 0.0398 - val_accuracy: 0.9667\n",
      "Epoch 741/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0338 - accuracy: 0.9750 - val_loss: 0.5355 - val_accuracy: 0.8333\n",
      "Epoch 742/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0211 - accuracy: 0.9833 - val_loss: 0.2050 - val_accuracy: 0.9333\n",
      "Epoch 743/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0430 - accuracy: 0.9833 - val_loss: 0.2708 - val_accuracy: 0.9333\n",
      "Epoch 744/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0196 - accuracy: 0.9833 - val_loss: 0.9149 - val_accuracy: 0.7000\n",
      "Epoch 745/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9833 - val_loss: 0.2979 - val_accuracy: 0.9333\n",
      "Epoch 746/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 0.9667\n",
      "Epoch 747/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9833 - val_loss: 0.3877 - val_accuracy: 0.9000\n",
      "Epoch 748/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0159 - accuracy: 0.9833 - val_loss: 0.8060 - val_accuracy: 0.7667\n",
      "Epoch 749/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0256 - accuracy: 0.9833 - val_loss: 0.4838 - val_accuracy: 0.9000\n",
      "Epoch 750/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 0.9833 - val_loss: 0.3571 - val_accuracy: 0.9333\n",
      "Epoch 751/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 0.9917 - val_loss: 2.7665 - val_accuracy: 0.6333\n",
      "Epoch 752/1000\n",
      "120/120 [==============================] - 0s 996us/step - loss: 0.0244 - accuracy: 0.9833 - val_loss: 1.2298 - val_accuracy: 0.7333\n",
      "Epoch 753/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0193 - accuracy: 0.9917 - val_loss: 0.7145 - val_accuracy: 0.8000\n",
      "Epoch 754/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.5964 - val_accuracy: 0.8667\n",
      "Epoch 755/1000\n",
      "120/120 [==============================] - 0s 974us/step - loss: 0.0228 - accuracy: 0.9833 - val_loss: 0.1700 - val_accuracy: 0.9333\n",
      "Epoch 756/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0153 - accuracy: 0.9917 - val_loss: 0.3064 - val_accuracy: 0.9333\n",
      "Epoch 757/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.9667 - val_loss: 0.4835 - val_accuracy: 0.8333\n",
      "Epoch 758/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.5345 - val_accuracy: 0.8333\n",
      "Epoch 759/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.5919 - val_accuracy: 0.7667\n",
      "Epoch 760/1000\n",
      "120/120 [==============================] - 0s 985us/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.6711 - val_accuracy: 0.7667\n",
      "Epoch 761/1000\n",
      "120/120 [==============================] - 0s 991us/step - loss: 0.0983 - accuracy: 0.9667 - val_loss: 0.0351 - val_accuracy: 1.0000\n",
      "Epoch 762/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0355 - accuracy: 0.9833 - val_loss: 0.5489 - val_accuracy: 0.7667\n",
      "Epoch 763/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.3670 - val_accuracy: 0.8667\n",
      "Epoch 764/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.4575 - val_accuracy: 0.8333\n",
      "Epoch 765/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.3766 - val_accuracy: 0.8667\n",
      "Epoch 766/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 0.9833 - val_loss: 0.6147 - val_accuracy: 0.7667\n",
      "Epoch 767/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.9917 - val_loss: 2.8911 - val_accuracy: 0.6000\n",
      "Epoch 768/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0323 - accuracy: 0.9833 - val_loss: 0.0860 - val_accuracy: 0.9667\n",
      "Epoch 769/1000\n",
      "120/120 [==============================] - 0s 989us/step - loss: 0.0347 - accuracy: 0.9833 - val_loss: 1.0723 - val_accuracy: 0.6333\n",
      "Epoch 770/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 1.0144 - val_accuracy: 0.7000\n",
      "Epoch 771/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0260 - accuracy: 0.9833 - val_loss: 0.9415 - val_accuracy: 0.7000\n",
      "Epoch 772/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0170 - accuracy: 0.9917 - val_loss: 0.9645 - val_accuracy: 0.7333\n",
      "Epoch 773/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0276 - accuracy: 0.9833 - val_loss: 0.9107 - val_accuracy: 0.7667\n",
      "Epoch 774/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0217 - accuracy: 0.9833 - val_loss: 1.6697 - val_accuracy: 0.6333\n",
      "Epoch 775/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0134 - accuracy: 0.9917 - val_loss: 0.4154 - val_accuracy: 0.8667\n",
      "Epoch 776/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0291 - accuracy: 0.9833 - val_loss: 0.4125 - val_accuracy: 0.9000\n",
      "Epoch 777/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.9917 - val_loss: 2.5137 - val_accuracy: 0.6333\n",
      "Epoch 778/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0156 - accuracy: 0.9917 - val_loss: 0.3528 - val_accuracy: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 779/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0155 - accuracy: 0.9917 - val_loss: 0.4382 - val_accuracy: 0.9000\n",
      "Epoch 780/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 0.9750 - val_loss: 1.0883 - val_accuracy: 0.7667\n",
      "Epoch 781/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0244 - accuracy: 0.9917 - val_loss: 0.7341 - val_accuracy: 0.8000\n",
      "Epoch 782/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.5135 - val_accuracy: 0.9000\n",
      "Epoch 783/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0244 - accuracy: 0.9833 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
      "Epoch 784/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0515 - accuracy: 0.9667 - val_loss: 0.5481 - val_accuracy: 0.9000\n",
      "Epoch 785/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 0.9833 - val_loss: 0.5819 - val_accuracy: 0.9000\n",
      "Epoch 786/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0185 - accuracy: 0.9833 - val_loss: 0.6949 - val_accuracy: 0.8667\n",
      "Epoch 787/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0126 - accuracy: 0.9917 - val_loss: 0.9134 - val_accuracy: 0.8333\n",
      "Epoch 788/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 1.1941 - val_accuracy: 0.7667\n",
      "Epoch 789/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.9750 - val_loss: 0.8127 - val_accuracy: 0.8667\n",
      "Epoch 790/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 0.9917 - val_loss: 1.2049 - val_accuracy: 0.7667\n",
      "Epoch 791/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0202 - accuracy: 0.9917 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 792/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0526 - accuracy: 0.9833 - val_loss: 1.3400 - val_accuracy: 0.7333\n",
      "Epoch 793/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0164 - accuracy: 0.9917 - val_loss: 0.8950 - val_accuracy: 0.8333\n",
      "Epoch 794/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 2.1497 - val_accuracy: 0.7000\n",
      "Epoch 795/1000\n",
      "120/120 [==============================] - 0s 991us/step - loss: 0.0204 - accuracy: 0.9917 - val_loss: 1.0702 - val_accuracy: 0.8000\n",
      "Epoch 796/1000\n",
      "120/120 [==============================] - 0s 990us/step - loss: 0.0101 - accuracy: 0.9917 - val_loss: 0.9032 - val_accuracy: 0.8667\n",
      "Epoch 797/1000\n",
      "120/120 [==============================] - 0s 995us/step - loss: 0.0145 - accuracy: 0.9917 - val_loss: 3.0664 - val_accuracy: 0.6333\n",
      "Epoch 798/1000\n",
      "120/120 [==============================] - 0s 968us/step - loss: 0.0069 - accuracy: 0.9917 - val_loss: 0.6923 - val_accuracy: 0.9333\n",
      "Epoch 799/1000\n",
      "120/120 [==============================] - 0s 980us/step - loss: 0.0142 - accuracy: 0.9917 - val_loss: 7.5205 - val_accuracy: 0.5667\n",
      "Epoch 800/1000\n",
      "120/120 [==============================] - 0s 993us/step - loss: 0.1885 - accuracy: 0.9750 - val_loss: 0.5555 - val_accuracy: 0.9000\n",
      "Epoch 801/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 0.9917 - val_loss: 0.6720 - val_accuracy: 0.8667\n",
      "Epoch 802/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0172 - accuracy: 0.9917 - val_loss: 0.5302 - val_accuracy: 0.9333\n",
      "Epoch 803/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0204 - accuracy: 0.9917 - val_loss: 0.5639 - val_accuracy: 0.9000\n",
      "Epoch 804/1000\n",
      "120/120 [==============================] - 0s 978us/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 5.1739 - val_accuracy: 0.6333\n",
      "Epoch 805/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0514 - accuracy: 0.9750 - val_loss: 0.6640 - val_accuracy: 0.9000\n",
      "Epoch 806/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0125 - accuracy: 0.9917 - val_loss: 1.2820 - val_accuracy: 0.7667\n",
      "Epoch 807/1000\n",
      "120/120 [==============================] - 0s 968us/step - loss: 0.0703 - accuracy: 0.9833 - val_loss: 0.0480 - val_accuracy: 0.9667\n",
      "Epoch 808/1000\n",
      "120/120 [==============================] - 0s 978us/step - loss: 0.0355 - accuracy: 0.9833 - val_loss: 0.5709 - val_accuracy: 0.7667\n",
      "Epoch 809/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0170 - accuracy: 0.9917 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
      "Epoch 810/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.6430 - val_accuracy: 0.7667\n",
      "Epoch 811/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0179 - accuracy: 0.9833 - val_loss: 1.1879 - val_accuracy: 0.6667\n",
      "Epoch 812/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0163 - accuracy: 0.9917 - val_loss: 1.1162 - val_accuracy: 0.7333\n",
      "Epoch 813/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0136 - accuracy: 0.9917 - val_loss: 1.9718 - val_accuracy: 0.6333\n",
      "Epoch 814/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0153 - accuracy: 0.9917 - val_loss: 0.3742 - val_accuracy: 0.9333\n",
      "Epoch 815/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0244 - accuracy: 0.9833 - val_loss: 1.1034 - val_accuracy: 0.7667\n",
      "Epoch 816/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0137 - accuracy: 0.9917 - val_loss: 0.6125 - val_accuracy: 0.9000\n",
      "Epoch 817/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0125 - accuracy: 0.9917 - val_loss: 0.7963 - val_accuracy: 0.8667\n",
      "Epoch 818/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 0.9917 - val_loss: 0.9888 - val_accuracy: 0.8000\n",
      "Epoch 819/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0116 - accuracy: 0.9917 - val_loss: 1.0967 - val_accuracy: 0.7667\n",
      "Epoch 820/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9750 - val_loss: 0.0821 - val_accuracy: 0.9667\n",
      "Epoch 821/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0318 - accuracy: 0.9833 - val_loss: 0.4895 - val_accuracy: 0.8333\n",
      "Epoch 822/1000\n",
      "120/120 [==============================] - 0s 973us/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.6198 - val_accuracy: 0.8333\n",
      "Epoch 823/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0122 - accuracy: 0.9917 - val_loss: 1.5620 - val_accuracy: 0.7000\n",
      "Epoch 824/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9833 - val_loss: 0.5795 - val_accuracy: 0.9000\n",
      "Epoch 825/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9917 - val_loss: 1.2837 - val_accuracy: 0.7667\n",
      "Epoch 826/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0121 - accuracy: 0.9917 - val_loss: 2.1725 - val_accuracy: 0.6667\n",
      "Epoch 827/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.9667 - val_loss: 0.8753 - val_accuracy: 0.7667\n",
      "Epoch 828/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.7045 - val_accuracy: 0.8000\n",
      "Epoch 829/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 0.9917 - val_loss: 0.6950 - val_accuracy: 0.8000\n",
      "Epoch 830/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0997 - val_accuracy: 0.9667\n",
      "Epoch 831/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0427 - accuracy: 0.9917 - val_loss: 0.5871 - val_accuracy: 0.8667\n",
      "Epoch 832/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.9833 - val_loss: 0.4763 - val_accuracy: 0.9333\n",
      "Epoch 833/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.9917 - val_loss: 0.1026 - val_accuracy: 0.9667\n",
      "Epoch 834/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0325 - accuracy: 0.9833 - val_loss: 0.7852 - val_accuracy: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 835/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9667 - val_loss: 0.6317 - val_accuracy: 0.9333\n",
      "Epoch 836/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0333 - accuracy: 0.9833 - val_loss: 0.6007 - val_accuracy: 0.8667\n",
      "Epoch 837/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0116 - accuracy: 0.9917 - val_loss: 1.2553 - val_accuracy: 0.7333\n",
      "Epoch 838/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0193 - accuracy: 0.9833 - val_loss: 1.0632 - val_accuracy: 0.7667\n",
      "Epoch 839/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.7136 - val_accuracy: 0.8667\n",
      "Epoch 840/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0181 - accuracy: 0.9833 - val_loss: 1.6456 - val_accuracy: 0.6333\n",
      "Epoch 841/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0124 - accuracy: 0.9917 - val_loss: 0.6690 - val_accuracy: 0.9000\n",
      "Epoch 842/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0190 - accuracy: 0.9917 - val_loss: 1.7420 - val_accuracy: 0.7333\n",
      "Epoch 843/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0152 - accuracy: 0.9917 - val_loss: 2.3665 - val_accuracy: 0.6333\n",
      "Epoch 844/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9833 - val_loss: 0.9252 - val_accuracy: 0.8667\n",
      "Epoch 845/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0115 - accuracy: 0.9917 - val_loss: 0.8128 - val_accuracy: 0.8667\n",
      "Epoch 846/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0113 - accuracy: 0.9917 - val_loss: 2.1402 - val_accuracy: 0.6333\n",
      "Epoch 847/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0220 - accuracy: 0.9917 - val_loss: 2.0961 - val_accuracy: 0.7333\n",
      "Epoch 848/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.6574 - val_accuracy: 0.9333\n",
      "Epoch 849/1000\n",
      "120/120 [==============================] - 0s 988us/step - loss: 0.0540 - accuracy: 0.9750 - val_loss: 0.0604 - val_accuracy: 0.9667\n",
      "Epoch 850/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0226 - accuracy: 0.9917 - val_loss: 0.7604 - val_accuracy: 0.7667\n",
      "Epoch 851/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.6837 - val_accuracy: 0.8333\n",
      "Epoch 852/1000\n",
      "120/120 [==============================] - 0s 980us/step - loss: 0.0179 - accuracy: 0.9917 - val_loss: 0.9978 - val_accuracy: 0.7667\n",
      "Epoch 853/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.7269 - val_accuracy: 0.8667\n",
      "Epoch 854/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0147 - accuracy: 0.9917 - val_loss: 1.3138 - val_accuracy: 0.7667\n",
      "Epoch 855/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0258 - accuracy: 0.9833 - val_loss: 1.2147 - val_accuracy: 0.7667\n",
      "Epoch 856/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.8696 - val_accuracy: 0.8000\n",
      "Epoch 857/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.7885 - val_accuracy: 0.8667\n",
      "Epoch 858/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.1907 - val_accuracy: 0.7667\n",
      "Epoch 859/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0088 - accuracy: 0.9917 - val_loss: 0.9689 - val_accuracy: 0.8000\n",
      "Epoch 860/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0312 - accuracy: 0.9917 - val_loss: 0.5463 - val_accuracy: 0.8000\n",
      "Epoch 861/1000\n",
      "120/120 [==============================] - 0s 980us/step - loss: 0.0113 - accuracy: 0.9917 - val_loss: 2.1811 - val_accuracy: 0.6333\n",
      "Epoch 862/1000\n",
      "120/120 [==============================] - 0s 975us/step - loss: 0.0132 - accuracy: 0.9917 - val_loss: 0.7450 - val_accuracy: 0.8000\n",
      "Epoch 863/1000\n",
      "120/120 [==============================] - 0s 974us/step - loss: 0.0160 - accuracy: 0.9917 - val_loss: 0.5695 - val_accuracy: 0.9000\n",
      "Epoch 864/1000\n",
      "120/120 [==============================] - 0s 984us/step - loss: 0.0203 - accuracy: 0.9917 - val_loss: 0.0971 - val_accuracy: 0.9667\n",
      "Epoch 865/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0446 - accuracy: 0.9750 - val_loss: 3.2512 - val_accuracy: 0.6000\n",
      "Epoch 866/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0261 - accuracy: 0.9833 - val_loss: 0.7577 - val_accuracy: 0.8000\n",
      "Epoch 867/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0102 - accuracy: 0.9917 - val_loss: 0.4409 - val_accuracy: 0.9000\n",
      "Epoch 868/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0199 - accuracy: 0.9833 - val_loss: 0.6627 - val_accuracy: 0.8667\n",
      "Epoch 869/1000\n",
      "120/120 [==============================] - 0s 957us/step - loss: 0.0174 - accuracy: 0.9917 - val_loss: 0.8026 - val_accuracy: 0.8667\n",
      "Epoch 870/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0086 - accuracy: 0.9917 - val_loss: 1.0597 - val_accuracy: 0.7667\n",
      "Epoch 871/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0144 - accuracy: 0.9917 - val_loss: 0.7625 - val_accuracy: 0.8667\n",
      "Epoch 872/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0135 - accuracy: 0.9833 - val_loss: 0.7652 - val_accuracy: 0.8667\n",
      "Epoch 873/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9833 - val_loss: 0.0362 - val_accuracy: 0.9667\n",
      "Epoch 874/1000\n",
      "120/120 [==============================] - 0s 992us/step - loss: 0.0531 - accuracy: 0.9917 - val_loss: 0.4483 - val_accuracy: 0.7667\n",
      "Epoch 875/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 0.9917 - val_loss: 0.5929 - val_accuracy: 0.7667\n",
      "Epoch 876/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 0.9917 - val_loss: 0.8512 - val_accuracy: 0.7000\n",
      "Epoch 877/1000\n",
      "120/120 [==============================] - 0s 998us/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.3528 - val_accuracy: 0.9000\n",
      "Epoch 878/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 1.0519 - val_accuracy: 0.7000\n",
      "Epoch 879/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0174 - accuracy: 0.9917 - val_loss: 0.4938 - val_accuracy: 0.8667\n",
      "Epoch 880/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.7946 - val_accuracy: 0.6333\n",
      "Epoch 881/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0140 - accuracy: 0.9917 - val_loss: 0.5823 - val_accuracy: 0.8667\n",
      "Epoch 882/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0290 - accuracy: 0.9750 - val_loss: 0.2970 - val_accuracy: 0.9333\n",
      "Epoch 883/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0098 - accuracy: 0.9917 - val_loss: 0.7770 - val_accuracy: 0.7667\n",
      "Epoch 884/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.5444 - val_accuracy: 0.8667\n",
      "Epoch 885/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.0081 - accuracy: 0.9917 - val_loss: 1.4572 - val_accuracy: 0.6333\n",
      "Epoch 886/1000\n",
      "120/120 [==============================] - 0s 967us/step - loss: 0.0222 - accuracy: 0.9833 - val_loss: 0.2792 - val_accuracy: 0.9333\n",
      "Epoch 887/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9833 - val_loss: 0.2347 - val_accuracy: 0.9333\n",
      "Epoch 888/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0403 - accuracy: 0.9917 - val_loss: 0.2828 - val_accuracy: 0.9000\n",
      "Epoch 889/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0145 - accuracy: 0.9917 - val_loss: 0.9317 - val_accuracy: 0.6333\n",
      "Epoch 890/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0205 - accuracy: 0.9833 - val_loss: 0.4994 - val_accuracy: 0.7667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 891/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.2501 - val_accuracy: 0.9333\n",
      "Epoch 892/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.6125 - val_accuracy: 0.7667\n",
      "Epoch 893/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.9833 - val_loss: 1.5282 - val_accuracy: 0.6333\n",
      "Epoch 894/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.2751 - val_accuracy: 0.9333\n",
      "Epoch 895/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0371 - accuracy: 0.9750 - val_loss: 0.4919 - val_accuracy: 0.8333\n",
      "Epoch 896/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0132 - accuracy: 0.9917 - val_loss: 2.2225 - val_accuracy: 0.6333\n",
      "Epoch 897/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0140 - accuracy: 0.9917 - val_loss: 0.4987 - val_accuracy: 0.9000\n",
      "Epoch 898/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.9667 - val_loss: 0.1388 - val_accuracy: 0.9333\n",
      "Epoch 899/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0181 - accuracy: 0.9917 - val_loss: 0.4867 - val_accuracy: 0.8333\n",
      "Epoch 900/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0151 - accuracy: 0.9917 - val_loss: 0.4284 - val_accuracy: 0.9000\n",
      "Epoch 901/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.6912 - val_accuracy: 0.6333\n",
      "Epoch 902/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0147 - accuracy: 0.9917 - val_loss: 1.8817 - val_accuracy: 0.6333\n",
      "Epoch 903/1000\n",
      "120/120 [==============================] - 0s 999us/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.7673 - val_accuracy: 0.8000\n",
      "Epoch 904/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.9062 - val_accuracy: 0.7667\n",
      "Epoch 905/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0133 - accuracy: 0.9917 - val_loss: 0.3741 - val_accuracy: 0.9333\n",
      "Epoch 906/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0114 - accuracy: 0.9917 - val_loss: 2.5285 - val_accuracy: 0.6333\n",
      "Epoch 907/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9750 - val_loss: 0.8239 - val_accuracy: 0.7667\n",
      "Epoch 908/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0168 - accuracy: 0.9917 - val_loss: 0.6210 - val_accuracy: 0.8333\n",
      "Epoch 909/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 3.7640 - val_accuracy: 0.6333\n",
      "Epoch 910/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.9833 - val_loss: 0.2116 - val_accuracy: 0.9333\n",
      "Epoch 911/1000\n",
      "120/120 [==============================] - 0s 994us/step - loss: 0.0270 - accuracy: 0.9833 - val_loss: 0.4823 - val_accuracy: 0.8333\n",
      "Epoch 912/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 1.1731 - val_accuracy: 0.7333\n",
      "Epoch 913/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.3266 - val_accuracy: 0.9333\n",
      "Epoch 914/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0110 - accuracy: 0.9917 - val_loss: 2.3242 - val_accuracy: 0.6333\n",
      "Epoch 915/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0506 - accuracy: 0.9833 - val_loss: 0.5658 - val_accuracy: 0.8000\n",
      "Epoch 916/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.5088 - val_accuracy: 0.8333\n",
      "Epoch 917/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.3258 - val_accuracy: 0.9333\n",
      "Epoch 918/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0324 - accuracy: 0.9833 - val_loss: 0.4620 - val_accuracy: 0.9000\n",
      "Epoch 919/1000\n",
      "120/120 [==============================] - 0s 998us/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.4548 - val_accuracy: 0.9000\n",
      "Epoch 920/1000\n",
      "120/120 [==============================] - 0s 998us/step - loss: 0.0257 - accuracy: 0.9833 - val_loss: 0.2839 - val_accuracy: 0.9333\n",
      "Epoch 921/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 1.1444 - val_accuracy: 0.7333\n",
      "Epoch 922/1000\n",
      "120/120 [==============================] - 0s 967us/step - loss: 0.0098 - accuracy: 0.9917 - val_loss: 2.1924 - val_accuracy: 0.6333\n",
      "Epoch 923/1000\n",
      "120/120 [==============================] - 0s 983us/step - loss: 0.0219 - accuracy: 0.9917 - val_loss: 2.4325 - val_accuracy: 0.6333\n",
      "Epoch 924/1000\n",
      "120/120 [==============================] - 0s 975us/step - loss: 0.0156 - accuracy: 0.9917 - val_loss: 2.5651 - val_accuracy: 0.6333\n",
      "Epoch 925/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0293 - accuracy: 0.9750 - val_loss: 1.6250 - val_accuracy: 0.7333\n",
      "Epoch 926/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.9833 - val_loss: 1.0941 - val_accuracy: 0.7333\n",
      "Epoch 927/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0142 - accuracy: 0.9917 - val_loss: 0.5895 - val_accuracy: 0.8333\n",
      "Epoch 928/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.4497 - val_accuracy: 0.6667\n",
      "Epoch 929/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0195 - accuracy: 0.9917 - val_loss: 3.7393 - val_accuracy: 0.6333\n",
      "Epoch 930/1000\n",
      "120/120 [==============================] - 0s 989us/step - loss: 0.0631 - accuracy: 0.9750 - val_loss: 0.4791 - val_accuracy: 0.8333\n",
      "Epoch 931/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.2996 - val_accuracy: 0.9333\n",
      "Epoch 932/1000\n",
      "120/120 [==============================] - 0s 962us/step - loss: 0.0213 - accuracy: 0.9833 - val_loss: 1.0308 - val_accuracy: 0.7000\n",
      "Epoch 933/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0179 - accuracy: 0.9917 - val_loss: 0.5433 - val_accuracy: 0.8333\n",
      "Epoch 934/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.8164 - val_accuracy: 0.7667\n",
      "Epoch 935/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0297 - accuracy: 0.9833 - val_loss: 0.3467 - val_accuracy: 0.8667\n",
      "Epoch 936/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.8897 - val_accuracy: 0.7333\n",
      "Epoch 937/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 1.7216 - val_accuracy: 0.6333\n",
      "Epoch 938/1000\n",
      "120/120 [==============================] - 0s 974us/step - loss: 0.0170 - accuracy: 0.9917 - val_loss: 0.4834 - val_accuracy: 0.8333\n",
      "Epoch 939/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 0.6833 - val_accuracy: 0.8333\n",
      "Epoch 940/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.1642 - accuracy: 0.9750 - val_loss: 1.9912 - val_accuracy: 0.6333\n",
      "Epoch 941/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.1799 - accuracy: 0.9833 - val_loss: 0.3151 - val_accuracy: 0.9000\n",
      "Epoch 942/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0324 - accuracy: 0.9833 - val_loss: 0.2844 - val_accuracy: 0.9000\n",
      "Epoch 943/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0143 - accuracy: 0.9917 - val_loss: 0.4105 - val_accuracy: 0.8333\n",
      "Epoch 944/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0122 - accuracy: 0.9917 - val_loss: 1.3363 - val_accuracy: 0.6333\n",
      "Epoch 945/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0092 - accuracy: 0.9917 - val_loss: 0.6757 - val_accuracy: 0.8667\n",
      "Epoch 946/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0075 - accuracy: 0.9917 - val_loss: 5.3662 - val_accuracy: 0.5667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 947/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9750 - val_loss: 0.4503 - val_accuracy: 0.8667\n",
      "Epoch 948/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.7895 - val_accuracy: 0.6333\n",
      "Epoch 949/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0301 - accuracy: 0.9833 - val_loss: 0.8169 - val_accuracy: 0.7667\n",
      "Epoch 950/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.3085 - val_accuracy: 0.9333\n",
      "Epoch 951/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.9833 - val_loss: 0.4085 - val_accuracy: 0.9000\n",
      "Epoch 952/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0244 - accuracy: 0.9833 - val_loss: 0.3545 - val_accuracy: 0.9333\n",
      "Epoch 953/1000\n",
      "120/120 [==============================] - 0s 973us/step - loss: 0.0092 - accuracy: 0.9917 - val_loss: 1.9751 - val_accuracy: 0.6333\n",
      "Epoch 954/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0293 - accuracy: 0.9833 - val_loss: 1.5876 - val_accuracy: 0.6333\n",
      "Epoch 955/1000\n",
      "120/120 [==============================] - 0s 992us/step - loss: 0.0140 - accuracy: 0.9917 - val_loss: 0.5484 - val_accuracy: 0.8333\n",
      "Epoch 956/1000\n",
      "120/120 [==============================] - 0s 999us/step - loss: 0.0198 - accuracy: 0.9833 - val_loss: 0.6288 - val_accuracy: 0.8000\n",
      "Epoch 957/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.6675 - val_accuracy: 0.8333\n",
      "Epoch 958/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.5728 - val_accuracy: 0.8667\n",
      "Epoch 959/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0181 - accuracy: 0.9833 - val_loss: 0.9690 - val_accuracy: 0.7667\n",
      "Epoch 960/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0230 - accuracy: 0.9833 - val_loss: 0.7580 - val_accuracy: 0.7667\n",
      "Epoch 961/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.3244 - val_accuracy: 0.7333\n",
      "Epoch 962/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0252 - accuracy: 0.9833 - val_loss: 2.9860 - val_accuracy: 0.6333\n",
      "Epoch 963/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4592 - val_accuracy: 0.9000\n",
      "Epoch 964/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9667 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 965/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0215 - accuracy: 0.9917 - val_loss: 0.3719 - val_accuracy: 0.8333\n",
      "Epoch 966/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0126 - accuracy: 0.9917 - val_loss: 0.7465 - val_accuracy: 0.7000\n",
      "Epoch 967/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.9929 - val_accuracy: 0.6333\n",
      "Epoch 968/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 0.9917 - val_loss: 0.2945 - val_accuracy: 0.9000\n",
      "Epoch 969/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 0.9917 - val_loss: 0.4277 - val_accuracy: 0.8333\n",
      "Epoch 970/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9917 - val_loss: 0.2807 - val_accuracy: 0.9000\n",
      "Epoch 971/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.5277 - val_accuracy: 0.8333\n",
      "Epoch 972/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0115 - accuracy: 0.9917 - val_loss: 2.5193 - val_accuracy: 0.6333\n",
      "Epoch 973/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0168 - accuracy: 0.9917 - val_loss: 0.4495 - val_accuracy: 0.8333\n",
      "Epoch 974/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9333\n",
      "Epoch 975/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0121 - accuracy: 0.9917 - val_loss: 2.2376 - val_accuracy: 0.6333\n",
      "Epoch 976/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0341 - accuracy: 0.9750 - val_loss: 0.4681 - val_accuracy: 0.8333\n",
      "Epoch 977/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0143 - accuracy: 0.9917 - val_loss: 0.3621 - val_accuracy: 0.9000\n",
      "Epoch 978/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0275 - accuracy: 0.9833 - val_loss: 0.2147 - val_accuracy: 0.9333\n",
      "Epoch 979/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.0314 - val_accuracy: 0.6333\n",
      "Epoch 980/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0157 - accuracy: 0.9917 - val_loss: 0.8283 - val_accuracy: 0.7667\n",
      "Epoch 981/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.3587 - val_accuracy: 0.9000\n",
      "Epoch 982/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.8505 - val_accuracy: 0.6333\n",
      "Epoch 983/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0208 - accuracy: 0.9917 - val_loss: 0.3188 - val_accuracy: 0.9000\n",
      "Epoch 984/1000\n",
      "120/120 [==============================] - 0s 973us/step - loss: 0.0101 - accuracy: 0.9917 - val_loss: 4.5052 - val_accuracy: 0.5667\n",
      "Epoch 985/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0359 - accuracy: 0.9667 - val_loss: 0.3213 - val_accuracy: 0.9333\n",
      "Epoch 986/1000\n",
      "120/120 [==============================] - 0s 984us/step - loss: 0.0211 - accuracy: 0.9917 - val_loss: 0.3350 - val_accuracy: 0.9000\n",
      "Epoch 987/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.8794 - val_accuracy: 0.7667\n",
      "Epoch 988/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0515 - accuracy: 0.9917 - val_loss: 0.1415 - val_accuracy: 0.9333\n",
      "Epoch 989/1000\n",
      "120/120 [==============================] - 0s 973us/step - loss: 0.0507 - accuracy: 0.9750 - val_loss: 0.4910 - val_accuracy: 0.7667\n",
      "Epoch 990/1000\n",
      "120/120 [==============================] - 0s 992us/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.3534 - val_accuracy: 0.8333\n",
      "Epoch 991/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.8969 - val_accuracy: 0.7333\n",
      "Epoch 992/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0527 - accuracy: 0.9833 - val_loss: 0.2454 - val_accuracy: 0.9000\n",
      "Epoch 993/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0114 - accuracy: 0.9917 - val_loss: 0.8971 - val_accuracy: 0.6333\n",
      "Epoch 994/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0259 - accuracy: 0.9833 - val_loss: 1.0426 - val_accuracy: 0.6333\n",
      "Epoch 995/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0353 - accuracy: 0.9833 - val_loss: 1.0947 - val_accuracy: 0.6333\n",
      "Epoch 996/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0158 - accuracy: 0.9917 - val_loss: 0.3489 - val_accuracy: 0.8333\n",
      "Epoch 997/1000\n",
      "120/120 [==============================] - 0s 998us/step - loss: 0.0190 - accuracy: 0.9917 - val_loss: 0.2343 - val_accuracy: 0.9000\n",
      "Epoch 998/1000\n",
      "120/120 [==============================] - 0s 990us/step - loss: 0.0206 - accuracy: 0.9917 - val_loss: 0.2185 - val_accuracy: 0.9333\n",
      "Epoch 999/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.5411 - val_accuracy: 0.8000\n",
      "Epoch 1000/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0087 - accuracy: 0.9917 - val_loss: 2.4373 - val_accuracy: 0.6333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x255e89d8070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model\n",
    "\n",
    "model.fit(data,target_new,epochs=1000,validation_split=0.2,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABE70lEQVR4nO2dd7wU1fXAv/f1R5VeA4gNFSIqGk0UjcYSYzTFJMZOEk3s5RejxhQSY4oYE5OoibEr9hYV7KBYEAGlgyhIefCAR3mP19++3fv7Y3bY2dmZ3dnd2be7w/l+PvvZ3Zk7t0w5c+65556rtNYIgiAIwaMk3xUQBEEQcoMIeEEQhIAiAl4QBCGgiIAXBEEIKCLgBUEQAkpZvitgpX///nrUqFH5roYgCELRMH/+/K1a6wFO+wpKwI8aNYp58+bluxqCIAhFg1Jqrds+MdEIgiAEFBHwgiAIAUUEvCAIQkApKBu8E6FQiJqaGtra2vJdlYKmqqqK4cOHU15enu+qCIJQIBS8gK+pqaFnz56MGjUKpVS+q1OQaK3Ztm0bNTU17LnnnvmujiAIBULBm2ja2tro16+fCPckKKXo16+f9HIEQYij4AU8IMLdA3KOBEGwUxQCXuhiwp3w0cMQCee7JoIgZIEIeA/06NEj31XoWj68G164DObfn++aCIKQBSLghURathrfrTvyWw9BELJCBHwaaK259tprGTt2LOPGjeOJJ54AoLa2lokTJzJ+/HjGjh3LO++8Qzgc5oILLtiV9m9/+1ueay8Iwu5GwbtJWvndi0tZtnGnr3keMLQXv/3mgZ7SPvvssyxYsICFCxeydetWDjvsMCZOnMijjz7KSSedxI033kg4HKalpYUFCxawYcMGlixZAkB9fb2v9RYEQUiFaPBp8O677/LDH/6Q0tJSBg0axDHHHMPcuXM57LDDuP/++5k8eTKLFy+mZ8+ejB49mtWrV3P55Zfzyiuv0KtXr3xXXxCE3Yyi0uC9atq5wm2B8okTJzJr1iymTZvGueeey7XXXst5553HwoULefXVV7njjjt48sknue+++7q4xoIg7M6IBp8GEydO5IknniAcDlNXV8esWbM4/PDDWbt2LQMHDuTCCy/kxz/+MR999BFbt24lEonw3e9+l5tuuomPPvoo39UXBGE3o6g0+Hzz7W9/m9mzZ3PQQQehlOKWW25h8ODBPPjgg0yZMoXy8nJ69OjBQw89xIYNG5g0aRKRSASAP/3pT3muvSAIuxs5FfBKqauBnwAaWAxM0loX3Xz6pqYmwJgtOmXKFKZMmRK3//zzz+f8889POE60dkEQ8knOTDRKqWHAFcAErfVYoBQ4M1flCYIgCPHk2gZfBlQrpcqAbsDGHJcnCIIgRMmZgNdabwBuBdYBtUCD1vo1ezql1EVKqXlKqXl1dXW5qo6QDi7eQoIgFBe5NNH0AU4H9gSGAt2VUufY02mt79ZaT9BaTxgwwHFhcEEQBCEDcmmi+Rrwuda6TmsdAp4FvpzD8gS/kNDDghAIcing1wFHKKW6KSNY+fHA8hyWJwiCIFjIpQ1+DvA08BGGi2QJcHeuyhMEQRDiyakXjdb6t1rrMVrrsVrrc7XW7bksrxBIFjt+zZo1jB07tgtrIwjC7oyEKhAEQQgoxRWq4OXrYdNif/McPA6+/mfX3ddddx0jR47kkksuAWDy5MkopZg1axY7duwgFArxhz/8gdNPPz2tYtva2rj44ouZN28eZWVl3HbbbXz1q19l6dKlTJo0iY6ODiKRCM888wxDhw7l+9//PjU1NYTDYX7961/zgx/8IKtmJ0XcJAUhEBSXgM8DZ555JlddddUuAf/kk0/yyiuvcPXVV9OrVy+2bt3KEUccwWmnnZbWwtd33HEHAIsXL2bFihWceOKJrFy5kn//+99ceeWVnH322XR0dBAOh5k+fTpDhw5l2rRpADQ0NPjfUEEQAkdxCfgkmnauOPjgg9myZQsbN26krq6OPn36MGTIEK6++mpmzZpFSUkJGzZsYPPmzQwePNhzvu+++y6XX345AGPGjGHkyJGsXLmSI488kptvvpmamhq+853vsM8++zBu3Dh+/vOfc91113Hqqady9NFH56q5BuImKQiBQGzwHjjjjDN4+umneeKJJzjzzDOZOnUqdXV1zJ8/nwULFjBo0CDa2tKLoeYWW/6ss87ihRdeoLq6mpNOOokZM2aw7777Mn/+fMaNG8cNN9zA73//ez+aJQiFxycvw/q5+a5FYCguDT5PnHnmmVx44YVs3bqVt99+myeffJKBAwdSXl7OzJkzWbt2bdp5Tpw4kalTp3LcccexcuVK1q1bx3777cfq1asZPXo0V1xxBatXr2bRokWMGTOGvn37cs4559CjRw8eeOAB/xspCIXAY9F4hJPFDOkHIuA9cOCBB9LY2MiwYcMYMmQIZ599Nt/85jeZMGEC48ePZ8yYMWnneckll/Czn/2McePGUVZWxgMPPEBlZSVPPPEEjzzyCOXl5QwePJjf/OY3zJ07l2uvvZaSkhLKy8u56667ctBKCzLIKgiBQLmZCvLBhAkT9Lx58+K2LV++nP333z9PNSoufDtXb/wO3r0NjvsVTLw2+/wEwSuTe0e/RYP3ilJqvtZ6gtM+scELgiAEFDHR5IDFixdz7rnnxm2rrKxkzpw5eaqRIAi7I0Uh4LXWafmY55tx48axYMGCLi3TV1NbEZ1rQRDcKXgTTVVVFdu2bfNXgAUMrTXbtm2jqqoq31URBKGAKHgNfvjw4dTU1CCrPSWnqqqK4cOH57sagiAUEAUv4MvLy9lzzz3zXY3dC+ktCUIgKHgTjSAIgpAZIuAFQRACigh4QRCEgCICXkhE3CQFIRCIgBcEQQgoIuAFQRACigh4IRFxkxSEQCACXhAEIaCIgBcEQQgoIuAFQRACigh4IRFxkxSEQCACXkhEBlkFIRCIgBcEQQgoIuAFQRACigh4QRCEgCICXhAEIaCIgBcEQQgoIuCFRMRNUhACgQh4IRFxkxSEQCACXhAEIaCIgBcEQQgoIuAFQRACigh4QRCEgCICXhAEIaDkVMArpfZQSj2tlFqhlFqulDoyl+UJPiFukoIQCMpynP/twCta6zOUUhVAtxyXJ/iBuEkKQiDImYBXSvUCJgIXAGitO4COXJUnCIIgxJNLE81ooA64Xyn1sVLqHqVUd3sipdRFSql5Sql5dXV1OayOIAjC7kUuBXwZcAhwl9b6YKAZuN6eSGt9t9Z6gtZ6woABA3JYHUEQhN2LXAr4GqBGaz0n+v9pDIEvCIIgdAE5E/Ba603AeqXUftFNxwPLclWekAvEm0YQiplce9FcDkyNetCsBibluDzBV8SbRhCKmZwKeK31AmBCLssQBEEQnAnWTNaNC+Cdv+a7FoIgCAVBrk00XcvdxxjfR/9ffushCIJQAARLgxcEQRB2IQJeEAQhoIiAF5IgbpKCUMyIgBeSIG6SglDMiIAXBEEIKCLgBUEQsmHnRgiH8l0LR0TAC4IgZEp7I9y2P0y7Jt81cUQEvCAIQqZ0NBvfK1/Nbz1cCKaAlxWJBEEQAirgBZ8QN0lBKGaCKeBFg/cJOY9Cnnj+Egi15rsWRU8wBbwIJkEobhZMhcVP57sWRU9ABbwgCEWPEhNhtgRTwIuJRhAEIaACXvAJ0aAEISkFrkwGVMAX9kkXBEHoCoIp4Av8rSoIQkAo8HGCYAp4wSfkRSkISSlwZTKgAr6wT7ogCEJXEEwBX+BvVUEQAoKYaARBEAJKgSuTARXwhX3Si4fC1k6EgFPgwjOewnxWAirgBUEQupLCfBkFU8AX1ZtfEARHCty+XQwEU8AX6Nu0+JDzKAjeKMyXUUAFvCAIRU9R9cQ91lVrCHfmtioWgingi+rGEASheElT1rwxGW7qB53tOamNHU8CXil1pVKqlzK4Vyn1kVLqxFxXLnNEwAtC0VMMNvhdyqTHus67z/jubMtJdex41eB/pLXeCZwIDAAmAX/OWa2EAqEIHjBByCuFrUx6FfDmk34KcL/WeiGF/PSLiUYQih95jrPGq4Cfr5R6DUPAv6qU6glEclctQRCEIqDAX0JlHtP9GBgPrNZatyil+mKYaQqUwj7pxYOcRyGPFIMNvsCfEa8a/JHAJ1rreqXUOcCvgIbcVStLCvytKgiCB+Q5zhqvAv4uoEUpdRDwC2At8FDOaiUUCMWgQQlCHinwl5BXAd+ptdbA6cDtWuvbgZ65q1a2FPZJF/LEn0fCzD/luxZCoChsWeNVwDcqpW4AzgWmKaVKgXIvByqlSpVSHyulXsq0koLgC2318LZ49xYNRWGDNylMQe9VwP8AaMfwh98EDAOmeDz2SmB5BnXLnK7qNm1eBrO8ngZBENKiwM0fQOZ17KK2eRLwUaE+FeitlDoVaNNap7TBK6WGA98A7smqloXKvSfAjD9AZ0e+ayIIQl5IcyZrwnG5xWuogu8DHwLfA74PzFFKneHh0L9jDMq6+swrpS5SSs1TSs2rq6vzUp3CwZxuXFRdyXQoAg1KCC7F8FwVeC/Dq4nmRuAwrfX5WuvzgMOBXyc7IKrpb9Faz0+WTmt9t9Z6gtZ6woABAzxWJwVOJz0Shvf/CaFWf8oQBCG3FLjwzIpCMtEAJVrrLZb/2zwc+xXgNKXUGuBx4Dil1CPpVzETHE7eoifgtV/B23/JQXFBvRGLQIMSBMEVrwL+FaXUq0qpC5RSFwDTgOnJDtBa36C1Hq61HgWcCczQWp+TVW2zoaPZ+G5vzEHmQRXwQlHSsh2Wi9Nal1Dgg6yeQhVora9VSn0XQytXwN1a6+dyWrNs6CqNOrCau1DUPH42rHsffv4Z9PDJ7JkPisEGnzEFJOABtNbPAM9kUojW+i3grUyOLQpE0AuFxPbVxncklN96ZEu+nqv6dfDSNfC9+6HSNp9zZy30GmLZUNjPflITjVKqUSm10+HTqJTa2VWVTJ+uPumFfZGF3ZUi04ALRVF68yb47HVYMS1+++q34LYxsOx/sW0FbqJJKuC11j211r0cPj211r26pIaZ0NU3SqHcmL4T1HYJghdsL8iNC4zvmrmWjZk+IwUg4AWviCAUhKyxK0oFa4P3oV6FoMEXL6LB+0OhPmDCbkHBPVcO9cm4jiLgMyfZSff1pim0G1AQYNd9WbAasBtF8jzFndfCrnMwBXyXU9gXWdjN2KXEFJuA340QE002iIlGsCHXqPAp9GvkVD8x0ewOFPiNKRS+8BCKCB9MNKLBZ4G4SQoJ7E7XqFjbWuj1TqLBpz3eIQI+C2Sik2BDXsKCXzgJ8wK9v4Ij4Ne82/VlFuhFFZzYHa9VkbW50J8nx/qJiSb3tNbDA9+I/RcTjWBnd7xGu2Ob7cy+A972e1lNiwZf4CYaz8HGCpqwPaiS3NiCnd3onihawZ6Der/6S+P7mGv9z9tKuudcNPg0UOk0IwcntmgfqN2IdK/Rxo+NuOpFibZ9C/7gpK2Lm2TusXePulzg+lReJAxbP/UnL8FGmtfo7mPh/lNyUpMuo9gUj2KrL2RhoukagiHg07ox/LwQPt+QM26Cf02IxfMuRiJhqJmX71okkonwqFvufz2y5aVrYOHjHhMXmcBsLZQeU6rzJn7wXYuO2Dd0cfk+lbf2feO7cbM/+eWDd/4K9xwP6+bkuyY2ikzYuTHvXnjup8bvLcshYr/3Lfh1X4ZDEGrzJ69kPH527stIh5xq5SLgvWMX8MVqoglC7JDNS4zvxo35rYedYuz+J2PTYrjzCOOFake72OA7muHjqemfi/8eBzcPyqiaabFjTe7L8ILb+dkV4sfBi8ZKJAyzpkBbkjWRRINPgwQNvqvLD5jwEAqfhhrje0MSc5j9vnz5OvjfJbD2vfTK2rQovfQZU2jPkReFy6HOK6bBjD/Aazf6XqN0CaiAL1YNvtBu8CCRxrkthhe2pzra0jRFTX8dzb5XJyVNW6B2YfI0BX/ePYYh72w3vvNxnm0EU8DLRKf8U2jnJJ36FFrd06YA63/H4fCfifmuhU9YTTRO+5OEa26Pmm266B4LxkSnfJtofCMANviCJZ0HqgAFZCaYQmT751Bant8XV+sOD4mK8bwncZO0b6tfn3hcjgmmBt/1Fchz+QVIofkFp6XB5/t+Sodkga+i3/8YD387MLPsP3sz/WOat8GK6ZmVV8h4jQfvdq+FWlKn8ZndUMAXw0zWALwwCs7MsTuZaKL40Y5HvpP+MY/9AB7/oUet3UJATntSE01CmtwSEAGvk/+H4vBpLTStNyMKtA1pCbvASBoXfLpGWsO6DxLPrTlRL9yZboa+VCt3pBmqoACe54AI+IBMdAoEATgXRWGi8XCeXe9LbcyGfema7Kqw+Gm47yRY9IRthynYCjMAl69kuoyfmGjSIO8PpM8Xqxhv9EInqF40SbXEJO147qfGrNhsMDX1bZ8516mYzqMXPC9mLiYaf8m3m6QfdLRAy7Z81yJKNucv/91SZ4I6yOpEtK2uz4FP18iM4ppQToYafCH3/ubeC/MfcNiRoYlGNPg0SMdEs+SZHJTvw8X673GwdWX2+RQMBfaw7k42+JTV92vMyCW/VBr8tlWw6CmHahXAed+xBpY8nbh92jXQtMn4nSpUgXVbJAyv3AANG3ytplcCIuDD3tO2NeSiAtlnUVCRCwtVC8+GgJpokpLrdrgJ8hT3z51HwLM/yUmNsmbpc2ke4HSOLSaadbPhgzuNEBEpj/OfgAh4mclacDx/qaG9FApO16hxc2xaeVzaAjXRWNuQ1UCeXyYaU8C7nS+X8sMd6aXvUhzOTbreQNYY8ea5sT8L1msz61Z4OAN3VA8EU8A7EWrNZQVymHeR0tmaOPjmBa1hwaPOgjcrHK7RX/eFJ8/zlrYQ8KxI2CY65QwXW3tXD7Jqndtwxgm9/hQmGk9YjqtfB5uXZphPcgIi4O0n2fa/oQZe+1UXlp91hj7n14Vk6/u78hV4/mJj8RM/cbtGK1/xnjbfpNuzyGU71n8Yewm7mWjSMZ065uORWVOMcMat9Zkdb8V6/0ZCRiz89iRhf1OZaFwPs/bGIjnzmQ+IgE9hoimUONO7A+maEey0NxnfO/2OJ59nG/za2TC5N2yYn3kejgI+D654O2vh3hPg7T9Hi7HVK6Xpxo0M6/vxI8a33ytCPX8x3D4e2hvjt6cSxrtMNHh8HnSa60p7J5gCXsiSPGqwpdH4d5F0Z0GmwMts59hOf8uGmM/55+9knkehaPAdTSkSZCrg7Xisv1mOKs2yPAd21iQK+Liyk9XR8iJIGHewCX4R8ElI5SaZ6y633/kXjIkgg3pk29UsMQW83wO0adwTfp//2oWwOOoWWFqReT5dpRGni+sKSCnKT/XS9XodzHulJMPguM1b4akLoiswOYX4tQv4VKEKLNvM52H9HPj0NUsSMdF4J+8afPRitTfBcz+DlkJZPDhTPN5soTaju+4n5kMaDvmbb4LwSLaWqc/309ZPY79Ly7PISLv8tidLYgrICW6DrCnOY6qxMyc2Lkicy+Jm63/xqtT5gbHs4dLn4OOHnfcn67E4zW6N86KxtOmz160HWn5G8M2zyUYwBXy+3CTn3QcLH3NeJzMfbFsFU/aOLe+WLqlO41MXwG1jMsvbjZKoAPTbRJOgwScTPjm8f7pEg081k9Vn3OqVqhdmPW7JM8nNGCZ3HwNP/8i5nMd/GN/m+fcnL98r9no5ymKnc21L6GaPL0YTjVLqC0qpmUqp5UqppUqpK3NVVt6DjZkUQPS4OObdB811sOTZ3OS/8mXj20+NsSRqR821DT4jDdgH8mqiydH9aT9fyTT4uLSW3wseTZ2va/lRAV+7MDfrxyZ9UaUw0cRtdtseKT4BD3QC/6e13h84ArhUKXVATkoqFBON68y+YsXrA2ZJl/ZMQBslORpkTUeDz+X9VOaTgLeaAVzTZ2AC8YS9TDc3yRQCPm6/Uzu82uAt+WRrMnQ6nwn3opMfvIuJJg6Xl1sxCnitda3W+qPo70ZgOTAsN4UViImm0DT4rsJPgWje6L4LeBu58KLRGlbNTJ63kwYfCXub2JX2fZ1G+psGwIyb08zfLMbl+jtuz9KN1rEci4adi5ezm42/bqXRQzYSOSSwm2isL2jb9mIeZFVKjQIOBuY47LtIKTVPKTWvrq4u4VhP5ExT8VyB6HemUfTc8sshM26GW/fNvPwP/2tJ7udDFS07r4OsGZ7/+ffDw98y4qS7UeIwyPrUBfCHganz93qezfqn045wB8y6xXt6p/JMvJporPudBJzW8MFdRiC+ZFhNKOlOrkrASYN3yfOOw+DZC2P/b90vOqEyiSkm9if+d7Fp8CZKqR7AM8BVWuuEKWFa67u11hO01hMGDBiQWSH5NtHYNfhs69MVM2Nn3QJNmzOvxwd3WdK5aXAZtMM8JuKzgO8KE82OtcZ3wzrv9QBY/oK3/NPu1bicf9+1xTRMNF49gcz9r1yfenJYrjV4u4B3PH/KiDb5/j/de/Sug6y5M9Fk6DjqDaVUOYZwn6q1ztFIH/k30fhug/ep/hnHBElTAPj5ULkFZ8o633R6eRmefy+CM5t746Wr00vfVY+BHxp8NkR8EvDrP4Rlzzvk7+XF6sFEk2zwtdg0eKWUAu4Flmutb8tVOUDXavD3nQzTf+G8zy/NyLcXVJb1adkGzW6LkDg8qF7qnSoyn5mX3yYat3Ic92V6/j28ULO5Vz+Zbs0oSULtIY2PJLQpmYB3GCi2HhOXNk0vGogJ+0yuoZNwt+cPZPxcuZloitQP/ivAucBxSqkF0c8pOSkpXTfJbAToutnw4X9c8vPJBp9vk5PJh/+BKaOd91nP4ex/GQ9WQk/K9n/pc3BTP6j7JEmhpv3YRw3+7Vvgn4fYismREDYyyGHeSfIJh6LXIYf3v5FBiv/m5lSDrLkYEDXvH5e8Q23w9I+NCI5e8aLBN1q8d9I20ejiG2TVWr+rtVZa6y9qrcdHP9NTH5kBGz+2F56icn7fWD7b4H3XvHLhfWFJ89afYNGTiWaVd/4KHc2x/8tfNL5rk/gq+3FtPn8HVs2I/X//nw7l5EC73XX9kyXyWG7jJnjz9/EugHHZOORzU3946HT3NLv+24RLuuciVUgBv0w0GY3hROK/7ax601ix6eXrvOdpvwYphbFV4XNrb5G7SXYp6c4cLZTYMQ0b4LmLE13kfLPQdKHbZkdTota99FlD+KdDJh4gdh48FR7+tiXPFKYCtzqkjRcbvMdyn7/EuK/Xf5BePmuswcyS2Hytv9MW8KnGvLwOsloP8cu8aZpofFTinDT425JM6Uk2ockpjQj4dMm3Bu/xgZn+c1j4KHz2hi27LAeKJvf2Hm43U2HmpMU51bvdGsfD9hB3NBt1fe8fDvn69Jb74C7nWCLZeNHM+AM8fnaSBBmaaKzntLMteXov182TZu1y3ZJnnOK/udkh31Uzk+/3ki/EtGq7dp1Kg9+VLo37y8lcuDPZGqtWeeBhcZBi94PvchLOYzoeFJmUl6UN3k8//g/vNr69hqXN+GXiUGcnzxfHGzd6bPNW49uss7U+fvWyXrneZUcWXjSzpsCKlxK3m1pYpr0DHbWfr5gW0xqzCgPhxUQT8f4SufNIo7eZSoM3r7nT/fCMZS3WbAZZTaH7/u227akEfAaC1K7Be743PZpoitkPvksY+ZX00udqEDNtG7zLzZaVcIvm2brDm7kjU3fEBPmunTUdaxtdp25btZyIbV8WbFvlvi8XfvBOL7PVb8MzP7bknUK7X/4iPH6WEV42GbtioCd5gbpqjHYTjUcBv2WZMTiZ0qkhmQ3exaUxWTvMuljD9prH1syLPySSCxON/b5OYxDb1e4uJhrvpKsBZ3rxTY0zMcPotwftYMdaY4An7qaxa1pZ3Jzmg/LKdfC+afqI5t+6A169Md4F0UkoZ/KCmftf+Mso9/okIy6Jj70ru+dMXDHJBG2WNnjr8W9MtuWd4sXS6DGWiqd7xIOJRkdcXsxJykg1KziZqdJVi02BjsQvTO3mDunVRLMrnwjMvTdFmiQLZjtimmhKEl+m9jSQUwGf04lOXYbrKu0uZPIAtzXAlL2S5+fFBv/cTw1Xy7HfTZI+g/o1biLlC+b138JHDxpujbuK8slE47rAttmjqI8tfL5rtmo4Po21Pj5b0RJI2u5sC3d5kFPlHQl7nDWJt3vYPoC6azDfsu3Jc+MXorASdomPk80gazqTkuxabs2HqY/1LOCjeS973r2du5JmqMHb48G7DrJqitEPvuuwC/hcuEkmXdDXwQavNSx83D2IVLKJPJm8gP66H/x1X5LeKE4vQkcTjY/S1RRQfxmZaLc2z4FViPk9yOrEzlr4+1j3/dmaaFy1thR5O9nDV810vld2pUthu7bmZ3rYWPe7CXcwlBHniqb471C2U9pUNvhkPvOuvQ4PZkkwFlt/4pzky/GZJNjgU6S395BSHViMM1m7lIQHwG1wyWV/tuyS75YHfNUM4wGxd9HNcLgNNRY7aw5MNHH5JdMaO919rZOR1kBTwsHRspMIrlyGm/gkxXSMvLlJOkwWe+dWw2snnXysM1nTdRG1smKaw0blIGzdTDSpNPQ0NXgvx6bjJrn8RW/p0rbBW1++Xuzx4kWTnJQmGj8EqBevC4sGb771rcu1QUzAP3dRLNRorl9Aybht/6jmby3eofzlL8K8+62JvOWf7Mb9fJaZyJKtj4OsbmTjnpcMZbn+bnmlsv07aaaO5i8P+br6uHtsn6sQdSgnDlPApxrAt5qP2tKri+sEMBcTTTYKQ7o2eOsAuJs230WDrAGxwds0wVTd4oxmyHk4xjqTsbKn8dvug+1lYWA/vGistDW4J+9sS/1wgdGdBZgwyfjORoM3j331l9Ek1jQeu9jZkK1m6YrLIKbXvHXEu6upp3zdNPgsz20qL5pMNPhVbzolcC9z18vD4Vn/eCoMHW/bro16JV2Ew62emWrw9t/JBllFg3cn1VqOqaZWZ4uTH7z5Rrbb+BwXXfbRROPE+/9InSa+Av6Vne6Nm0vBvqsMHyfBWPHS1GRlO8XzsTL8cEs+tjo6vRjcXCCzPsepnievAj5FPd79uyWtywvN/nw1boL/XQJTv+ecPqEOXkw0afrBm9ciwYsmibAXG3wSUplo0hnocs/E+z6t4b6TjN9tlhD4rTucF8D200TTZeEJsrHBJ0lTzCYaJzfJhKySmWjcJh1F8+093PgeNC4xndPMykiny0SvLM6tUx0zFZ6p9rdY3JITTDRhWP2WLTQDMVmQcD58NNGkysusQ4KJJslgtAj4JKRrosnkYid7a997gq0cS1qrC9YdX/K2KLDfJho/SRXu145SsH116jQmXr0gsmHT4uT7M9bgvdjg0xxkteZrmgq2rzbCXJjUr4d/jLfkEy1z1QzDLTahHJ8FvL2NJVGxkjI0tDZcZyf39lCuw3l0WgjEzdzo1affiXQ1eFPAJ/SgkploRMC7s+9JUNEjSQIfNPh0Jpa4ucm5rqCU5IapXw9NWxK318yHh74Fnbbeix/yPdkN3NGYOo0VpeAfB9sLiP8bN/GqCzT4hY8l35+1jdrl+qfK280Gb2LuCzXHb9/uMmO3xSWWfzYmQB1J3SZzWcKUq3JpWPKst3ITBjrDzs+8m1uyl1m9bqSrIJp10BESBLlTuTqC+MEn44x7YfxZlg0pBPrdx8IDp6bOt7M9piV4eSgchZMXDSFJl/fvY+HWfRKP+d8lsHqm+8PtWD8fhOYuk5MPg6wmVq3LzU1y/oOpewK+4bFtkYgx+PzxI9ENHh7SuhWxCV8JxUZIGu7B6fopZQvoBrvq7xRkzbo/E5w0+NqF8SEDzHGmVAuJN9Z6n6S4xLbObcRNwLs5DOjYcekS6YTybvFlJ6PTcn1TDbJ+9iZ0tIgGn5okUdvs/xtrE213TrxwhbHg787axIt638mJ6c2LueiJ1Hlb2Wmzy6cjiO1pdwkbG64rM6VJ+07nct3wsjJTnMDTtm+Mrv6LVxg9gUVPxR87/0F41m1CToZ41XB12HAf/d+lxn8vJpq59xg9r1v3g9l3xu9b/bY3E42dDptGb16bjhaXemch4N/5qxGO2UrDerjn+Nh/01Ms1bV/8Jve15m1jyVoDeVViencXirm2gRb7YvNeFHAwlBebfmfol0hMxKodhbq5s/P34FHvgPbPhUBn5JkJyjTLqm5aESkM/HhWjc7/v8nLzs/OF4eJvtkKLebbulz0LI9dX5OTBntbdZeKtLNY85dqdP02zv220mDtz5Qz/4k9gCBIfgXPZ5enVLhVQC62WaTuUmCEeO9aZMRldLKi1e4a4fbPzdmXzrRYbsmZpl2wW/StMl5uxeWejCpmBp8qhAAkPnSjNohrAM49GaizJoCb/3ZWEQlLh+PXjRWDd5pARkroZZY3tZ7yR5naNc8EMRNMiVxporoSV33QTSiYJIHtmmLMcgz/4F4wQEx//FUtlGAx850uVkyGdB1yKd+PTx1ATw9Kf38TNp3uu/TGjYtif12I20TjVNZ0fYNjQYDq94jvh72/BMG0X12I00gSdsaLeMobjOovb4gnOYnuHnRPP0j93zs4zC7TDQuAn7HGve8Zv4pcw3fPK60wvj2Yn7xqsEnlBVxfjmsfdf9mEwVHG3T4FNhhuSYc5ehkDhnGpvoCCLgU2J9c4dDsGW54ar4z0OSCwQzpOyLV8LNg+L3mRqIkwbvRKYavJdjzK5n/froBp/WfzWZew/8+yvxWoWVsugNnq6Jxok3fmscbz7cjoOsFhI0ZR/Xa3Ui2f0y65bYb6tpadGTzsclHVQNJ/bI3Ew0bm2OhHG9B+yavUky75a3/2wsgfncxe5p3GjdYXzM3nTCi8epLh60fCciYQ+DuDacevmz7/BWVjoC3gtaG+fKREw0KajoHvv9xmS484jY/0yE0fRrY78jYW/xWrKJ+xF/UPJtG+ZD3fJY/pnEkrFTu9D4dhvIrIh2Udt90OBbthkvYPPcWAW4k4nGrgmaval1lpjprTvgX4fDp7bVsdKhtd7ozZlrxzph1bqtg5jPXxy7Dm52Vyc2L4n/79hTVFBa6Xz8ipfczRxuNvjVM5PX6e2/GCuNpcvdxxoho832e9HgkwbxS4KbBp8MJyHasD5xm51ION5E4wcrXjQiWZqIgE+F5UGqXWDblYEAtK4yFAl5zMMnbTpZWUrBsv/F/odD3jUZTy5hGsd2lHj0jPCKUjHBHuk0HqK171vKTmKiMYWg1Q96+2pjAG3qdzOvkxk3aM6/nffvWAuLLYO8IasAtUxqiVvUIsU5T5jA46Bdt++MD5Vrx+36u12rlDHnMzQX1K+N/khDwDu5AHshHErfvOM11r6dSCeUOQzoZsNHD8X/FwGfgmQ2cjeB+f6/4lfbMdm4wJa3VxONXzb4FKaevpa49JGwd00mmX1y11qZDmWveRdKSo3f1kkcWaHiTTTv3gb3fz1mInIbZAXna2EfP8kIe5tsgu7+r8f/t9q4rbMW09Es7fV2EtbJQvo65WGSqfkj2zEOczDYScD3twW2a7YI+L2O815G6470Xw5Oyyx6QYddQoz4SW5s8MEINgaZ3ZSv3Zi4LRKBu4+xbev05j/raKIxv9MRiNpYnOPT1x3ytt0IkZA/WrXpqtm8Ld42CPDAN6D3F4zfmXo92LnzS7HfkXBsdqn50LbVx/bbbcZOmlvIxRzhhXAIbuoPg8cl7nvsh9C9P5z2z8SFzBPcE02Tk/UcpZoUYxfwGYwvuPm7Z3pf+DWI7VR+iU1QNtUlpvHC+g8MpaAriHSCKs1tGRJNMgWpovR5xUmDCvugwafTPdQReO/v7vWyakaRTmhY5z3vVMx0iD0OMVul06pA2RIJxQbkrNE237sdRh0N//2qLb1DJMGpZ2Revulj7RTCwIwdP+ZUEtqcYKKJ1ivVoLEVu4DP5AXq5hqY6TXyaxDbqS2lNpHTnKGJxh6moLpPvGJS0dN9kDldIpFYDzZXiIBPQao4215xXPXIowbv6PamDU+dZOuDOh1jZ0F00Eup+AcnHDJ8pLsK30w01jxDMXOC6WIH8PpvnNP77UVjznewY11ybcNHifvfu91Wr6gw//wdY7D2qw49RDv2Wa2ZuA36JchM3M5HurTvNDxyrN409nDZVlfByl7e8173QfL9X/wezLvPe37JiHQWrYAPhA3+zy+vYEezy/RvSE+Df/m6xG0PnAKfv5362Hf+mrito9GycpNXHITn6reM760r47W+SDh5vHe/+fR1wwPGTyKdMSHgGBfcnt5hLddscHUbtOTvNGuyZq4lqcUGb5q7Zt6c+kWYYKLJQMC7avB5ZudGw7PmvhNj26wvcDtfONx9nx27SW6ixevtO/f4Oyiqw97WccgG8YN35+HZa2hqTTZin4a2uWCqy/YM3MZMnk/Tp3ja/yVuswqCN38X+x0Jucc2yQV1yw0X1HQXOk9GZ5t7YCwnnEw02eDWljKLa2KdfYp7PKFwhI/WOswyto4lOB6YpgbfrV/ito0OvYtCwCmEcS4E5eQGOPLS2P9hh/g7KNolNngR8K5UlZei/bLBu5HpjDu/cBswi3RmN8CYKZ7K9HjTNm2O+fV7we9r4TZ139rGFBEoOyOwYK3DS8o+YJ1Qhk3Ap7LBj/5q4rZUZeSLZocBVCcB361/7He6mve47yVuKylL3lNIl1CbkadTWX4hJhp3qspL0ckm+/gh4PP9ELlp6eEu1uDTYV+HgGx+sPqt6EzYHM9oTQOFpp9KEgrCibLq+Jmx3Qek7hnZoyoWG06a9Z4TjW+t4UevJj/+2Btg9LGx/4c7BJorLYeDz8m4igk0bTJi3H/3Hm/pJ72c6C2Uihz1EAIi4EuSD7z5aU7IF2529k2L4J1b3Y9L90bzk5P/mJt8X70BljyTuZ93DqhSIY4pWZjeQZ22F3Nznbcop8WM0/1onSU6dDxctRj2O8X432dUfNpew+LHNexeOWBo231Gwdk+vgy9athnPgYjv5x+GAXrTHwfCYiAL02upfvlu51P3F5gc1N4CmRqi/ypD4LGz26ynWd+DDNcXDrzxB7KJbiXE4ecF/+/PDcPeMHhJJDt9+geI2LjH3bBWtEt/ll3emGYZiA/7z8HL6qwdjBBZmpq8TvWTZRACPjqVDZ4v6bXe2XU0V1XlusCB1Eytc8PHgfH/TqzY028dDv3Oj51Gq/0HpF9Hl5c9cq7J2qW6TLkoPj/1oia6XDRW7HfFyaJMfOjFLNhu4qeQxK3OQlic5zFLjDLbILQSYExt1kHyY/7lfc6OuEQpnuJ3tP48ZUrYxszfd5EwLtTVV6aPOBWKp9ZvznMIfxBrki3K+iFL1+RuGBwJnjxmMhWUFqZNB32PsF9/+Avuu8zQxeXuQT1snP5x97r5YRd86zaIzHNwANjv3sMhkvnwogjY9v67AlDD4aT/ghfm+w8E7eqd7Q8D9fCLjxzQXUfONPmkbarbtbwFNEe68HnGiacXtEFx8sq4000Tu0yz60p6Icc5F2zPumP0Hd04nYHb6geRE1sZt0g82cmRz24gAj4ElSyE7vy5dwU7DbiX90nN+VZ6TEo+X6nm9Qr5oNhHcT85u2wz4mJaXuPgGNsq+10H2gICy+TQ/zSXI64FPb4QvIyK3u67zMFodeY4SVZPjo9B8Pww2L/++6ZmOZCy5yAoeNhwL5w3gvG/72Og/OjUS+PvBSOutpZ2O0x0vj24oZ39DXx/y+b55zOCyfeHP9/eNTHXZUY9vUfWhZpcaqbee8N2A9urIXew4z/ZVXEvQgcBbzNRBO2TFT8wpcS01s54hK4xGHeik0zbzjuFjbr6HNu7X0d+J3k+bshGrw73SrKvL85z/BpdhvEhIKdAfv7V4YbqV4iA8ZknrepAVm1nrJq5650t76J2tE1y+D6dc5a0762gF2m+ezEP8DhFxm/uw9Iv869HLr+dsx4Ok6Ymnsyk9ewQ+P/D40uJh598c0IjzcGAZ3o1i92vxx1jXHM6ZZY5L2GJh5jfehNF72yCvjVFjj7GeOFZsVJUHoR7IPGws8/TTSX9d8HbnDwZTe5YLrz9vLuiXUz7x2ljM9+X088zopporEv/1daYRtkdbLBl8aXGe6ICfjRXzXMVeYLx45ZP5PJUeeGY6ITICe9DKf9k6Yvnsftnd8lopXxor5wBlw2P/WL/9t3w5GXJW73OxxxlEAI+MG9q+gMRy9gKq8Ra3cqW5zeuuO+Dz1TaNdeSSaQki1ArUrdl/Y74/7Y7xNvhu9bwpaabo0lpbR2hIl0HxjbV17lbL4oKYV6Wyyc0nJDEDlp0/32iv9vnqs+o+CgM6PHezSTWEmmnZucMsV9XyoFYZ8T4eS/mImNr3OfMwRg9IX0QWR/+JbLEoXdB8bMMAecbgiRfvsY9tvLP0o9ljLOEmunrNJdkHz58uT5WBkfdSXUGnoMdL5eyQbpew523h7uSJxda9Y3hamkvTPMlsa2mFOBWSdT4JeWpR5kNQV0nIA3XxilMOJLcI7Fw+bYG+wZRNNGXy6TG+CrvzR+j/wyHHIekYhmjt6f0e1Tjd7XsEOh/94kcJUtvlFJqRG8zsS0Ang1DaZJIAS8UlCCcdF3VKbQ5Kwn147VNnjWU+7pTJxMNN36Gt/ZuCeOPArOehKuXuKeJpnr5xUfx0wNx/4yft9YSxfyy5cZwsbEHPiLhNn/N69w5yKLplRW7Sx4VYnRtT/oh/DTWXDNitg+a/d5xJejeXfGbzvsJ4Zr2ZhTY+fM681+wXTDb/qoq+Ggs8wKJaYbfazRo6hKMoBqCo3jfgUn/Sm+DRe9bdTRLgCr+0BlD8LRXsnHkX1gtC0SKRga+1mPWwZwo+e1pARO+L3x0jND/iYImzQ51L6ko+18HHqB5ff5trRO6wA4mEAOPgcuft+o949fNyJtWomEHJYLjNYj2cC71lz26MccfvObaLsGv+t/eXw9nbxydu0zBXwo8YVR1Rt+8Tn8ZjscazMxlpbB0T9POmgdjnicRV3RA077l+EZdNTVsP9psdAYR/8fXLvKuAesvv0+EggBP+nLe1IaFfA1zSma1G8vtNNsQIh1uVHeYlM7CaJMFzCwMmkaD23bjyUbHHzfvdj3+4yMCfixDgtgXDYfrlyUuD0qYDtDxsvj1k8tGnxVb0Mrt1PV23jQv/1v4wVhNZVYH2bz5WGu6dpvbzj/BaM9Y06JvqXNrrXHl+Oor8CII4wBxjKLCcDON2+Hqt7MXGG5Nmc9GZ/G7PZ36wdHXhLbfuFMw/5dWkakMmpisdljQ/ucwui2R5irbWYxc+D2a781eijmC8YpdowZh2Xfk5xauovWjjB1jUm8whJ6MhZBNLnBOBcmNpNbR8hBabBo3FuGnwBjz4Cv/Q4GHRir98HnJh538Nlw+EVocwzArEcKDf71Zcaat+FO04vGXIfANNGU2wZZk9wrThq89Z7s1td9zOb4X8MQ90H5sKUOneEkvb/ybnDIuYYmb96nfaLnpP++UNnD6MUlUz6yIKcCXil1slLqE6XUZ0qp61MfkRmDe1ex+eg/sioyhNmRA5OmHXX9NH62Yrzjvo83GlpHx7DD+WxbKw+f7CAELTSFjZvrvfCBXNz3XgCW9D+ZtduaPUVJaTzqRuoPujBh+ytLNvGb/y3l1H++S833bHbOy1PEHfnh46zf3oJuj74cLANAHRd/yMOz1/A5Q6DPSHT0Jg2f+EcaTv0vHT0M4XzXLNP8o2gsM3okzWW92NBumKTaDjoPffFs6g6/nkOXfJf3PttKfUsHSzc2ELFoNo/MrYnVyxTAZn0OPhdKy2kLhbnmiQWs3dYc06I9ejys3NzIba+vZPILS2np6OTh2WvY2mQTUpMbaKgahtaaSQ/MpV1HNb5BYw3tKkp7hVGv1rLehCwP7PH/+IB3P92K1pqnVxvHdiqjLaFwhLrGdkLhCBGnR2nSdENLBDbUt7KmW/TerOyRkLT+wPPgyoW7lIyIi032nHvncNjNxrKE0xbVMneNYYoLRzThiKahtA+TQxYfe7ONUUHWFrIMnEe3haMeaKs3O8zEtbwwP57wVzjj3rhe8Py129nSZHnhVPQwTH8V3Vl92G/Z0GKU0Y6X3lns3unoMK6jjtaxPWQI+BClNPSNeQvNWl3vnp1pQh08lu1NhsfLlmZD0GutuWPmZ6zcbChCy77/DnNONgawT7n9HU647W1qdiS6PK7Z2szOtlDcff6vmZ/x0Ow1zi1yGrca9z3C50+HL/7Ave4+kbMQaUqpUuAO4ASgBpirlHpBa70sF+VN+Nr3eGngUdz62IdcVDbNMc1J7X8GYCexh+eF8JGcVjobgG8/sJKvl1zJe6sOZOdtxspC50atMN9ov5lplfHhX9+uLeEbpbCFPXh5YzWjeBReBV59i+vLTuZnZUnW9gSmzKxhavhrXF+2gQvLprM0MpL7Sr7DM4/EYl0f9XA9a6J1uLjjSl77/buscnHe+dGoN5hx/xZgJg+Xf4GjSxs46+HlmIanA/+xilDYuDErykro6DQF2SgASqjm3NLzmR6ODUCVhZpAwdH/WMA3S1v4XTk8PX8Dv5rzOWBoOGffE/M6OGxUH6rKS3nn062A5pxoXR/8cCPnA7e9s4V7w/fS/FIVJ6yeR11jOwvW1/PsxxuYtGcDvwWWbWrmAJu8nNp5PEPUNuZF9mUHPamgkwf/FlsgfFVdE+98upVB5Ts40aKU/fK5xTw6JzZG0EQ1lTRyyWMLOHD885y+9CqGNy/h5GUncnxnFfc+XsHQV97ivWj6Rt2Nc+6Nte+lkuvYoPvztZeX88KCjdQ2xNvOr3lyAWNHTWHrzhbWPbWcj9fV062ilE+3NFHK0RyiBlP1cgf7DFzGi4s20qdbOT84bAQ3vbSMbhWlnD5+Jx+3/5ltbb3Y89+zMfsZ1z29iMb2EPPXGiEzXlq0kcseNVw1/3veBC58aB57DejO4Xv2ZWHE6Emsjgzmto7LGF8+giVvhXl+ofFcmPfTb176hN8DC7Z08qe73ueI0vVY+yCjrp/Gt8YP5e/R/z99fAk8voSv7T+QQ0f2RSkjkqs1z1E774YX4JaK9fzi6UW8XBGCEnix11n07TeQ+xaMYe/NS1lWu3NX2xZtaOCLwJPz1gOGHfu9WsUJpfDM4h302FnLrXWXcHbpm/z3v5+zZecxHFo2kprO3mx4cCH3T6qgR2UZA0pGMCqyjleW1NK7uoJFNfWERt/JJ+FhfGP1/ZwMvLy0juEjNrNycxNTXv2EKa9+wllfGsGjc4z1Dl4d1ciyWuNFd9RfDBPNyQcO5otf6M2by7fsOv8/mBAbH/v7G8ZSjx+t3cGIvt04fM9+HKorqFYdTFuyiZF9u/PK0lpq69s449Dh/P6lZazY1AhMp7xU8ZfvfpGTxw42nEV8Rmk/43pbM1bqSGCy1vqk6P8bALTWf3I7ZsKECXrevCxcszC6sB3v3UnlZ9Oo2mAI7vWRATwS/hp3R77JISP60G/HAu7uMGydT504h3nzP6Bnzz1o7DGaFZt2srAmZhq5oWwqPy2bxhkDp/P0FmP69GOdX+XdyDhW6uG8XvkLLu64kpcjdvcr47yOVJvpTTPVdNBEVdxL4qKOq3ktchhVtPOrske4pfMH7CRRu1tTZdiXR7U9uuv/ysgw9i2JeThMCx/OpaGrdv3vQQsj1RZqu+3LR+Ez4o5Ph0WVP6GXamHvtocYobYwo/Ln/Kjj58yIeItvv6bqLBZHRvGvzm/zn4q/cWnHFUyLHOGYdhDbmVN1Gf/qPJ1vl77LMLWNOzpP49KyF+LqPqBnJVub2h0j8V5c+gLXlRsueJt0H45ovyNu//SKGzigZC1Htd9OjR5AFe2MVrUs06Pi0r1VcTWjSjZndM78xH7tvdCdVt6ovJarQ5fwQeSAhP13lv+dU0o/ZFTbVC4pfYFnw0exiX6MUJuZVXk1AG+ED+YnoWt31eHV8AR+GromIS+Ts0rfZLvuySuReO+UQ9RKbih/lLM7bqSDeHOK2bY/hn7IL8sf49rQRTwVPhaA3jRxculcngi7mFMdqKaNHrRRxx4J+wazjb9X3MnPOq6iHg8D8lkyXNWxl9rI25GDUicGeleXs/C3Dm7IHlBKzddaT3Dcl0MBfwZwstb6J9H/5wJf0lpfZkt3EXARwIgRIw5du3ZtQl4ZEw7B3HuMgTyrXVdrI5bJmFOd43xb0XpXN7Vp+QwqPnmeilP+DBXdiEQ09U3NdO9WTWWZoTbW7GhheB9j33urtnLE6H6Ul5bQ1N5JdXkpdTPvpH7wlxndv5r2XntSUV7Ghh1G97Ffj0raQ2F2toXoXlnGkN5GF3P75wvo07IGdeC3CEc0rRuX0r3vMNqb6qGsgqqqamrbyvhwXSPH7juQirISQpEI3cpLKSstgRWG5hbe9xQ6IxHWbG1he3MHowd0p6xEsXprMwd/YQ92tIToVV1GY1snO1tDDOxVRUndcirXv8um/S9AAUN6VbKjtZMNO1ppaA3RGYnQv0clvavLae8MA4qOzggRrdEa+jZ9guozkm2hKoZ2rGJZ53CG9+3O1qZ2+veoZM3WZkYP6E63ijLaQmF6heqo3GMIzS3N6HCYJqrp16OCusZ2GlqNbvoBQ3vR3B6mtqGVjk6j/LXbWhiyRxXVZYrW2uV09hzG3gN7Ux8qZe6aHXSvLOXzrc2MLNvB0LUvUHfQJQzoVUXNjlbGDe/NpoY2KstKUEqxtamdSPM2qlWIbv1H0LOqjB0tHYTCml5VZQzrU01bR4Q125opUYqBvSrpWVXGxvo26ls6qCovZVVdE2MG92J1XRObd7ahgdEDelBZVkI4omnvDKO18WB3RjTN7Z3sO6gnG+pbKS8tYd32ZkKdmgOb59BRPYDWfgdS29DG/kN6EY5odraF2NrUjlKKnpXGuWvpCFNWqjh+/0HU1rfSo6qM2au2MbhXFZsb2+jbvZLNO9voX60YWBmG6j3Ya0APtjS28fG6egb2rKR2/SqGDh/B4D16sLMtTENrCNXZwrgRA1hc20zv6nI272xnY30rI/p1o25nO5XlJazf3sLXxw1hR3MHraEwpUqxtdkws/TrXhG9nypZv70VpaCitITqhk/pXwWzdg7kK81v8kHPE1ElJew7qCfrt7cwqn93mtuN5wENvarLWLGpkfLSEg4Z0Ye9B/Zg2qKNhKPn8a1PtnDkXkY45fqWEDuaOzh2v4E0tIbY0dLBvoN6smlnGwvW1VNaAoeO7ENLR5jtzR10qyijoqyE2vpWKspK6Nu9gpaOMI1tITo6I5SXlrDPoB6s395Kr+oyPtnUxAkHDKJ7ZSmvLt3Emm0tHDqiDzU7WiktgQXr6/nqmIHsPaAHtQ1thCOazTvb2NrUQc+qMnpXl9Pc3smXRvejrrGdjs4wF3zFYS6EB/Il4L8HnGQT8IdrrV39uPzQ4AVBEHYnkgn4XA6y1gBWR+7hwEaXtIIgCILP5FLAzwX2UUrtqZSqAM4EXshheYIgCIKFnHnRaK07lVKXYfiVlAL3aa2X5qo8QRAEIZ6criSrtZ4OuASsEARBEHJJIGayCoIgCImIgBcEQQgoIuAFQRACigh4QRCEgJKziU6ZoJSqAzKdytof2OpjdYoBafPugbQ5+GTT3pFaa8dVcgpKwGeDUmqe22yuoCJt3j2QNgefXLVXTDSCIAgBRQS8IAhCQAmSgL873xXIA9Lm3QNpc/DJSXsDY4MXBEEQ4gmSBi8IgiBYEAEvCIIQUIpewHfVwt5djVLqC0qpmUqp5UqppUqpK6Pb+yqlXldKfRr97mM55oboefhEKXVS/mqfHUqpUqXUx0qpl6L/A91mpdQeSqmnlVIrotf7yN2gzVdH7+slSqnHlFJVQWuzUuo+pdQWpdQSy7a026iUOlQptTi67x9KWVZCT4XWumg/GGGIVwGjgQpgIXBAvuvlU9uGAIdEf/cEVgIHALcA10e3Xw/8Jfr7gGj7K4E9o+elNN/tyLDt1wCPAi9F/we6zcCDwE+ivyuAPYLcZmAY8DlQHf3/JHBB0NoMTAQOAZZYtqXdRuBD4EhAAS8DX/dah2LX4A8HPtNar9ZadwCPA6fnuU6+oLWu1Vp/FP3dCCzHeDBOxxAIRL+/Ff19OvC41rpda/058BnG+SkqlFLDgW8A91g2B7bNSqleGILgXgCtdYfWup4AtzlKGVCtlCoDumGs9haoNmutZwHbbZvTaqNSagjQS2s9WxvS/iHLMSkpdgE/DFhv+V8T3RYolFKjgIOBOcAgrXUtGC8BYGA0WVDOxd+BXwARy7Ygt3k0UAfcHzVL3aOU6k6A26y13gDcCqwDaoEGrfVrBLjNFtJt47Dob/t2TxS7gHeyRQXK71Mp1QN4BrhKa70zWVKHbUV1LpRSpwJbtNbzvR7isK2o2oyhyR4C3KW1Phhoxui6u1H0bY7anU/HMEUMBborpc5JdojDtqJqswfc2phV24tdwAd6YW+lVDmGcJ+qtX42unlztNtG9HtLdHsQzsVXgNOUUmswzG3HKaUeIdhtrgFqtNZzov+fxhD4QW7z14DPtdZ1WusQ8CzwZYLdZpN021gT/W3f7oliF/CBXdg7OlJ+L7Bca32bZdcLwPnR3+cD/7NsP1MpVamU2hPYB2NwpmjQWt+gtR6utR6FcS1naK3PIdht3gSsV0rtF910PLCMALcZwzRzhFKqW/Q+Px5jjCnIbTZJq41RM06jUuqI6Lk6z3JMavI90uzDSPUpGB4mq4Ab810fH9t1FEZXbBGwIPo5BegHvAl8Gv3uaznmxuh5+IQ0RtoL8QMcS8yLJtBtBsYD86LX+nmgz27Q5t8BK4AlwMMY3iOBajPwGMYYQwhDE/9xJm0EJkTP0yrgX0QjEHj5SKgCQRCEgFLsJhpBEATBBRHwgiAIAUUEvCAIQkARAS8IghBQRMALgiAEFBHwgpAmSqljzUiXHtPvp5R6QBm8n8u6CYIVEfCCkHuOBt4BvggszXNdhN0IEfBCIFFKnaOU+lAptUAp9R+lVGl0e5NS6q9KqY+UUm8qpQZEt49XSn2glFqklHrOjNOtlNpbKfWGUmph9Ji9okX0sMRwn+oUo1spdbRSagFGiNifA9OAk5RS87riHAiCCHghcCil9gd+AHxFaz0eCANnR3d3Bz7SWh8CvA38Nrr9IeA6rfUXgcWW7VOBO7TWB2HES6mNbj8YuAojjvdojDg6cWit34mWb8byfwNjhuIEv9oqCMkoy3cFBCEHHA8cCsyNKtbVxII6RYAnor8fAZ5VSvUG9tBavx3d/iDwlFKqJzBMa/0cgNa6DSCa54da65ro/wXAKOBde0WUUt2ANq21VkrtgzENXRC6BBHwQhBRwINa6xs8pE0WqyPZ0mjtlt9hHJ4lpdQLwBhgD6XUIoyXwDyl1J+01k/Y0wuC34iJRggibwJnKKUGwq51MEdG95UAZ0R/nwW8q7VuAHYopY6Obj8XeFsb8fdrlFLfiuZTGdXIPaG1Pg34L3AxcAXwb631eBHuQlchGrwQOLTWy5RSvwJeU0qVYETzuxRYi7GgxoFKqflAA4atHozQrf+OCvDVwKTo9nOB/yilfh/N53tpVmcihn3/IgybvyB0GRJNUtitUEo1aa175LsegtAViIlGEAQhoIgGLwiCEFBEgxcEQQgoIuAFQRACigh4QRCEgCICXhAEIaCIgBcEQQgo/w9snWjBHprF5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(model.history.history['loss'],label='loss')\n",
    "plt.plot(model.history.history['val_loss'],label='val_loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('epoch #')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history.history['accuracy'],label='accuracy')\n",
    "plt.plot(model.history.history['val_accuracy'],label='val_accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('epoch #')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: FFNN_iris.model\\assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "\n",
    "model.save('FFNN_iris.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only weights & bias\n",
    "\n",
    "model.save_weights('FFNN_iris_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

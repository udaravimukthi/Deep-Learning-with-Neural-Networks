{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width  species\n",
      "0           5.1          3.5           1.4          0.2        0\n",
      "1           4.9          3.0           1.4          0.2        0\n",
      "2           4.7          3.2           1.3          0.2        0\n",
      "3           4.6          3.1           1.5          0.2        0\n",
      "4           5.0          3.6           1.4          0.2        0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset=pd.read_csv('iris.csv')\n",
    "print(dataset.head())\n",
    "dataset=dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset[:,0:4]\n",
    "target=dataset[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-nightly~=2.5.0.dev in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard~=2.5 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py~=3.1.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied, skipping upgrade: wheel~=0.35 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.4.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: grpcio~=1.34.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: six~=1.15.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.32.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (57.0.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.6)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied, skipping upgrade: chardet<5,>=3.0.2 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: keras in c:\\users\\udara vimukthi\\appdata\\roaming\\python\\python38\\site-packages (2.4.3)\n",
      "Requirement already satisfied, skipping upgrade: h5py in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from keras) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in c:\\users\\udara vimukthi\\anaconda3\\lib\\site-packages (from keras) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#categorical encoding\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "target_new=to_categorical(target)\n",
    "print(target_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 17,091\n",
      "Trainable params: 17,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model=Sequential()\n",
    "#an empty NN created\n",
    "\n",
    "model.add(Dense(64,input_dim=4,activation='relu')) #first hidden layer\n",
    "model.add(Dense(128,activation='relu'))  #2nd hidden layer\n",
    "model.add(Dense(64,activation='relu'))  #3rd hidden layer\n",
    "model.add(Dense(3,activation='softmax')) #output layer\n",
    "\n",
    "#because of classification problem, we have to use categorical crossentropy \n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense [array([[ 0.18063954, -0.08357798, -0.0309988 , -0.09625378,  0.09015864,\n",
      "         0.1962637 ,  0.27589452, -0.01781619, -0.06412221,  0.00828922,\n",
      "         0.00671834, -0.05607086, -0.29006064, -0.1383791 ,  0.26545644,\n",
      "        -0.0478885 ,  0.2888137 , -0.28381225,  0.06422916,  0.06552836,\n",
      "         0.18696195,  0.25067025, -0.16883568, -0.15963528,  0.22148615,\n",
      "        -0.04663888, -0.10330896, -0.21609007, -0.27347332,  0.02182919,\n",
      "        -0.04377487,  0.1235992 , -0.2334814 , -0.23571737, -0.24055529,\n",
      "         0.13489422, -0.229276  ,  0.2313593 ,  0.01251838, -0.06997633,\n",
      "         0.25082487, -0.18498519,  0.23847818,  0.12427455,  0.01723987,\n",
      "        -0.12266523, -0.08436169, -0.05025604,  0.10992846,  0.19123477,\n",
      "        -0.01983133,  0.28060287, -0.13842988,  0.03337973,  0.02286473,\n",
      "        -0.1686131 , -0.1251531 , -0.05764373, -0.0488147 , -0.29562575,\n",
      "        -0.23234805, -0.18456097, -0.013877  ,  0.20698887],\n",
      "       [ 0.22003669, -0.01506317, -0.13775702,  0.03635871, -0.1078915 ,\n",
      "        -0.09740406, -0.2867162 , -0.10212143, -0.10261393,  0.05473447,\n",
      "        -0.06625471, -0.23143616,  0.1571511 , -0.00313064, -0.12670301,\n",
      "         0.15163988,  0.29021907,  0.14553422,  0.20293057,  0.03380847,\n",
      "        -0.00559336,  0.19106168, -0.261913  ,  0.28294122,  0.1558817 ,\n",
      "         0.23562896,  0.12094316,  0.19756588,  0.14539576, -0.11863552,\n",
      "         0.16416916,  0.25312924,  0.22744042,  0.19541702, -0.1614616 ,\n",
      "        -0.06783755, -0.1473111 ,  0.18711278, -0.06480648,  0.05355638,\n",
      "         0.03107232,  0.14657244, -0.23565426,  0.2329309 ,  0.10681862,\n",
      "         0.28371078, -0.09695067,  0.01215455, -0.07433139,  0.22919035,\n",
      "         0.00745183,  0.16037706, -0.04070798,  0.05616945,  0.28445655,\n",
      "        -0.21292678,  0.19973773,  0.10804313, -0.23611163, -0.14457217,\n",
      "         0.03038868,  0.23269033, -0.2731261 ,  0.12869936],\n",
      "       [ 0.04909706,  0.08575508, -0.03427121, -0.13866691, -0.19785525,\n",
      "        -0.02947721,  0.26574254,  0.23164368, -0.10564342,  0.21362531,\n",
      "         0.22459155, -0.2653618 ,  0.22887743,  0.19966337, -0.16169015,\n",
      "         0.22763252, -0.06546541,  0.24889666,  0.23233116, -0.07863879,\n",
      "        -0.0581511 ,  0.2585411 , -0.10523501, -0.08725429, -0.29443914,\n",
      "         0.22087121, -0.02623531, -0.00990862, -0.13014065, -0.06356542,\n",
      "         0.24873531, -0.13469988,  0.20157602, -0.02484319,  0.06166238,\n",
      "        -0.22904915,  0.10331666, -0.19569366,  0.03383157,  0.22013307,\n",
      "        -0.26293376,  0.04986164,  0.08158699,  0.2511441 , -0.28775096,\n",
      "         0.10810891, -0.01410007,  0.06542644, -0.0551616 , -0.04818851,\n",
      "        -0.29093698, -0.14388253, -0.24196321, -0.24105754,  0.08899662,\n",
      "         0.16030064, -0.084925  , -0.15913826, -0.19858018,  0.21456069,\n",
      "         0.07111323, -0.15753381,  0.22320783, -0.28656265],\n",
      "       [-0.21305986,  0.1782155 , -0.03213879, -0.11505806,  0.02754387,\n",
      "         0.1499224 ,  0.15343952, -0.10121131,  0.13245961,  0.26947296,\n",
      "        -0.0886268 , -0.22862104,  0.24339187, -0.2436253 , -0.08018771,\n",
      "         0.10702223,  0.12350106, -0.08915888,  0.00945523,  0.25438088,\n",
      "         0.23441905, -0.17289776,  0.0754607 ,  0.11204514, -0.12027487,\n",
      "        -0.28089783, -0.1839415 ,  0.09391204, -0.08585799,  0.2655794 ,\n",
      "        -0.2137318 ,  0.25825465, -0.06075191, -0.20325392, -0.07750197,\n",
      "        -0.0319975 , -0.03697091,  0.08228841, -0.09273264, -0.10255586,\n",
      "         0.20497793,  0.0065141 ,  0.20165303, -0.22060299, -0.19426838,\n",
      "        -0.12487313,  0.02403823,  0.23275203, -0.06297944, -0.29325175,\n",
      "        -0.18179229,  0.03386796,  0.29421508,  0.02055866, -0.21733233,\n",
      "        -0.01711655,  0.15437767, -0.27884358, -0.20509377, -0.07166252,\n",
      "        -0.04680508,  0.10574052, -0.26090997, -0.28002155]],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "dense_1 [array([[-0.11013038, -0.17611769, -0.10109587, ..., -0.13192141,\n",
      "         0.09087951,  0.13586678],\n",
      "       [-0.03438126,  0.10527049, -0.15335771, ..., -0.10753945,\n",
      "        -0.01146449, -0.13163477],\n",
      "       [-0.14796439,  0.08999364,  0.16488336, ...,  0.07684551,\n",
      "         0.12148573, -0.027784  ],\n",
      "       ...,\n",
      "       [-0.141918  , -0.01723249, -0.01958682, ...,  0.09827988,\n",
      "         0.0768664 ,  0.17660476],\n",
      "       [-0.16271313, -0.014991  ,  0.1220157 , ...,  0.09138651,\n",
      "        -0.17376517, -0.06975851],\n",
      "       [-0.03595489,  0.03848197, -0.09316961, ..., -0.09798615,\n",
      "         0.0882035 , -0.02942339]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "dense_2 [array([[ 0.13866355, -0.00437307,  0.13654558, ..., -0.00639878,\n",
      "        -0.15839338,  0.09359719],\n",
      "       [ 0.13185842, -0.01125388,  0.12273963, ...,  0.07696064,\n",
      "         0.07727726,  0.05063443],\n",
      "       [-0.03503537,  0.17562251, -0.09656985, ...,  0.03762756,\n",
      "         0.02339272,  0.05782624],\n",
      "       ...,\n",
      "       [-0.02064256,  0.03514908,  0.11807032, ...,  0.1128809 ,\n",
      "         0.1722809 , -0.14083627],\n",
      "       [ 0.13787843, -0.11418045,  0.06242348, ...,  0.11951728,\n",
      "         0.10583822,  0.05716474],\n",
      "       [-0.0069689 , -0.10652435, -0.15781774, ...,  0.06203742,\n",
      "         0.01991801, -0.09843746]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "dense_3 [array([[-0.08720118, -0.2596181 ,  0.17505571],\n",
      "       [-0.2630651 , -0.23755348,  0.11815497],\n",
      "       [-0.00248125, -0.08958212,  0.1964336 ],\n",
      "       [-0.13000993, -0.11341865, -0.16575   ],\n",
      "       [ 0.2670542 ,  0.2812935 , -0.15681365],\n",
      "       [ 0.05914208, -0.06657122,  0.1956056 ],\n",
      "       [ 0.12841982,  0.15643737, -0.28136295],\n",
      "       [ 0.28521508,  0.00584692,  0.09424737],\n",
      "       [-0.14835228, -0.25517702,  0.06554189],\n",
      "       [ 0.2897395 ,  0.03862941, -0.24921621],\n",
      "       [-0.14744046,  0.01373154, -0.2464413 ],\n",
      "       [-0.10094491, -0.20496576, -0.22236097],\n",
      "       [ 0.13988513,  0.125166  , -0.11962353],\n",
      "       [ 0.2129494 ,  0.07953647, -0.03955838],\n",
      "       [-0.19278724,  0.18128368, -0.19751665],\n",
      "       [-0.20820272,  0.19952402,  0.06577227],\n",
      "       [ 0.22869688,  0.1063706 ,  0.05074865],\n",
      "       [ 0.11844787, -0.16443628, -0.27841514],\n",
      "       [-0.17446572, -0.01420471, -0.12204705],\n",
      "       [-0.15332362,  0.04921255,  0.25158644],\n",
      "       [ 0.08841595, -0.20143905,  0.23859686],\n",
      "       [-0.07026073,  0.13059369, -0.04222113],\n",
      "       [-0.14170305,  0.27086997, -0.24861832],\n",
      "       [ 0.26992244, -0.02653283,  0.16369021],\n",
      "       [ 0.05515093, -0.1682986 ,  0.11111841],\n",
      "       [ 0.23723418,  0.0790526 , -0.03810009],\n",
      "       [-0.13850199, -0.04916003,  0.10880527],\n",
      "       [ 0.01546577,  0.14733586,  0.14783251],\n",
      "       [-0.15953156,  0.02091914, -0.19451156],\n",
      "       [ 0.2903545 ,  0.10776645, -0.19635355],\n",
      "       [-0.09035781, -0.26721045,  0.09009868],\n",
      "       [ 0.06211948,  0.03247899,  0.2511704 ],\n",
      "       [-0.04660064,  0.04072106, -0.2602074 ],\n",
      "       [ 0.23369706, -0.08776419, -0.02254987],\n",
      "       [ 0.23062485, -0.22156687, -0.27134377],\n",
      "       [-0.18768713,  0.04854152, -0.21126187],\n",
      "       [-0.1574565 ,  0.09592718,  0.03814855],\n",
      "       [-0.14528449, -0.08160569, -0.09234227],\n",
      "       [ 0.22746748, -0.01702601,  0.02196223],\n",
      "       [-0.25293106,  0.2145893 ,  0.21336746],\n",
      "       [-0.25722674,  0.11679289, -0.15705182],\n",
      "       [-0.18630484,  0.27027315,  0.02932307],\n",
      "       [ 0.0592325 , -0.2738175 ,  0.05578563],\n",
      "       [ 0.15903348, -0.10227819, -0.15439655],\n",
      "       [-0.08606897,  0.1947822 , -0.14164741],\n",
      "       [-0.27414066,  0.01661733, -0.26947388],\n",
      "       [-0.13302507, -0.09321371, -0.09347811],\n",
      "       [ 0.00351015, -0.228551  ,  0.09537745],\n",
      "       [ 0.13173667, -0.05125222, -0.04484636],\n",
      "       [ 0.19537038, -0.22665766,  0.26352763],\n",
      "       [ 0.0422506 ,  0.12216714,  0.0611867 ],\n",
      "       [ 0.09976405,  0.20956618,  0.08595148],\n",
      "       [-0.13847573, -0.02947605, -0.12730658],\n",
      "       [-0.03157672,  0.04217818, -0.21211734],\n",
      "       [-0.03054205,  0.04474369, -0.03510836],\n",
      "       [ 0.16848218, -0.23901232, -0.27374005],\n",
      "       [ 0.20212382, -0.0611531 , -0.28721002],\n",
      "       [-0.12087946,  0.04191554, -0.2712322 ],\n",
      "       [-0.16951114, -0.0635996 , -0.20819415],\n",
      "       [ 0.21913236, -0.10885891, -0.00242981],\n",
      "       [ 0.07333931,  0.10846564, -0.10733086],\n",
      "       [-0.2943643 ,  0.05803949,  0.07761794],\n",
      "       [ 0.08267677, -0.06870122,  0.10895509],\n",
      "       [-0.22746196, -0.14442796, -0.27626523]], dtype=float32), array([0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#to see the randomly adjusted weights\n",
    "\n",
    "for layer in model.layers:\n",
    "    parameters= layer.get_weights()\n",
    "    name=layer.name\n",
    "    \n",
    "    print(name, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "120/120 [==============================] - 1s 2ms/step - loss: 0.5903 - accuracy: 0.7750 - val_loss: 1.3466 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2385 - accuracy: 0.9000 - val_loss: 1.1168 - val_accuracy: 0.2333\n",
      "Epoch 3/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9417 - val_loss: 0.8107 - val_accuracy: 0.5333\n",
      "Epoch 4/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.9500 - val_loss: 0.1793 - val_accuracy: 1.0000\n",
      "Epoch 5/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9833 - val_loss: 0.7717 - val_accuracy: 0.6000\n",
      "Epoch 6/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.9583 - val_loss: 0.3722 - val_accuracy: 0.7333\n",
      "Epoch 7/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9667 - val_loss: 1.0697 - val_accuracy: 0.5000\n",
      "Epoch 8/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9583 - val_loss: 0.1442 - val_accuracy: 1.0000\n",
      "Epoch 9/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9667 - val_loss: 0.1222 - val_accuracy: 1.0000\n",
      "Epoch 10/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9750 - val_loss: 0.2760 - val_accuracy: 0.8333\n",
      "Epoch 11/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9583 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 12/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0531 - accuracy: 0.9833 - val_loss: 0.5017 - val_accuracy: 0.7333\n",
      "Epoch 13/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9333 - val_loss: 0.1655 - val_accuracy: 1.0000\n",
      "Epoch 14/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.9583 - val_loss: 0.2416 - val_accuracy: 0.8333\n",
      "Epoch 15/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0535 - accuracy: 0.9750 - val_loss: 0.6848 - val_accuracy: 0.7000\n",
      "Epoch 16/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9667 - val_loss: 0.4289 - val_accuracy: 0.7333\n",
      "Epoch 17/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9750 - val_loss: 0.2017 - val_accuracy: 0.8667\n",
      "Epoch 18/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9500 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
      "Epoch 19/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0623 - accuracy: 0.9667 - val_loss: 3.0827 - val_accuracy: 0.3000\n",
      "Epoch 20/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9500 - val_loss: 0.2763 - val_accuracy: 0.8333\n",
      "Epoch 21/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.9750 - val_loss: 0.0776 - val_accuracy: 1.0000\n",
      "Epoch 22/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.9667 - val_loss: 1.3843 - val_accuracy: 0.5000\n",
      "Epoch 23/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.9583 - val_loss: 0.8309 - val_accuracy: 0.6333\n",
      "Epoch 24/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9667 - val_loss: 0.8599 - val_accuracy: 0.6333\n",
      "Epoch 25/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.9667 - val_loss: 0.3695 - val_accuracy: 0.7667\n",
      "Epoch 26/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9750 - val_loss: 0.0164 - val_accuracy: 1.0000\n",
      "Epoch 27/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9500 - val_loss: 0.3821 - val_accuracy: 0.7333\n",
      "Epoch 28/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9833 - val_loss: 0.1889 - val_accuracy: 0.9333\n",
      "Epoch 29/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0439 - accuracy: 0.9833 - val_loss: 0.3512 - val_accuracy: 0.7667\n",
      "Epoch 30/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0490 - accuracy: 0.9833 - val_loss: 0.2618 - val_accuracy: 0.8333\n",
      "Epoch 31/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0494 - accuracy: 0.9667 - val_loss: 0.4593 - val_accuracy: 0.7667\n",
      "Epoch 32/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9750 - val_loss: 3.8681 - val_accuracy: 0.1333\n",
      "Epoch 33/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9750 - val_loss: 0.3092 - val_accuracy: 0.8000\n",
      "Epoch 34/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.3890 - val_accuracy: 0.7667\n",
      "Epoch 35/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9667 - val_loss: 0.0902 - val_accuracy: 1.0000\n",
      "Epoch 36/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0564 - accuracy: 0.9750 - val_loss: 0.1039 - val_accuracy: 1.0000\n",
      "Epoch 37/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0462 - accuracy: 0.9750 - val_loss: 0.4249 - val_accuracy: 0.7667\n",
      "Epoch 38/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9917 - val_loss: 0.7021 - val_accuracy: 0.7000\n",
      "Epoch 39/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0452 - accuracy: 0.9833 - val_loss: 0.4216 - val_accuracy: 0.7667\n",
      "Epoch 40/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9750 - val_loss: 0.3880 - val_accuracy: 0.8000\n",
      "Epoch 41/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0464 - accuracy: 0.9833 - val_loss: 0.3131 - val_accuracy: 0.8000\n",
      "Epoch 42/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0456 - accuracy: 0.9833 - val_loss: 0.6238 - val_accuracy: 0.7333\n",
      "Epoch 43/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9750 - val_loss: 0.4515 - val_accuracy: 0.7667\n",
      "Epoch 44/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.9833 - val_loss: 0.2845 - val_accuracy: 0.8333\n",
      "Epoch 45/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0611 - accuracy: 0.9667 - val_loss: 1.4870 - val_accuracy: 0.5333\n",
      "Epoch 46/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9583 - val_loss: 0.1105 - val_accuracy: 0.9667\n",
      "Epoch 47/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.4997 - val_accuracy: 0.7667\n",
      "Epoch 48/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.9750 - val_loss: 0.1197 - val_accuracy: 0.9333\n",
      "Epoch 49/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.9750 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 50/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9750 - val_loss: 0.5842 - val_accuracy: 0.7333\n",
      "Epoch 51/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0440 - accuracy: 0.9750 - val_loss: 1.0268 - val_accuracy: 0.6667\n",
      "Epoch 52/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0934 - val_accuracy: 0.9667\n",
      "Epoch 53/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.9667 - val_loss: 0.8407 - val_accuracy: 0.6667\n",
      "Epoch 54/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.9750 - val_loss: 1.1436 - val_accuracy: 0.6000\n",
      "Epoch 55/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.9667 - val_loss: 0.4584 - val_accuracy: 0.7667\n",
      "Epoch 56/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0521 - accuracy: 0.9833 - val_loss: 0.0761 - val_accuracy: 1.0000\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0344 - accuracy: 0.9917 - val_loss: 1.1014 - val_accuracy: 0.7000\n",
      "Epoch 58/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0422 - accuracy: 0.9917 - val_loss: 0.4693 - val_accuracy: 0.8000\n",
      "Epoch 59/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0416 - accuracy: 0.9750 - val_loss: 1.2370 - val_accuracy: 0.5667\n",
      "Epoch 60/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.9667 - val_loss: 0.0845 - val_accuracy: 1.0000\n",
      "Epoch 61/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0518 - accuracy: 0.9750 - val_loss: 0.2601 - val_accuracy: 0.8667\n",
      "Epoch 62/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.9750 - val_loss: 0.2559 - val_accuracy: 0.8667\n",
      "Epoch 63/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0287 - accuracy: 0.9917 - val_loss: 0.7564 - val_accuracy: 0.7000\n",
      "Epoch 64/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0394 - accuracy: 0.9917 - val_loss: 0.0697 - val_accuracy: 0.9667\n",
      "Epoch 65/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9750 - val_loss: 0.4960 - val_accuracy: 0.7667\n",
      "Epoch 66/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9833 - val_loss: 0.1795 - val_accuracy: 0.9000\n",
      "Epoch 67/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0389 - accuracy: 0.9917 - val_loss: 1.0378 - val_accuracy: 0.5667\n",
      "Epoch 68/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0492 - accuracy: 0.9750 - val_loss: 0.0360 - val_accuracy: 1.0000\n",
      "Epoch 69/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9750 - val_loss: 0.1086 - val_accuracy: 0.9667\n",
      "Epoch 70/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.9750 - val_loss: 0.5459 - val_accuracy: 0.7333\n",
      "Epoch 71/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0447 - accuracy: 0.9750 - val_loss: 0.1554 - val_accuracy: 0.9333\n",
      "Epoch 72/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0291 - accuracy: 0.9833 - val_loss: 0.2185 - val_accuracy: 0.9000\n",
      "Epoch 73/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0566 - accuracy: 0.9667 - val_loss: 0.4916 - val_accuracy: 0.7667\n",
      "Epoch 74/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9750 - val_loss: 0.1854 - val_accuracy: 0.9333\n",
      "Epoch 75/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0424 - accuracy: 0.9833 - val_loss: 0.4536 - val_accuracy: 0.8000\n",
      "Epoch 76/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0341 - accuracy: 0.9750 - val_loss: 0.1464 - val_accuracy: 0.9333\n",
      "Epoch 77/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0520 - accuracy: 0.9583 - val_loss: 0.0727 - val_accuracy: 1.0000\n",
      "Epoch 78/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0398 - accuracy: 0.9917 - val_loss: 0.6413 - val_accuracy: 0.7333\n",
      "Epoch 79/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0409 - accuracy: 0.9833 - val_loss: 0.3562 - val_accuracy: 0.8000\n",
      "Epoch 80/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0335 - accuracy: 0.9750 - val_loss: 0.2435 - val_accuracy: 0.9000\n",
      "Epoch 81/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0191 - accuracy: 0.9917 - val_loss: 2.8258 - val_accuracy: 0.4333\n",
      "Epoch 82/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9667 - val_loss: 0.5441 - val_accuracy: 0.7667\n",
      "Epoch 83/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.3025 - val_accuracy: 0.9000\n",
      "Epoch 84/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.9417 - val_loss: 0.1107 - val_accuracy: 1.0000\n",
      "Epoch 85/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0496 - accuracy: 0.9917 - val_loss: 0.6790 - val_accuracy: 0.7333\n",
      "Epoch 86/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0527 - accuracy: 0.9750 - val_loss: 0.4846 - val_accuracy: 0.7333\n",
      "Epoch 87/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0485 - accuracy: 0.9917 - val_loss: 0.5182 - val_accuracy: 0.7333\n",
      "Epoch 88/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0364 - accuracy: 0.9750 - val_loss: 0.3478 - val_accuracy: 0.8000\n",
      "Epoch 89/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0351 - accuracy: 0.9917 - val_loss: 0.8669 - val_accuracy: 0.7333\n",
      "Epoch 90/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9667 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 91/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0867 - val_accuracy: 0.9667\n",
      "Epoch 92/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0392 - accuracy: 0.9750 - val_loss: 1.1739 - val_accuracy: 0.6000\n",
      "Epoch 93/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.9833 - val_loss: 0.3359 - val_accuracy: 0.8000\n",
      "Epoch 94/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0315 - accuracy: 0.9833 - val_loss: 0.2643 - val_accuracy: 0.8333\n",
      "Epoch 95/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 0.9833 - val_loss: 0.2652 - val_accuracy: 0.8667\n",
      "Epoch 96/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0457 - accuracy: 0.9833 - val_loss: 0.1701 - val_accuracy: 0.9333\n",
      "Epoch 97/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0442 - accuracy: 0.9833 - val_loss: 0.0790 - val_accuracy: 0.9667\n",
      "Epoch 98/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.9833 - val_loss: 0.1605 - val_accuracy: 0.9333\n",
      "Epoch 99/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.9750 - val_loss: 0.7764 - val_accuracy: 0.5667\n",
      "Epoch 100/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0335 - accuracy: 0.9917 - val_loss: 0.1056 - val_accuracy: 0.9333\n",
      "Epoch 101/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9750 - val_loss: 0.4164 - val_accuracy: 0.8000\n",
      "Epoch 102/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 0.9917 - val_loss: 2.0129 - val_accuracy: 0.5333\n",
      "Epoch 103/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9583 - val_loss: 0.4282 - val_accuracy: 0.7333\n",
      "Epoch 104/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9917 - val_loss: 0.0988 - val_accuracy: 0.9333\n",
      "Epoch 105/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0443 - accuracy: 0.9750 - val_loss: 0.1091 - val_accuracy: 0.9333\n",
      "Epoch 106/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0478 - accuracy: 0.9833 - val_loss: 0.0459 - val_accuracy: 1.0000\n",
      "Epoch 107/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0408 - accuracy: 0.9750 - val_loss: 0.6841 - val_accuracy: 0.7333\n",
      "Epoch 108/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.9667 - val_loss: 0.9285 - val_accuracy: 0.5333\n",
      "Epoch 109/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 0.9833 - val_loss: 0.5706 - val_accuracy: 0.7333\n",
      "Epoch 110/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.9667 - val_loss: 0.6957 - val_accuracy: 0.7333\n",
      "Epoch 111/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0290 - accuracy: 0.9833 - val_loss: 0.0542 - val_accuracy: 1.0000\n",
      "Epoch 112/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0198 - accuracy: 0.9833 - val_loss: 0.9703 - val_accuracy: 0.7333\n",
      "Epoch 113/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0479 - accuracy: 0.9750 - val_loss: 0.0450 - val_accuracy: 1.0000\n",
      "Epoch 114/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.9750 - val_loss: 0.3631 - val_accuracy: 0.7667\n",
      "Epoch 115/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0322 - accuracy: 0.9833 - val_loss: 0.3124 - val_accuracy: 0.8000\n",
      "Epoch 116/1000\n",
      "120/120 [==============================] - 0s 995us/step - loss: 0.0227 - accuracy: 0.9917 - val_loss: 0.1045 - val_accuracy: 0.9333\n",
      "Epoch 117/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0394 - accuracy: 0.9917 - val_loss: 1.6542 - val_accuracy: 0.5333\n",
      "Epoch 118/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 0.3678 - val_accuracy: 0.8667\n",
      "Epoch 119/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0324 - accuracy: 0.9833 - val_loss: 1.9124 - val_accuracy: 0.5333\n",
      "Epoch 120/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0490 - accuracy: 0.9833 - val_loss: 0.8611 - val_accuracy: 0.7333\n",
      "Epoch 121/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0422 - accuracy: 0.9917 - val_loss: 1.3084 - val_accuracy: 0.6667\n",
      "Epoch 122/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0218 - accuracy: 0.9917 - val_loss: 0.0741 - val_accuracy: 0.9667\n",
      "Epoch 123/1000\n",
      "120/120 [==============================] - 0s 990us/step - loss: 0.0845 - accuracy: 0.9667 - val_loss: 0.8821 - val_accuracy: 0.7333\n",
      "Epoch 124/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9750 - val_loss: 0.2828 - val_accuracy: 0.8000\n",
      "Epoch 125/1000\n",
      "120/120 [==============================] - 0s 998us/step - loss: 0.0493 - accuracy: 0.9833 - val_loss: 0.7038 - val_accuracy: 0.7000\n",
      "Epoch 126/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.9750 - val_loss: 0.2352 - val_accuracy: 0.8667\n",
      "Epoch 127/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0281 - accuracy: 0.9917 - val_loss: 0.5149 - val_accuracy: 0.7667\n",
      "Epoch 128/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9750 - val_loss: 0.1814 - val_accuracy: 0.9333\n",
      "Epoch 129/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0309 - accuracy: 0.9833 - val_loss: 0.2483 - val_accuracy: 0.8333\n",
      "Epoch 130/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0378 - accuracy: 0.9833 - val_loss: 0.2759 - val_accuracy: 0.8000\n",
      "Epoch 131/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0475 - accuracy: 0.9833 - val_loss: 0.4512 - val_accuracy: 0.7667\n",
      "Epoch 132/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0260 - accuracy: 0.9833 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 133/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0615 - accuracy: 0.9750 - val_loss: 0.0636 - val_accuracy: 0.9667\n",
      "Epoch 134/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.1537 - val_accuracy: 0.9333\n",
      "Epoch 135/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0389 - accuracy: 0.9750 - val_loss: 0.2825 - val_accuracy: 0.8000\n",
      "Epoch 136/1000\n",
      "120/120 [==============================] - 0s 945us/step - loss: 0.0316 - accuracy: 0.9917 - val_loss: 0.2702 - val_accuracy: 0.8000\n",
      "Epoch 137/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0474 - accuracy: 0.9667 - val_loss: 0.3585 - val_accuracy: 0.8000\n",
      "Epoch 138/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.9833 - val_loss: 0.2657 - val_accuracy: 0.8667\n",
      "Epoch 139/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0298 - accuracy: 0.9917 - val_loss: 1.2454 - val_accuracy: 0.7000\n",
      "Epoch 140/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9750 - val_loss: 0.5277 - val_accuracy: 0.7333\n",
      "Epoch 141/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0253 - accuracy: 0.9917 - val_loss: 0.6403 - val_accuracy: 0.7333\n",
      "Epoch 142/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0427 - accuracy: 0.9667 - val_loss: 0.7343 - val_accuracy: 0.7333\n",
      "Epoch 143/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0332 - accuracy: 0.9833 - val_loss: 2.6589 - val_accuracy: 0.5000\n",
      "Epoch 144/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9750 - val_loss: 0.1787 - val_accuracy: 0.9333\n",
      "Epoch 145/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0336 - accuracy: 0.9750 - val_loss: 1.2736 - val_accuracy: 0.7333\n",
      "Epoch 146/1000\n",
      "120/120 [==============================] - 0s 982us/step - loss: 0.0497 - accuracy: 0.9833 - val_loss: 0.1567 - val_accuracy: 0.9333\n",
      "Epoch 147/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0239 - accuracy: 0.9833 - val_loss: 0.4234 - val_accuracy: 0.8000\n",
      "Epoch 148/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.9917 - val_loss: 1.1230 - val_accuracy: 0.5667\n",
      "Epoch 149/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0255 - accuracy: 0.9917 - val_loss: 0.2703 - val_accuracy: 0.8667\n",
      "Epoch 150/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0546 - accuracy: 0.9917 - val_loss: 0.1098 - val_accuracy: 0.9333\n",
      "Epoch 151/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0517 - accuracy: 0.9750 - val_loss: 0.2750 - val_accuracy: 0.8667\n",
      "Epoch 152/1000\n",
      "120/120 [==============================] - 0s 991us/step - loss: 0.0338 - accuracy: 0.9917 - val_loss: 0.2012 - val_accuracy: 0.9333\n",
      "Epoch 153/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0419 - accuracy: 0.9833 - val_loss: 0.4756 - val_accuracy: 0.8000\n",
      "Epoch 154/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0332 - accuracy: 0.9833 - val_loss: 0.7441 - val_accuracy: 0.7333\n",
      "Epoch 155/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9750 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 156/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0540 - accuracy: 0.9750 - val_loss: 0.4526 - val_accuracy: 0.8000\n",
      "Epoch 157/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9583 - val_loss: 0.2455 - val_accuracy: 0.9000\n",
      "Epoch 158/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9833 - val_loss: 0.3003 - val_accuracy: 0.8667\n",
      "Epoch 159/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0310 - accuracy: 0.9917 - val_loss: 0.6569 - val_accuracy: 0.7333\n",
      "Epoch 160/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0491 - accuracy: 0.9833 - val_loss: 0.4235 - val_accuracy: 0.8000\n",
      "Epoch 161/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0255 - accuracy: 0.9833 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
      "Epoch 162/1000\n",
      "120/120 [==============================] - 0s 996us/step - loss: 0.0306 - accuracy: 0.9833 - val_loss: 1.0791 - val_accuracy: 0.7333\n",
      "Epoch 163/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0769 - accuracy: 0.9667 - val_loss: 0.2048 - val_accuracy: 0.9333\n",
      "Epoch 164/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.9917 - val_loss: 0.6125 - val_accuracy: 0.7333\n",
      "Epoch 165/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0438 - accuracy: 0.9833 - val_loss: 0.3731 - val_accuracy: 0.8000\n",
      "Epoch 166/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0410 - accuracy: 0.9750 - val_loss: 0.2293 - val_accuracy: 0.9000\n",
      "Epoch 167/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0304 - accuracy: 0.9833 - val_loss: 0.2742 - val_accuracy: 0.8667\n",
      "Epoch 168/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0403 - accuracy: 0.9833 - val_loss: 0.7279 - val_accuracy: 0.7333\n",
      "Epoch 169/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 992us/step - loss: 0.0262 - accuracy: 0.9833 - val_loss: 0.3481 - val_accuracy: 0.8333\n",
      "Epoch 170/1000\n",
      "120/120 [==============================] - 0s 993us/step - loss: 0.0311 - accuracy: 0.9833 - val_loss: 0.2390 - val_accuracy: 0.9333\n",
      "Epoch 171/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 1.9766 - val_accuracy: 0.5667\n",
      "Epoch 172/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0321 - accuracy: 0.9833 - val_loss: 0.3609 - val_accuracy: 0.8667\n",
      "Epoch 173/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0341 - accuracy: 0.9833 - val_loss: 0.3189 - val_accuracy: 0.9000\n",
      "Epoch 174/1000\n",
      "120/120 [==============================] - 0s 990us/step - loss: 0.0616 - accuracy: 0.9833 - val_loss: 0.0417 - val_accuracy: 0.9667\n",
      "Epoch 175/1000\n",
      "120/120 [==============================] - 0s 996us/step - loss: 0.0372 - accuracy: 0.9833 - val_loss: 1.1589 - val_accuracy: 0.5667\n",
      "Epoch 176/1000\n",
      "120/120 [==============================] - 0s 991us/step - loss: 0.0212 - accuracy: 0.9917 - val_loss: 0.1630 - val_accuracy: 0.9333\n",
      "Epoch 177/1000\n",
      "120/120 [==============================] - 0s 995us/step - loss: 0.0285 - accuracy: 0.9917 - val_loss: 3.0572 - val_accuracy: 0.4333\n",
      "Epoch 178/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0401 - accuracy: 0.9833 - val_loss: 1.4264 - val_accuracy: 0.5667\n",
      "Epoch 179/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0413 - accuracy: 0.9750 - val_loss: 0.3589 - val_accuracy: 0.8000\n",
      "Epoch 180/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.5148 - val_accuracy: 0.8000\n",
      "Epoch 181/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0248 - accuracy: 0.9917 - val_loss: 0.8838 - val_accuracy: 0.7333\n",
      "Epoch 182/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0395 - accuracy: 0.9750 - val_loss: 0.7688 - val_accuracy: 0.7333\n",
      "Epoch 183/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0307 - accuracy: 0.9833 - val_loss: 0.1181 - val_accuracy: 0.9333\n",
      "Epoch 184/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0453 - accuracy: 0.9833 - val_loss: 0.4620 - val_accuracy: 0.8000\n",
      "Epoch 185/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0397 - accuracy: 0.9750 - val_loss: 0.3779 - val_accuracy: 0.8000\n",
      "Epoch 186/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0251 - accuracy: 0.9833 - val_loss: 0.6343 - val_accuracy: 0.8000\n",
      "Epoch 187/1000\n",
      "120/120 [==============================] - 0s 930us/step - loss: 0.0483 - accuracy: 0.9667 - val_loss: 0.2572 - val_accuracy: 0.9000\n",
      "Epoch 188/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0299 - accuracy: 0.9833 - val_loss: 0.1075 - val_accuracy: 0.9333\n",
      "Epoch 189/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0333 - accuracy: 0.9917 - val_loss: 3.4922 - val_accuracy: 0.4667\n",
      "Epoch 190/1000\n",
      "120/120 [==============================] - 0s 949us/step - loss: 0.1000 - accuracy: 0.9833 - val_loss: 0.5610 - val_accuracy: 0.7333\n",
      "Epoch 191/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0287 - accuracy: 0.9833 - val_loss: 0.3965 - val_accuracy: 0.8000\n",
      "Epoch 192/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0318 - accuracy: 0.9833 - val_loss: 0.3170 - val_accuracy: 0.8000\n",
      "Epoch 193/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0793 - accuracy: 0.9750 - val_loss: 0.2635 - val_accuracy: 0.8667\n",
      "Epoch 194/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0389 - accuracy: 0.9917 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
      "Epoch 195/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3156 - accuracy: 0.9667 - val_loss: 0.2418 - val_accuracy: 0.9333\n",
      "Epoch 196/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0365 - accuracy: 0.9833 - val_loss: 0.8804 - val_accuracy: 0.5667\n",
      "Epoch 197/1000\n",
      "120/120 [==============================] - 0s 975us/step - loss: 0.0315 - accuracy: 0.9917 - val_loss: 0.4805 - val_accuracy: 0.7333\n",
      "Epoch 198/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0279 - accuracy: 0.9833 - val_loss: 0.0823 - val_accuracy: 0.9667\n",
      "Epoch 199/1000\n",
      "120/120 [==============================] - 0s 951us/step - loss: 0.0336 - accuracy: 0.9833 - val_loss: 0.1524 - val_accuracy: 0.9333\n",
      "Epoch 200/1000\n",
      "120/120 [==============================] - 0s 975us/step - loss: 0.0324 - accuracy: 0.9917 - val_loss: 0.0329 - val_accuracy: 1.0000\n",
      "Epoch 201/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0263 - accuracy: 0.9833 - val_loss: 1.0070 - val_accuracy: 0.6333\n",
      "Epoch 202/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0460 - accuracy: 0.9667 - val_loss: 0.5537 - val_accuracy: 0.7333\n",
      "Epoch 203/1000\n",
      "120/120 [==============================] - 0s 977us/step - loss: 0.0562 - accuracy: 0.9750 - val_loss: 0.7724 - val_accuracy: 0.7000\n",
      "Epoch 204/1000\n",
      "120/120 [==============================] - 0s 944us/step - loss: 0.0235 - accuracy: 0.9917 - val_loss: 0.2388 - val_accuracy: 0.9333\n",
      "Epoch 205/1000\n",
      "120/120 [==============================] - 0s 952us/step - loss: 0.0232 - accuracy: 0.9917 - val_loss: 0.4143 - val_accuracy: 0.8000\n",
      "Epoch 206/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.0919 - accuracy: 0.9583 - val_loss: 0.3196 - val_accuracy: 0.8000\n",
      "Epoch 207/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0199 - accuracy: 0.9833 - val_loss: 0.7948 - val_accuracy: 0.7000\n",
      "Epoch 208/1000\n",
      "120/120 [==============================] - 0s 967us/step - loss: 0.0318 - accuracy: 0.9750 - val_loss: 0.7293 - val_accuracy: 0.7333\n",
      "Epoch 209/1000\n",
      "120/120 [==============================] - 0s 992us/step - loss: 0.0455 - accuracy: 0.9833 - val_loss: 0.0329 - val_accuracy: 1.0000\n",
      "Epoch 210/1000\n",
      "120/120 [==============================] - 0s 967us/step - loss: 0.0624 - accuracy: 0.9583 - val_loss: 0.0813 - val_accuracy: 0.9667\n",
      "Epoch 211/1000\n",
      "120/120 [==============================] - 0s 959us/step - loss: 0.0517 - accuracy: 0.9667 - val_loss: 0.5115 - val_accuracy: 0.8000\n",
      "Epoch 212/1000\n",
      "120/120 [==============================] - 0s 944us/step - loss: 0.0254 - accuracy: 0.9917 - val_loss: 0.3191 - val_accuracy: 0.9333\n",
      "Epoch 213/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0263 - accuracy: 0.9917 - val_loss: 0.6081 - val_accuracy: 0.8000\n",
      "Epoch 214/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0213 - accuracy: 0.9833 - val_loss: 2.3359 - val_accuracy: 0.5667\n",
      "Epoch 215/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.1044 - accuracy: 0.9750 - val_loss: 0.4183 - val_accuracy: 0.8000\n",
      "Epoch 216/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0287 - accuracy: 0.9917 - val_loss: 0.4512 - val_accuracy: 0.8000\n",
      "Epoch 217/1000\n",
      "120/120 [==============================] - 0s 960us/step - loss: 0.0261 - accuracy: 0.9917 - val_loss: 0.5383 - val_accuracy: 0.7667\n",
      "Epoch 218/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0225 - accuracy: 0.9917 - val_loss: 0.8937 - val_accuracy: 0.7000\n",
      "Epoch 219/1000\n",
      "120/120 [==============================] - 0s 936us/step - loss: 0.0434 - accuracy: 0.9917 - val_loss: 0.5557 - val_accuracy: 0.8000\n",
      "Epoch 220/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0264 - accuracy: 0.9833 - val_loss: 0.2024 - val_accuracy: 0.9333\n",
      "Epoch 221/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.1054 - accuracy: 0.9500 - val_loss: 0.1529 - val_accuracy: 0.9333\n",
      "Epoch 222/1000\n",
      "120/120 [==============================] - 0s 949us/step - loss: 0.0309 - accuracy: 0.9833 - val_loss: 0.4866 - val_accuracy: 0.7333\n",
      "Epoch 223/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.1783 - val_accuracy: 0.9333\n",
      "Epoch 224/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 955us/step - loss: 0.0249 - accuracy: 0.9833 - val_loss: 0.5990 - val_accuracy: 0.7333\n",
      "Epoch 225/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0233 - accuracy: 0.9917 - val_loss: 1.2494 - val_accuracy: 0.6333\n",
      "Epoch 226/1000\n",
      "120/120 [==============================] - 0s 950us/step - loss: 0.0640 - accuracy: 0.9833 - val_loss: 0.4155 - val_accuracy: 0.7667\n",
      "Epoch 227/1000\n",
      "120/120 [==============================] - 0s 957us/step - loss: 0.0217 - accuracy: 0.9917 - val_loss: 0.0846 - val_accuracy: 0.9667\n",
      "Epoch 228/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0530 - accuracy: 0.9750 - val_loss: 0.2588 - val_accuracy: 0.9000\n",
      "Epoch 229/1000\n",
      "120/120 [==============================] - 0s 944us/step - loss: 0.0299 - accuracy: 0.9833 - val_loss: 1.4282 - val_accuracy: 0.5667\n",
      "Epoch 230/1000\n",
      "120/120 [==============================] - 0s 965us/step - loss: 0.0243 - accuracy: 0.9917 - val_loss: 0.4403 - val_accuracy: 0.8000\n",
      "Epoch 231/1000\n",
      "120/120 [==============================] - 0s 971us/step - loss: 0.0204 - accuracy: 0.9833 - val_loss: 0.2974 - val_accuracy: 0.9000\n",
      "Epoch 232/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0425 - accuracy: 0.9833 - val_loss: 0.3749 - val_accuracy: 0.8000\n",
      "Epoch 233/1000\n",
      "120/120 [==============================] - 0s 943us/step - loss: 0.0388 - accuracy: 0.9667 - val_loss: 0.2220 - val_accuracy: 0.9333\n",
      "Epoch 234/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.0514 - accuracy: 0.9833 - val_loss: 0.3566 - val_accuracy: 0.8000\n",
      "Epoch 235/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0386 - accuracy: 0.9667 - val_loss: 0.3752 - val_accuracy: 0.8000\n",
      "Epoch 236/1000\n",
      "120/120 [==============================] - 0s 974us/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 1.9564 - val_accuracy: 0.5667\n",
      "Epoch 237/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.1003 - accuracy: 0.9750 - val_loss: 0.1055 - val_accuracy: 0.9667\n",
      "Epoch 238/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0267 - accuracy: 0.9917 - val_loss: 0.5843 - val_accuracy: 0.7667\n",
      "Epoch 239/1000\n",
      "120/120 [==============================] - 0s 948us/step - loss: 0.0261 - accuracy: 0.9833 - val_loss: 0.2623 - val_accuracy: 0.9333\n",
      "Epoch 240/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0421 - accuracy: 0.9833 - val_loss: 0.2811 - val_accuracy: 0.8667\n",
      "Epoch 241/1000\n",
      "120/120 [==============================] - 0s 946us/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.9221 - val_accuracy: 0.6667\n",
      "Epoch 242/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0340 - accuracy: 0.9833 - val_loss: 2.0939 - val_accuracy: 0.5333\n",
      "Epoch 243/1000\n",
      "120/120 [==============================] - 0s 957us/step - loss: 0.0473 - accuracy: 0.9833 - val_loss: 0.5796 - val_accuracy: 0.8000\n",
      "Epoch 244/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0332 - accuracy: 0.9833 - val_loss: 0.7101 - val_accuracy: 0.7333\n",
      "Epoch 245/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0233 - accuracy: 0.9917 - val_loss: 0.5220 - val_accuracy: 0.8000\n",
      "Epoch 246/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0239 - accuracy: 0.9917 - val_loss: 1.3077 - val_accuracy: 0.6667\n",
      "Epoch 247/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.7732 - val_accuracy: 0.8000\n",
      "Epoch 248/1000\n",
      "120/120 [==============================] - 0s 966us/step - loss: 0.0985 - accuracy: 0.9917 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
      "Epoch 249/1000\n",
      "120/120 [==============================] - 0s 968us/step - loss: 0.0276 - accuracy: 0.9917 - val_loss: 0.3340 - val_accuracy: 0.8000\n",
      "Epoch 250/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0532 - accuracy: 0.9833 - val_loss: 0.6047 - val_accuracy: 0.7333\n",
      "Epoch 251/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0307 - accuracy: 0.9917 - val_loss: 0.6442 - val_accuracy: 0.7333\n",
      "Epoch 252/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0281 - accuracy: 0.9833 - val_loss: 0.4640 - val_accuracy: 0.8000\n",
      "Epoch 253/1000\n",
      "120/120 [==============================] - 0s 962us/step - loss: 0.0591 - accuracy: 0.9667 - val_loss: 0.0958 - val_accuracy: 0.9667\n",
      "Epoch 254/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0282 - accuracy: 0.9750 - val_loss: 0.3738 - val_accuracy: 0.8000\n",
      "Epoch 255/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0219 - accuracy: 0.9917 - val_loss: 1.2911 - val_accuracy: 0.6000\n",
      "Epoch 256/1000\n",
      "120/120 [==============================] - 0s 963us/step - loss: 0.0334 - accuracy: 0.9750 - val_loss: 0.1211 - val_accuracy: 0.9333\n",
      "Epoch 257/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0428 - accuracy: 0.9833 - val_loss: 0.3012 - val_accuracy: 0.8000\n",
      "Epoch 258/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0243 - accuracy: 0.9917 - val_loss: 0.7509 - val_accuracy: 0.7000\n",
      "Epoch 259/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0444 - accuracy: 0.9833 - val_loss: 1.3234 - val_accuracy: 0.6000\n",
      "Epoch 260/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0351 - accuracy: 0.9750 - val_loss: 0.1795 - val_accuracy: 0.9333\n",
      "Epoch 261/1000\n",
      "120/120 [==============================] - 0s 959us/step - loss: 0.0306 - accuracy: 0.9833 - val_loss: 0.1894 - val_accuracy: 0.9333\n",
      "Epoch 262/1000\n",
      "120/120 [==============================] - 0s 957us/step - loss: 0.0267 - accuracy: 0.9917 - val_loss: 0.4397 - val_accuracy: 0.8000\n",
      "Epoch 263/1000\n",
      "120/120 [==============================] - 0s 949us/step - loss: 0.0586 - accuracy: 0.9917 - val_loss: 1.2501 - val_accuracy: 0.7000\n",
      "Epoch 264/1000\n",
      "120/120 [==============================] - 0s 951us/step - loss: 0.1790 - accuracy: 0.9500 - val_loss: 0.2768 - val_accuracy: 0.9333\n",
      "Epoch 265/1000\n",
      "120/120 [==============================] - 0s 950us/step - loss: 0.0279 - accuracy: 0.9917 - val_loss: 0.2022 - val_accuracy: 0.9333\n",
      "Epoch 266/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0324 - accuracy: 0.9833 - val_loss: 0.1746 - val_accuracy: 0.9333\n",
      "Epoch 267/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0306 - accuracy: 0.9833 - val_loss: 1.5289 - val_accuracy: 0.6667\n",
      "Epoch 268/1000\n",
      "120/120 [==============================] - 0s 950us/step - loss: 0.0549 - accuracy: 0.9667 - val_loss: 0.2894 - val_accuracy: 0.9333\n",
      "Epoch 269/1000\n",
      "120/120 [==============================] - 0s 951us/step - loss: 0.0288 - accuracy: 0.9917 - val_loss: 0.2999 - val_accuracy: 0.9333\n",
      "Epoch 270/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0247 - accuracy: 0.9917 - val_loss: 0.6229 - val_accuracy: 0.8000\n",
      "Epoch 271/1000\n",
      "120/120 [==============================] - 0s 946us/step - loss: 0.0368 - accuracy: 0.9833 - val_loss: 0.3129 - val_accuracy: 0.9333\n",
      "Epoch 272/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0219 - accuracy: 0.9833 - val_loss: 0.1096 - val_accuracy: 0.9333\n",
      "Epoch 273/1000\n",
      "120/120 [==============================] - 0s 942us/step - loss: 0.0377 - accuracy: 0.9833 - val_loss: 0.4778 - val_accuracy: 0.8333\n",
      "Epoch 274/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0292 - accuracy: 0.9833 - val_loss: 0.2207 - val_accuracy: 0.9333\n",
      "Epoch 275/1000\n",
      "120/120 [==============================] - 0s 967us/step - loss: 0.0319 - accuracy: 0.9750 - val_loss: 1.0937 - val_accuracy: 0.7333\n",
      "Epoch 276/1000\n",
      "120/120 [==============================] - 0s 962us/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.4826 - val_accuracy: 0.8667\n",
      "Epoch 277/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0491 - accuracy: 0.9917 - val_loss: 0.7229 - val_accuracy: 0.8000\n",
      "Epoch 278/1000\n",
      "120/120 [==============================] - 0s 980us/step - loss: 0.1353 - accuracy: 0.9667 - val_loss: 0.0535 - val_accuracy: 1.0000\n",
      "Epoch 279/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 956us/step - loss: 0.0285 - accuracy: 0.9917 - val_loss: 0.2913 - val_accuracy: 0.9333\n",
      "Epoch 280/1000\n",
      "120/120 [==============================] - 0s 968us/step - loss: 0.0243 - accuracy: 0.9917 - val_loss: 0.7395 - val_accuracy: 0.7333\n",
      "Epoch 281/1000\n",
      "120/120 [==============================] - 0s 959us/step - loss: 0.0441 - accuracy: 0.9750 - val_loss: 0.2221 - val_accuracy: 0.9333\n",
      "Epoch 282/1000\n",
      "120/120 [==============================] - 0s 976us/step - loss: 0.0263 - accuracy: 0.9917 - val_loss: 0.6243 - val_accuracy: 0.8000\n",
      "Epoch 283/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0244 - accuracy: 0.9833 - val_loss: 0.2388 - val_accuracy: 0.9333\n",
      "Epoch 284/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0270 - accuracy: 0.9833 - val_loss: 0.1737 - val_accuracy: 0.9333\n",
      "Epoch 285/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0270 - accuracy: 0.9833 - val_loss: 0.3128 - val_accuracy: 0.9333\n",
      "Epoch 286/1000\n",
      "120/120 [==============================] - 0s 972us/step - loss: 0.0382 - accuracy: 0.9833 - val_loss: 0.1571 - val_accuracy: 0.9333\n",
      "Epoch 287/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0223 - accuracy: 0.9833 - val_loss: 0.6184 - val_accuracy: 0.7333\n",
      "Epoch 288/1000\n",
      "120/120 [==============================] - 0s 939us/step - loss: 0.0244 - accuracy: 0.9917 - val_loss: 0.3984 - val_accuracy: 0.8000\n",
      "Epoch 289/1000\n",
      "120/120 [==============================] - 0s 922us/step - loss: 0.0266 - accuracy: 0.9917 - val_loss: 0.4911 - val_accuracy: 0.8000\n",
      "Epoch 290/1000\n",
      "120/120 [==============================] - 0s 930us/step - loss: 0.0199 - accuracy: 0.9833 - val_loss: 0.2907 - val_accuracy: 0.9333\n",
      "Epoch 291/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0268 - accuracy: 0.9917 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 292/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.9750 - val_loss: 0.3762 - val_accuracy: 0.8000\n",
      "Epoch 293/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 0.9917 - val_loss: 0.3811 - val_accuracy: 0.8667\n",
      "Epoch 294/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 0.9917 - val_loss: 0.3571 - val_accuracy: 0.8667\n",
      "Epoch 295/1000\n",
      "120/120 [==============================] - 0s 947us/step - loss: 0.0164 - accuracy: 0.9917 - val_loss: 1.5386 - val_accuracy: 0.6333\n",
      "Epoch 296/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0308 - accuracy: 0.9917 - val_loss: 0.1776 - val_accuracy: 0.9333\n",
      "Epoch 297/1000\n",
      "120/120 [==============================] - 0s 940us/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.9203 - val_accuracy: 0.7333\n",
      "Epoch 298/1000\n",
      "120/120 [==============================] - 0s 941us/step - loss: 0.0878 - accuracy: 0.9750 - val_loss: 0.0838 - val_accuracy: 0.9333\n",
      "Epoch 299/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0443 - accuracy: 0.9833 - val_loss: 0.1955 - val_accuracy: 0.9333\n",
      "Epoch 300/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0239 - accuracy: 0.9917 - val_loss: 0.2137 - val_accuracy: 0.9333\n",
      "Epoch 301/1000\n",
      "120/120 [==============================] - 0s 952us/step - loss: 0.0172 - accuracy: 0.9917 - val_loss: 0.8744 - val_accuracy: 0.7333\n",
      "Epoch 302/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0778 - accuracy: 0.9750 - val_loss: 0.5745 - val_accuracy: 0.7333\n",
      "Epoch 303/1000\n",
      "120/120 [==============================] - 0s 979us/step - loss: 0.0233 - accuracy: 0.9833 - val_loss: 0.1791 - val_accuracy: 0.9333\n",
      "Epoch 304/1000\n",
      "120/120 [==============================] - 0s 956us/step - loss: 0.0261 - accuracy: 0.9917 - val_loss: 1.4506 - val_accuracy: 0.5667\n",
      "Epoch 305/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0418 - accuracy: 0.9750 - val_loss: 0.0900 - val_accuracy: 0.9667\n",
      "Epoch 306/1000\n",
      "120/120 [==============================] - 0s 949us/step - loss: 0.0352 - accuracy: 0.9833 - val_loss: 0.5373 - val_accuracy: 0.8000\n",
      "Epoch 307/1000\n",
      "120/120 [==============================] - 0s 951us/step - loss: 0.0384 - accuracy: 0.9833 - val_loss: 0.3081 - val_accuracy: 0.8667\n",
      "Epoch 308/1000\n",
      "120/120 [==============================] - 0s 957us/step - loss: 0.0239 - accuracy: 0.9833 - val_loss: 0.1563 - val_accuracy: 0.9333\n",
      "Epoch 309/1000\n",
      "120/120 [==============================] - 0s 958us/step - loss: 0.0227 - accuracy: 0.9833 - val_loss: 0.9685 - val_accuracy: 0.6667\n",
      "Epoch 310/1000\n",
      "120/120 [==============================] - 0s 961us/step - loss: 0.0358 - accuracy: 0.9917 - val_loss: 0.7059 - val_accuracy: 0.7000\n",
      "Epoch 311/1000\n",
      "120/120 [==============================] - 0s 962us/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 0.2814 - val_accuracy: 0.9333\n",
      "Epoch 312/1000\n",
      "120/120 [==============================] - 0s 950us/step - loss: 0.0703 - accuracy: 0.9833 - val_loss: 0.6728 - val_accuracy: 0.6667\n",
      "Epoch 313/1000\n",
      "120/120 [==============================] - 0s 952us/step - loss: 0.0346 - accuracy: 0.9833 - val_loss: 0.2325 - val_accuracy: 0.9333\n",
      "Epoch 314/1000\n",
      "120/120 [==============================] - 0s 997us/step - loss: 0.0199 - accuracy: 0.9917 - val_loss: 0.3940 - val_accuracy: 0.8000\n",
      "Epoch 315/1000\n",
      "120/120 [==============================] - 0s 949us/step - loss: 0.0302 - accuracy: 0.9917 - val_loss: 1.4475 - val_accuracy: 0.6333\n",
      "Epoch 316/1000\n",
      "120/120 [==============================] - 0s 954us/step - loss: 0.0396 - accuracy: 0.9917 - val_loss: 0.2755 - val_accuracy: 0.9333\n",
      "Epoch 317/1000\n",
      "120/120 [==============================] - 0s 941us/step - loss: 0.0215 - accuracy: 0.9917 - val_loss: 1.2056 - val_accuracy: 0.6667\n",
      "Epoch 318/1000\n",
      "120/120 [==============================] - 0s 958us/step - loss: 0.0299 - accuracy: 0.9917 - val_loss: 0.0855 - val_accuracy: 0.9667\n",
      "Epoch 319/1000\n",
      "120/120 [==============================] - 0s 944us/step - loss: 0.0739 - accuracy: 0.9667 - val_loss: 0.1605 - val_accuracy: 0.9333\n",
      "Epoch 320/1000\n",
      "120/120 [==============================] - 0s 975us/step - loss: 0.0192 - accuracy: 0.9917 - val_loss: 0.4661 - val_accuracy: 0.8333\n",
      "Epoch 321/1000\n",
      "120/120 [==============================] - 0s 946us/step - loss: 0.0635 - accuracy: 0.9833 - val_loss: 0.3883 - val_accuracy: 0.8333\n",
      "Epoch 322/1000\n",
      "120/120 [==============================] - 0s 946us/step - loss: 0.0381 - accuracy: 0.9833 - val_loss: 0.0985 - val_accuracy: 0.9667\n",
      "Epoch 323/1000\n",
      "120/120 [==============================] - 0s 942us/step - loss: 0.0387 - accuracy: 0.9750 - val_loss: 0.1691 - val_accuracy: 0.9333\n",
      "Epoch 324/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0296 - accuracy: 0.9833 - val_loss: 0.9823 - val_accuracy: 0.6333\n",
      "Epoch 325/1000\n",
      "120/120 [==============================] - 0s 953us/step - loss: 0.0261 - accuracy: 0.9833 - val_loss: 0.7812 - val_accuracy: 0.7000\n",
      "Epoch 326/1000\n",
      "120/120 [==============================] - 0s 941us/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.4517 - val_accuracy: 0.8333\n",
      "Epoch 327/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 0.9833 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
      "Epoch 328/1000\n",
      "120/120 [==============================] - 0s 981us/step - loss: 0.0584 - accuracy: 0.9667 - val_loss: 0.2157 - val_accuracy: 0.9333\n",
      "Epoch 329/1000\n",
      "120/120 [==============================] - 0s 922us/step - loss: 0.0220 - accuracy: 0.9917 - val_loss: 0.7690 - val_accuracy: 0.7000\n",
      "Epoch 330/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0243 - accuracy: 0.9833 - val_loss: 0.3208 - val_accuracy: 0.9333\n",
      "Epoch 331/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0276 - accuracy: 0.9917 - val_loss: 0.6331 - val_accuracy: 0.8000\n",
      "Epoch 332/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0262 - accuracy: 0.9917 - val_loss: 0.1575 - val_accuracy: 0.9333\n",
      "Epoch 333/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0396 - accuracy: 0.9750 - val_loss: 0.2516 - val_accuracy: 0.9333\n",
      "Epoch 334/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 994us/step - loss: 0.0228 - accuracy: 0.9917 - val_loss: 0.5305 - val_accuracy: 0.8000\n",
      "Epoch 335/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0182 - accuracy: 0.9917 - val_loss: 0.3656 - val_accuracy: 0.9333\n",
      "Epoch 336/1000\n",
      "120/120 [==============================] - 0s 955us/step - loss: 0.0433 - accuracy: 0.9750 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 337/1000\n",
      "120/120 [==============================] - 0s 980us/step - loss: 0.0862 - accuracy: 0.9833 - val_loss: 0.2892 - val_accuracy: 0.9333\n",
      "Epoch 338/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.9917 - val_loss: 0.5093 - val_accuracy: 0.8000\n",
      "Epoch 339/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.9917 - val_loss: 0.6609 - val_accuracy: 0.8000\n",
      "Epoch 340/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 0.1595 - val_accuracy: 0.9333\n",
      "Epoch 341/1000\n",
      "120/120 [==============================] - 0s 983us/step - loss: 0.0244 - accuracy: 0.9833 - val_loss: 0.2430 - val_accuracy: 0.9333\n",
      "Epoch 342/1000\n",
      "120/120 [==============================] - 0s 980us/step - loss: 0.0242 - accuracy: 0.9917 - val_loss: 0.2983 - val_accuracy: 0.9333\n",
      "Epoch 343/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0546 - accuracy: 0.9917 - val_loss: 0.2941 - val_accuracy: 0.8667\n",
      "Epoch 344/1000\n",
      "120/120 [==============================] - 0s 969us/step - loss: 0.0304 - accuracy: 0.9833 - val_loss: 0.3672 - val_accuracy: 0.8333\n",
      "Epoch 345/1000\n",
      "120/120 [==============================] - 0s 974us/step - loss: 0.0394 - accuracy: 0.9833 - val_loss: 0.5754 - val_accuracy: 0.7000\n",
      "Epoch 346/1000\n",
      "120/120 [==============================] - 0s 968us/step - loss: 0.0301 - accuracy: 0.9833 - val_loss: 0.2296 - val_accuracy: 0.9333\n",
      "Epoch 347/1000\n",
      "120/120 [==============================] - 0s 978us/step - loss: 0.0452 - accuracy: 0.9833 - val_loss: 2.0453 - val_accuracy: 0.4333\n",
      "Epoch 348/1000\n",
      "120/120 [==============================] - 0s 970us/step - loss: 0.0622 - accuracy: 0.9750 - val_loss: 0.2644 - val_accuracy: 0.8333\n",
      "Epoch 349/1000\n",
      "120/120 [==============================] - 0s 973us/step - loss: 0.0277 - accuracy: 0.9917 - val_loss: 0.4150 - val_accuracy: 0.8000\n",
      "Epoch 350/1000\n",
      "120/120 [==============================] - 0s 960us/step - loss: 0.0268 - accuracy: 0.9833 - val_loss: 0.3824 - val_accuracy: 0.8333\n",
      "Epoch 351/1000\n",
      "120/120 [==============================] - 0s 973us/step - loss: 0.0232 - accuracy: 0.9917 - val_loss: 0.6085 - val_accuracy: 0.8000\n",
      "Epoch 352/1000\n",
      "120/120 [==============================] - 0s 978us/step - loss: 0.0214 - accuracy: 0.9917 - val_loss: 0.2949 - val_accuracy: 0.9333\n",
      "Epoch 353/1000\n",
      "120/120 [==============================] - 0s 995us/step - loss: 0.0448 - accuracy: 0.9750 - val_loss: 0.0578 - val_accuracy: 0.9667\n",
      "Epoch 354/1000\n",
      "120/120 [==============================] - 0s 995us/step - loss: 0.0296 - accuracy: 0.9833 - val_loss: 0.4485 - val_accuracy: 0.8333\n",
      "Epoch 355/1000\n",
      "120/120 [==============================] - 0s 993us/step - loss: 0.0285 - accuracy: 0.9833 - val_loss: 0.1222 - val_accuracy: 0.9333\n",
      "Epoch 356/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0575 - accuracy: 0.9917 - val_loss: 0.6387 - val_accuracy: 0.8000\n",
      "Epoch 357/1000\n",
      "120/120 [==============================] - 0s 964us/step - loss: 0.0229 - accuracy: 0.9917 - val_loss: 0.4909 - val_accuracy: 0.8000\n",
      "Epoch 358/1000\n",
      "120/120 [==============================] - 0s 980us/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 0.2889 - val_accuracy: 0.9333\n",
      "Epoch 359/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0373 - accuracy: 0.9833 - val_loss: 0.3087 - val_accuracy: 0.9333\n",
      "Epoch 360/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0260 - accuracy: 0.9917 - val_loss: 0.4856 - val_accuracy: 0.8333\n",
      "Epoch 361/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 0.9917 - val_loss: 0.4914 - val_accuracy: 0.8333\n",
      "Epoch 362/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0225 - accuracy: 0.9917 - val_loss: 0.3350 - val_accuracy: 0.9333\n",
      "Epoch 363/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0482 - accuracy: 0.9750 - val_loss: 0.3363 - val_accuracy: 0.8667\n",
      "Epoch 364/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0209 - accuracy: 0.9917 - val_loss: 0.2986 - val_accuracy: 0.9333\n",
      "Epoch 365/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0313 - accuracy: 0.9833 - val_loss: 0.1556 - val_accuracy: 0.9333\n",
      "Epoch 366/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0274 - accuracy: 0.9833 - val_loss: 0.2542 - val_accuracy: 0.9333\n",
      "Epoch 367/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0254 - accuracy: 0.9833 - val_loss: 0.6302 - val_accuracy: 0.8000\n",
      "Epoch 368/1000\n",
      "120/120 [==============================] - 0s 986us/step - loss: 0.0227 - accuracy: 0.9917 - val_loss: 0.4493 - val_accuracy: 0.8333\n",
      "Epoch 369/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9833 - val_loss: 0.4462 - val_accuracy: 0.8667\n",
      "Epoch 370/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0185 - accuracy: 0.9917 - val_loss: 0.3913 - val_accuracy: 0.9333\n",
      "Epoch 371/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
      "Epoch 372/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0259 - accuracy: 0.9750 - val_loss: 0.5851 - val_accuracy: 0.7000\n",
      "Epoch 373/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0226 - accuracy: 0.9917 - val_loss: 0.4177 - val_accuracy: 0.8000\n",
      "Epoch 374/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0393 - accuracy: 0.9833 - val_loss: 0.4336 - val_accuracy: 0.8000\n",
      "Epoch 375/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9667 - val_loss: 0.1435 - val_accuracy: 0.9333\n",
      "Epoch 376/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0292 - accuracy: 0.9917 - val_loss: 0.1776 - val_accuracy: 0.9333\n",
      "Epoch 377/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0267 - accuracy: 0.9917 - val_loss: 0.8164 - val_accuracy: 0.7000\n",
      "Epoch 378/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0275 - accuracy: 0.9917 - val_loss: 0.5837 - val_accuracy: 0.7333\n",
      "Epoch 379/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0267 - accuracy: 0.9833 - val_loss: 0.5016 - val_accuracy: 0.8000\n",
      "Epoch 380/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0195 - accuracy: 0.9917 - val_loss: 0.1806 - val_accuracy: 0.9333\n",
      "Epoch 381/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0351 - accuracy: 0.9833 - val_loss: 0.1147 - val_accuracy: 0.9333\n",
      "Epoch 382/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 1.4008 - val_accuracy: 0.6333\n",
      "Epoch 383/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0330 - accuracy: 0.9667 - val_loss: 0.8694 - val_accuracy: 0.6333\n",
      "Epoch 384/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0468 - accuracy: 0.9750 - val_loss: 1.0448 - val_accuracy: 0.6333\n",
      "Epoch 385/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9750 - val_loss: 0.8506 - val_accuracy: 0.6000\n",
      "Epoch 386/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0241 - accuracy: 0.9917 - val_loss: 0.2923 - val_accuracy: 0.9000\n",
      "Epoch 387/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0238 - accuracy: 0.9917 - val_loss: 0.2920 - val_accuracy: 0.9333\n",
      "Epoch 388/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0198 - accuracy: 0.9917 - val_loss: 0.2748 - val_accuracy: 0.9333\n",
      "Epoch 389/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0253 - accuracy: 0.9833 - val_loss: 0.1764 - val_accuracy: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 390/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0139 - accuracy: 0.9917 - val_loss: 1.2603 - val_accuracy: 0.6667\n",
      "Epoch 391/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0471 - accuracy: 0.9833 - val_loss: 0.9731 - val_accuracy: 0.6333\n",
      "Epoch 392/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0306 - accuracy: 0.9833 - val_loss: 0.5266 - val_accuracy: 0.8000\n",
      "Epoch 393/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0256 - accuracy: 0.9833 - val_loss: 0.5321 - val_accuracy: 0.8000\n",
      "Epoch 394/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0219 - accuracy: 0.9917 - val_loss: 0.1101 - val_accuracy: 0.9667\n",
      "Epoch 395/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0307 - accuracy: 0.9917 - val_loss: 0.6998 - val_accuracy: 0.8333\n",
      "Epoch 396/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0220 - accuracy: 0.9917 - val_loss: 1.3154 - val_accuracy: 0.7000\n",
      "Epoch 397/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0273 - accuracy: 0.9833 - val_loss: 0.4838 - val_accuracy: 0.8667\n",
      "Epoch 398/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 0.9917 - val_loss: 0.5982 - val_accuracy: 0.8333\n",
      "Epoch 399/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.9833 - val_loss: 1.1683 - val_accuracy: 0.7333\n",
      "Epoch 400/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0551 - accuracy: 0.9750 - val_loss: 0.7886 - val_accuracy: 0.7000\n",
      "Epoch 401/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0230 - accuracy: 0.9917 - val_loss: 0.6136 - val_accuracy: 0.8000\n",
      "Epoch 402/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.9917 - val_loss: 1.3203 - val_accuracy: 0.6333\n",
      "Epoch 403/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0371 - accuracy: 0.9833 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
      "Epoch 404/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.9833 - val_loss: 0.7131 - val_accuracy: 0.7333\n",
      "Epoch 405/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0233 - accuracy: 0.9833 - val_loss: 0.5343 - val_accuracy: 0.8000\n",
      "Epoch 406/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0391 - accuracy: 0.9750 - val_loss: 1.1066 - val_accuracy: 0.7000\n",
      "Epoch 407/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.6642 - val_accuracy: 0.8333\n",
      "Epoch 408/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0339 - accuracy: 0.9917 - val_loss: 6.8801 - val_accuracy: 0.4000\n",
      "Epoch 409/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0409 - accuracy: 0.9667 - val_loss: 0.1280 - val_accuracy: 0.9667\n",
      "Epoch 410/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0207 - accuracy: 0.9917 - val_loss: 0.1791 - val_accuracy: 0.9333\n",
      "Epoch 411/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0362 - accuracy: 0.9833 - val_loss: 0.3772 - val_accuracy: 0.9000\n",
      "Epoch 412/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0205 - accuracy: 0.9917 - val_loss: 0.3320 - val_accuracy: 0.9333\n",
      "Epoch 413/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 0.9917 - val_loss: 0.4591 - val_accuracy: 0.9000\n",
      "Epoch 414/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.9917 - val_loss: 0.7850 - val_accuracy: 0.8000\n",
      "Epoch 415/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 0.4127 - val_accuracy: 0.9333\n",
      "Epoch 416/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.9917 - val_loss: 0.5059 - val_accuracy: 0.9333\n",
      "Epoch 417/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0225 - accuracy: 0.9833 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
      "Epoch 418/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9917 - val_loss: 0.5265 - val_accuracy: 0.8667\n",
      "Epoch 419/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9750 - val_loss: 0.1679 - val_accuracy: 0.9667\n",
      "Epoch 420/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0329 - accuracy: 0.9833 - val_loss: 0.4638 - val_accuracy: 0.7667\n",
      "Epoch 421/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0412 - accuracy: 0.9833 - val_loss: 0.3615 - val_accuracy: 0.8333\n",
      "Epoch 422/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0197 - accuracy: 0.9917 - val_loss: 0.6897 - val_accuracy: 0.7333\n",
      "Epoch 423/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0290 - accuracy: 0.9917 - val_loss: 0.6770 - val_accuracy: 0.7667\n",
      "Epoch 424/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9833 - val_loss: 0.2316 - val_accuracy: 0.9333\n",
      "Epoch 425/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0243 - accuracy: 0.9917 - val_loss: 0.3841 - val_accuracy: 0.8333\n",
      "Epoch 426/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 0.9917 - val_loss: 0.4088 - val_accuracy: 0.8333\n",
      "Epoch 427/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 0.9917 - val_loss: 1.0079 - val_accuracy: 0.7000\n",
      "Epoch 428/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0198 - accuracy: 0.9833 - val_loss: 2.3314 - val_accuracy: 0.6333\n",
      "Epoch 429/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0389 - accuracy: 0.9750 - val_loss: 0.6227 - val_accuracy: 0.8000\n",
      "Epoch 430/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0381 - accuracy: 0.9833 - val_loss: 0.2523 - val_accuracy: 0.9333\n",
      "Epoch 431/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 0.9750 - val_loss: 0.9340 - val_accuracy: 0.6333\n",
      "Epoch 432/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0376 - accuracy: 0.9750 - val_loss: 0.6483 - val_accuracy: 0.7000\n",
      "Epoch 433/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 0.9833 - val_loss: 0.3342 - val_accuracy: 0.9000\n",
      "Epoch 434/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0212 - accuracy: 0.9917 - val_loss: 1.2533 - val_accuracy: 0.6333\n",
      "Epoch 435/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.9917 - val_loss: 1.6479 - val_accuracy: 0.6333\n",
      "Epoch 436/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 0.9917 - val_loss: 0.3262 - val_accuracy: 0.9333\n",
      "Epoch 437/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0156 - accuracy: 0.9917 - val_loss: 0.6341 - val_accuracy: 0.8333\n",
      "Epoch 438/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9750 - val_loss: 0.0713 - val_accuracy: 0.9667\n",
      "Epoch 439/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0271 - accuracy: 0.9917 - val_loss: 1.3785 - val_accuracy: 0.4667\n",
      "Epoch 440/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0517 - accuracy: 0.9750 - val_loss: 0.3947 - val_accuracy: 0.8000\n",
      "Epoch 441/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0246 - accuracy: 0.9917 - val_loss: 0.5808 - val_accuracy: 0.8000\n",
      "Epoch 442/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.3261 - val_accuracy: 0.9000\n",
      "Epoch 443/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0212 - accuracy: 0.9917 - val_loss: 0.4524 - val_accuracy: 0.8333\n",
      "Epoch 444/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0236 - accuracy: 0.9833 - val_loss: 0.4141 - val_accuracy: 0.8333\n",
      "Epoch 445/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0399 - accuracy: 0.9750 - val_loss: 0.6365 - val_accuracy: 0.8000\n",
      "Epoch 446/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0253 - accuracy: 0.9917 - val_loss: 0.4758 - val_accuracy: 0.8000\n",
      "Epoch 447/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0195 - accuracy: 0.9917 - val_loss: 0.8132 - val_accuracy: 0.7000\n",
      "Epoch 448/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0278 - accuracy: 0.9917 - val_loss: 1.3663 - val_accuracy: 0.6333\n",
      "Epoch 449/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0292 - accuracy: 0.9750 - val_loss: 0.3128 - val_accuracy: 0.9333\n",
      "Epoch 450/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0448 - accuracy: 0.9750 - val_loss: 2.8160 - val_accuracy: 0.5667\n",
      "Epoch 451/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 0.9917 - val_loss: 0.3909 - val_accuracy: 0.9333\n",
      "Epoch 452/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 0.9917 - val_loss: 0.4942 - val_accuracy: 0.9000\n",
      "Epoch 453/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 0.9833 - val_loss: 1.0773 - val_accuracy: 0.7333\n",
      "Epoch 454/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0219 - accuracy: 0.9917 - val_loss: 1.1628 - val_accuracy: 0.7333\n",
      "Epoch 455/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.9833 - val_loss: 0.0272 - val_accuracy: 1.0000\n",
      "Epoch 456/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0286 - accuracy: 0.9833 - val_loss: 0.4874 - val_accuracy: 0.8333\n",
      "Epoch 457/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0263 - accuracy: 0.9833 - val_loss: 0.8429 - val_accuracy: 0.7333\n",
      "Epoch 458/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.5844 - val_accuracy: 0.8333\n",
      "Epoch 459/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0173 - accuracy: 0.9833 - val_loss: 1.4445 - val_accuracy: 0.6667\n",
      "Epoch 460/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9750 - val_loss: 1.2119 - val_accuracy: 0.6333\n",
      "Epoch 461/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.5340 - val_accuracy: 0.8333\n",
      "Epoch 462/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0198 - accuracy: 0.9917 - val_loss: 0.5210 - val_accuracy: 0.8333\n",
      "Epoch 463/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0191 - accuracy: 0.9917 - val_loss: 0.7175 - val_accuracy: 0.8000\n",
      "Epoch 464/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0138 - accuracy: 0.9917 - val_loss: 0.3919 - val_accuracy: 0.9333\n",
      "Epoch 465/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0176 - accuracy: 0.9917 - val_loss: 1.6577 - val_accuracy: 0.6667\n",
      "Epoch 466/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9750 - val_loss: 0.2656 - val_accuracy: 0.9333\n",
      "Epoch 467/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0216 - accuracy: 0.9917 - val_loss: 0.1903 - val_accuracy: 0.9333\n",
      "Epoch 468/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9667 - val_loss: 0.2013 - val_accuracy: 0.9333\n",
      "Epoch 469/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0207 - accuracy: 0.9917 - val_loss: 0.5599 - val_accuracy: 0.8333\n",
      "Epoch 470/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0163 - accuracy: 0.9917 - val_loss: 0.2339 - val_accuracy: 0.9333\n",
      "Epoch 471/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0311 - accuracy: 0.9917 - val_loss: 1.0101 - val_accuracy: 0.7333\n",
      "Epoch 472/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0238 - accuracy: 0.9917 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 473/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0409 - accuracy: 0.9750 - val_loss: 0.5615 - val_accuracy: 0.8000\n",
      "Epoch 474/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0179 - accuracy: 0.9833 - val_loss: 1.1675 - val_accuracy: 0.6333\n",
      "Epoch 475/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0138 - accuracy: 0.9917 - val_loss: 0.1512 - val_accuracy: 0.9333\n",
      "Epoch 476/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0268 - accuracy: 0.9833 - val_loss: 0.9121 - val_accuracy: 0.7333\n",
      "Epoch 477/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 0.9917 - val_loss: 0.8584 - val_accuracy: 0.8000\n",
      "Epoch 478/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0184 - accuracy: 0.9833 - val_loss: 0.1350 - val_accuracy: 0.9333\n",
      "Epoch 479/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9833 - val_loss: 1.2448 - val_accuracy: 0.7000\n",
      "Epoch 480/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0193 - accuracy: 0.9917 - val_loss: 0.4931 - val_accuracy: 0.8667\n",
      "Epoch 481/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9917 - val_loss: 0.3566 - val_accuracy: 0.9333\n",
      "Epoch 482/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0209 - accuracy: 0.9917 - val_loss: 4.1116 - val_accuracy: 0.5000\n",
      "Epoch 483/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0404 - accuracy: 0.9833 - val_loss: 0.2332 - val_accuracy: 0.9333\n",
      "Epoch 484/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.9917 - val_loss: 1.0665 - val_accuracy: 0.7000\n",
      "Epoch 485/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.4586 - val_accuracy: 0.9333\n",
      "Epoch 486/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0287 - accuracy: 0.9833 - val_loss: 0.0529 - val_accuracy: 0.9667\n",
      "Epoch 487/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 0.9917 - val_loss: 0.2979 - val_accuracy: 0.9333\n",
      "Epoch 488/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0141 - accuracy: 0.9833 - val_loss: 2.9322 - val_accuracy: 0.6333\n",
      "Epoch 489/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9750 - val_loss: 0.5604 - val_accuracy: 0.7000\n",
      "Epoch 490/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0272 - accuracy: 0.9917 - val_loss: 0.5304 - val_accuracy: 0.8000\n",
      "Epoch 491/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0203 - accuracy: 0.9917 - val_loss: 0.2562 - val_accuracy: 0.9333\n",
      "Epoch 492/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0190 - accuracy: 0.9917 - val_loss: 0.3104 - val_accuracy: 0.9333\n",
      "Epoch 493/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0228 - accuracy: 0.9833 - val_loss: 0.9148 - val_accuracy: 0.7667\n",
      "Epoch 494/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.9583 - val_loss: 0.0700 - val_accuracy: 0.9667\n",
      "Epoch 495/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.9833 - val_loss: 0.6735 - val_accuracy: 0.8000\n",
      "Epoch 496/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0203 - accuracy: 0.9917 - val_loss: 0.5132 - val_accuracy: 0.8667\n",
      "Epoch 497/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0178 - accuracy: 0.9917 - val_loss: 0.4448 - val_accuracy: 0.9333\n",
      "Epoch 498/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 2.7071 - val_accuracy: 0.6333\n",
      "Epoch 499/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0413 - accuracy: 0.9750 - val_loss: 1.0191 - val_accuracy: 0.7333\n",
      "Epoch 500/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0252 - accuracy: 0.9833 - val_loss: 1.1191 - val_accuracy: 0.7000\n",
      "Epoch 501/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.9833 - val_loss: 0.6706 - val_accuracy: 0.8000\n",
      "Epoch 502/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0186 - accuracy: 0.9917 - val_loss: 0.7237 - val_accuracy: 0.8000\n",
      "Epoch 503/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0196 - accuracy: 0.9917 - val_loss: 0.3671 - val_accuracy: 0.9333\n",
      "Epoch 504/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0142 - accuracy: 0.9917 - val_loss: 1.7001 - val_accuracy: 0.7000\n",
      "Epoch 505/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.9833 - val_loss: 0.2443 - val_accuracy: 0.9333\n",
      "Epoch 506/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0294 - accuracy: 0.9917 - val_loss: 0.4983 - val_accuracy: 0.8000\n",
      "Epoch 507/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0224 - accuracy: 0.9917 - val_loss: 0.5599 - val_accuracy: 0.8333\n",
      "Epoch 508/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 0.7606 - val_accuracy: 0.8000\n",
      "Epoch 509/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 0.9917 - val_loss: 1.2218 - val_accuracy: 0.7333\n",
      "Epoch 510/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 0.9917 - val_loss: 0.6905 - val_accuracy: 0.8333\n",
      "Epoch 511/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 0.9917 - val_loss: 0.9259 - val_accuracy: 0.8000\n",
      "Epoch 512/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 2.2876 - val_accuracy: 0.6333\n",
      "Epoch 513/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9583 - val_loss: 0.3721 - val_accuracy: 0.9333\n",
      "Epoch 514/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9667 - val_loss: 0.2111 - val_accuracy: 0.9333\n",
      "Epoch 515/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0301 - accuracy: 0.9917 - val_loss: 0.3489 - val_accuracy: 0.9000\n",
      "Epoch 516/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0278 - accuracy: 0.9917 - val_loss: 0.6367 - val_accuracy: 0.7667\n",
      "Epoch 517/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 0.9917 - val_loss: 0.5626 - val_accuracy: 0.8333\n",
      "Epoch 518/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 0.9917 - val_loss: 1.2423 - val_accuracy: 0.6667\n",
      "Epoch 519/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0147 - accuracy: 0.9917 - val_loss: 0.3038 - val_accuracy: 0.9333\n",
      "Epoch 520/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0378 - accuracy: 0.9750 - val_loss: 0.3969 - val_accuracy: 0.8333\n",
      "Epoch 521/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0228 - accuracy: 0.9917 - val_loss: 0.6363 - val_accuracy: 0.7667\n",
      "Epoch 522/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0190 - accuracy: 0.9833 - val_loss: 0.3631 - val_accuracy: 0.9333\n",
      "Epoch 523/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 0.9917 - val_loss: 0.7854 - val_accuracy: 0.7667\n",
      "Epoch 524/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0275 - accuracy: 0.9917 - val_loss: 2.2351 - val_accuracy: 0.5667\n",
      "Epoch 525/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0478 - accuracy: 0.9750 - val_loss: 0.4139 - val_accuracy: 0.8333\n",
      "Epoch 526/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0209 - accuracy: 0.9917 - val_loss: 0.7391 - val_accuracy: 0.7333\n",
      "Epoch 527/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0194 - accuracy: 0.9917 - val_loss: 0.4663 - val_accuracy: 0.8333\n",
      "Epoch 528/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0160 - accuracy: 0.9917 - val_loss: 0.4346 - val_accuracy: 0.9333\n",
      "Epoch 529/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0255 - accuracy: 0.9917 - val_loss: 0.3941 - val_accuracy: 0.9333\n",
      "Epoch 530/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0208 - accuracy: 0.9917 - val_loss: 0.6949 - val_accuracy: 0.8333\n",
      "Epoch 531/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.1377 - val_accuracy: 0.9333\n",
      "Epoch 532/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0300 - accuracy: 0.9917 - val_loss: 0.6493 - val_accuracy: 0.7000\n",
      "Epoch 533/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0216 - accuracy: 0.9917 - val_loss: 0.6693 - val_accuracy: 0.7333\n",
      "Epoch 534/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0244 - accuracy: 0.9917 - val_loss: 0.5418 - val_accuracy: 0.8333\n",
      "Epoch 535/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0207 - accuracy: 0.9833 - val_loss: 0.4067 - val_accuracy: 0.8667\n",
      "Epoch 536/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 0.9917 - val_loss: 0.3522 - val_accuracy: 0.9333\n",
      "Epoch 537/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0267 - accuracy: 0.9833 - val_loss: 0.6991 - val_accuracy: 0.7667\n",
      "Epoch 538/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0184 - accuracy: 0.9917 - val_loss: 0.4297 - val_accuracy: 0.9333\n",
      "Epoch 539/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0182 - accuracy: 0.9917 - val_loss: 2.6249 - val_accuracy: 0.6000\n",
      "Epoch 540/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 0.9833 - val_loss: 0.5065 - val_accuracy: 0.7333\n",
      "Epoch 541/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9833 - val_loss: 0.7500 - val_accuracy: 0.6333\n",
      "Epoch 542/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0272 - accuracy: 0.9833 - val_loss: 1.0693 - val_accuracy: 0.6333\n",
      "Epoch 543/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.9833 - val_loss: 0.5646 - val_accuracy: 0.7667\n",
      "Epoch 544/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0440 - accuracy: 0.9833 - val_loss: 0.3427 - val_accuracy: 0.8000\n",
      "Epoch 545/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0205 - accuracy: 0.9917 - val_loss: 0.5709 - val_accuracy: 0.7333\n",
      "Epoch 546/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 0.9917 - val_loss: 0.2855 - val_accuracy: 0.9333\n",
      "Epoch 547/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0206 - accuracy: 0.9917 - val_loss: 0.8236 - val_accuracy: 0.7333\n",
      "Epoch 548/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0169 - accuracy: 0.9917 - val_loss: 1.0468 - val_accuracy: 0.6667\n",
      "Epoch 549/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0233 - accuracy: 0.9833 - val_loss: 2.0999 - val_accuracy: 0.6333\n",
      "Epoch 550/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.9833 - val_loss: 0.2735 - val_accuracy: 0.9000\n",
      "Epoch 551/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 0.9917 - val_loss: 0.5741 - val_accuracy: 0.7667\n",
      "Epoch 552/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0204 - accuracy: 0.9917 - val_loss: 0.2567 - val_accuracy: 0.9333\n",
      "Epoch 553/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0150 - accuracy: 0.9917 - val_loss: 0.2595 - val_accuracy: 0.9333\n",
      "Epoch 554/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0348 - accuracy: 0.9833 - val_loss: 0.6388 - val_accuracy: 0.7333\n",
      "Epoch 555/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0176 - accuracy: 0.9917 - val_loss: 0.2074 - val_accuracy: 0.9333\n",
      "Epoch 556/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 0.5598 - val_accuracy: 0.8333\n",
      "Epoch 557/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0225 - accuracy: 0.9917 - val_loss: 0.3753 - val_accuracy: 0.9333\n",
      "Epoch 558/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0305 - accuracy: 0.9750 - val_loss: 0.2155 - val_accuracy: 0.9333\n",
      "Epoch 559/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0222 - accuracy: 0.9917 - val_loss: 1.3221 - val_accuracy: 0.7000\n",
      "Epoch 560/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 1.0623 - val_accuracy: 0.7667\n",
      "Epoch 561/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0370 - accuracy: 0.9917 - val_loss: 2.4050 - val_accuracy: 0.6333\n",
      "Epoch 562/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0396 - accuracy: 0.9750 - val_loss: 0.4996 - val_accuracy: 0.8333\n",
      "Epoch 563/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 0.9833 - val_loss: 0.3134 - val_accuracy: 0.9333\n",
      "Epoch 564/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.9833 - val_loss: 0.1999 - val_accuracy: 0.9333\n",
      "Epoch 565/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.9917 - val_loss: 0.1878 - val_accuracy: 0.9333\n",
      "Epoch 566/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0228 - accuracy: 0.9833 - val_loss: 0.2861 - val_accuracy: 0.8333\n",
      "Epoch 567/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0206 - accuracy: 0.9917 - val_loss: 0.2102 - val_accuracy: 0.9333\n",
      "Epoch 568/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0292 - accuracy: 0.9917 - val_loss: 0.1380 - val_accuracy: 0.9333\n",
      "Epoch 569/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0391 - accuracy: 0.9833 - val_loss: 0.1090 - val_accuracy: 0.9667\n",
      "Epoch 570/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0329 - accuracy: 0.9833 - val_loss: 0.6304 - val_accuracy: 0.7667\n",
      "Epoch 571/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0215 - accuracy: 0.9917 - val_loss: 0.6910 - val_accuracy: 0.7667\n",
      "Epoch 572/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 0.9833 - val_loss: 0.4976 - val_accuracy: 0.8333\n",
      "Epoch 573/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.2714 - val_accuracy: 0.9333\n",
      "Epoch 574/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0490 - accuracy: 0.9750 - val_loss: 0.5543 - val_accuracy: 0.8000\n",
      "Epoch 575/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0320 - accuracy: 0.9750 - val_loss: 1.2104 - val_accuracy: 0.6667\n",
      "Epoch 576/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0268 - accuracy: 0.9833 - val_loss: 0.7288 - val_accuracy: 0.7667\n",
      "Epoch 577/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0203 - accuracy: 0.9917 - val_loss: 2.0881 - val_accuracy: 0.6333\n",
      "Epoch 578/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0410 - accuracy: 0.9833 - val_loss: 0.2949 - val_accuracy: 0.8667\n",
      "Epoch 579/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0224 - accuracy: 0.9917 - val_loss: 0.4444 - val_accuracy: 0.8333\n",
      "Epoch 580/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.3760 - val_accuracy: 0.9333\n",
      "Epoch 581/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 2.7999 - val_accuracy: 0.6333\n",
      "Epoch 582/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.9667 - val_loss: 0.3857 - val_accuracy: 0.8000\n",
      "Epoch 583/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 0.9917 - val_loss: 0.5621 - val_accuracy: 0.7667\n",
      "Epoch 584/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.4164 - val_accuracy: 0.8333\n",
      "Epoch 585/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0187 - accuracy: 0.9917 - val_loss: 0.4582 - val_accuracy: 0.8333\n",
      "Epoch 586/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0182 - accuracy: 0.9917 - val_loss: 0.3953 - val_accuracy: 0.9000\n",
      "Epoch 587/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0175 - accuracy: 0.9917 - val_loss: 0.3932 - val_accuracy: 0.9000\n",
      "Epoch 588/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0132 - accuracy: 0.9917 - val_loss: 2.6323 - val_accuracy: 0.6333\n",
      "Epoch 589/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9750 - val_loss: 0.3248 - val_accuracy: 0.9000\n",
      "Epoch 590/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 0.9917 - val_loss: 0.4471 - val_accuracy: 0.8333\n",
      "Epoch 591/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0447 - accuracy: 0.9667 - val_loss: 0.0443 - val_accuracy: 0.9667\n",
      "Epoch 592/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0166 - accuracy: 0.9917 - val_loss: 0.6754 - val_accuracy: 0.7667\n",
      "Epoch 593/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0141 - accuracy: 0.9917 - val_loss: 0.2212 - val_accuracy: 0.9333\n",
      "Epoch 594/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0207 - accuracy: 0.9917 - val_loss: 0.4517 - val_accuracy: 0.8333\n",
      "Epoch 595/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0156 - accuracy: 0.9917 - val_loss: 0.2616 - val_accuracy: 0.9333\n",
      "Epoch 596/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0209 - accuracy: 0.9917 - val_loss: 2.0034 - val_accuracy: 0.6333\n",
      "Epoch 597/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0333 - accuracy: 0.9833 - val_loss: 1.4565 - val_accuracy: 0.6333\n",
      "Epoch 598/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0472 - accuracy: 0.9833 - val_loss: 0.4616 - val_accuracy: 0.9000\n",
      "Epoch 599/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0171 - accuracy: 0.9917 - val_loss: 0.7721 - val_accuracy: 0.8333\n",
      "Epoch 600/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0184 - accuracy: 0.9917 - val_loss: 0.7629 - val_accuracy: 0.8333\n",
      "Epoch 601/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 2.2495 - val_accuracy: 0.6333\n",
      "Epoch 602/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.2986 - val_accuracy: 0.9333\n",
      "Epoch 603/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0376 - accuracy: 0.9833 - val_loss: 0.3044 - val_accuracy: 0.9333\n",
      "Epoch 604/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 0.9917 - val_loss: 1.7586 - val_accuracy: 0.7000\n",
      "Epoch 605/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0316 - accuracy: 0.9917 - val_loss: 1.3628 - val_accuracy: 0.7000\n",
      "Epoch 606/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0340 - accuracy: 0.9917 - val_loss: 0.0409 - val_accuracy: 1.0000\n",
      "Epoch 607/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0364 - accuracy: 0.9833 - val_loss: 0.1404 - val_accuracy: 0.9667\n",
      "Epoch 608/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0309 - accuracy: 0.9917 - val_loss: 0.4023 - val_accuracy: 0.9333\n",
      "Epoch 609/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0168 - accuracy: 0.9917 - val_loss: 1.1320 - val_accuracy: 0.7333\n",
      "Epoch 610/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.6635 - val_accuracy: 0.9000\n",
      "Epoch 611/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0190 - accuracy: 0.9917 - val_loss: 1.8533 - val_accuracy: 0.7000\n",
      "Epoch 612/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9667 - val_loss: 0.2951 - val_accuracy: 0.9000\n",
      "Epoch 613/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0256 - accuracy: 0.9833 - val_loss: 0.2397 - val_accuracy: 0.9333\n",
      "Epoch 614/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9833 - val_loss: 0.4334 - val_accuracy: 0.9333\n",
      "Epoch 615/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0260 - accuracy: 0.9833 - val_loss: 0.9845 - val_accuracy: 0.7333\n",
      "Epoch 616/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.9917 - val_loss: 0.9244 - val_accuracy: 0.7667\n",
      "Epoch 617/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0166 - accuracy: 0.9917 - val_loss: 0.7591 - val_accuracy: 0.8333\n",
      "Epoch 618/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 0.9917 - val_loss: 0.6443 - val_accuracy: 0.8333\n",
      "Epoch 619/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0149 - accuracy: 0.9917 - val_loss: 1.5727 - val_accuracy: 0.7333\n",
      "Epoch 620/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9833 - val_loss: 0.1594 - val_accuracy: 0.9333\n",
      "Epoch 621/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0208 - accuracy: 0.9917 - val_loss: 0.8294 - val_accuracy: 0.6667\n",
      "Epoch 622/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0175 - accuracy: 0.9917 - val_loss: 0.8964 - val_accuracy: 0.7333\n",
      "Epoch 623/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0278 - accuracy: 0.9750 - val_loss: 0.9443 - val_accuracy: 0.6333\n",
      "Epoch 624/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.4355 - val_accuracy: 0.8333\n",
      "Epoch 625/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9833 - val_loss: 1.1667 - val_accuracy: 0.6333\n",
      "Epoch 626/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0175 - accuracy: 0.9917 - val_loss: 0.4213 - val_accuracy: 0.8333\n",
      "Epoch 627/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0160 - accuracy: 0.9833 - val_loss: 0.3563 - val_accuracy: 0.8333\n",
      "Epoch 628/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0087 - accuracy: 0.9917 - val_loss: 1.5056 - val_accuracy: 0.6333\n",
      "Epoch 629/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0194 - accuracy: 0.9917 - val_loss: 0.0741 - val_accuracy: 0.9667\n",
      "Epoch 630/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.9750 - val_loss: 0.2364 - val_accuracy: 0.9333\n",
      "Epoch 631/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.9917 - val_loss: 0.2050 - val_accuracy: 0.9333\n",
      "Epoch 632/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.4549 - val_accuracy: 0.8333\n",
      "Epoch 633/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0181 - accuracy: 0.9917 - val_loss: 0.3363 - val_accuracy: 0.9333\n",
      "Epoch 634/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0152 - accuracy: 0.9917 - val_loss: 2.4994 - val_accuracy: 0.6333\n",
      "Epoch 635/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0365 - accuracy: 0.9750 - val_loss: 1.2272 - val_accuracy: 0.6333\n",
      "Epoch 636/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.1322 - val_accuracy: 0.9333\n",
      "Epoch 637/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0271 - accuracy: 0.9833 - val_loss: 0.3694 - val_accuracy: 0.8667\n",
      "Epoch 638/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0165 - accuracy: 0.9917 - val_loss: 0.4799 - val_accuracy: 0.8000\n",
      "Epoch 639/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0144 - accuracy: 0.9917 - val_loss: 0.2385 - val_accuracy: 0.9333\n",
      "Epoch 640/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 2.2143 - val_accuracy: 0.6333\n",
      "Epoch 641/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9500 - val_loss: 0.3619 - val_accuracy: 0.7667\n",
      "Epoch 642/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0270 - accuracy: 0.9917 - val_loss: 0.2691 - val_accuracy: 0.9000\n",
      "Epoch 643/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9833 - val_loss: 0.4223 - val_accuracy: 0.7667\n",
      "Epoch 644/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0243 - accuracy: 0.9833 - val_loss: 0.3797 - val_accuracy: 0.8333\n",
      "Epoch 645/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0177 - accuracy: 0.9917 - val_loss: 0.9614 - val_accuracy: 0.6333\n",
      "Epoch 646/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.2688 - val_accuracy: 0.9333\n",
      "Epoch 647/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0278 - accuracy: 0.9833 - val_loss: 0.3100 - val_accuracy: 0.9333\n",
      "Epoch 648/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 1.7309 - val_accuracy: 0.6333\n",
      "Epoch 649/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9750 - val_loss: 0.3897 - val_accuracy: 0.7667\n",
      "Epoch 650/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 0.9917 - val_loss: 0.4357 - val_accuracy: 0.7667\n",
      "Epoch 651/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0190 - accuracy: 0.9917 - val_loss: 0.7310 - val_accuracy: 0.6667\n",
      "Epoch 652/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 0.9833 - val_loss: 0.1968 - val_accuracy: 0.9333\n",
      "Epoch 653/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 0.9917 - val_loss: 0.5460 - val_accuracy: 0.7667\n",
      "Epoch 654/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.9917 - val_loss: 0.3054 - val_accuracy: 0.9000\n",
      "Epoch 655/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0224 - accuracy: 0.9833 - val_loss: 0.3527 - val_accuracy: 0.9000\n",
      "Epoch 656/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0222 - accuracy: 0.9833 - val_loss: 0.1755 - val_accuracy: 0.9333\n",
      "Epoch 657/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0122 - accuracy: 0.9917 - val_loss: 1.4329 - val_accuracy: 0.6333\n",
      "Epoch 658/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.9583 - val_loss: 0.3687 - val_accuracy: 0.9000\n",
      "Epoch 659/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 0.9833 - val_loss: 0.3012 - val_accuracy: 0.9333\n",
      "Epoch 660/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0186 - accuracy: 0.9917 - val_loss: 0.8716 - val_accuracy: 0.7333\n",
      "Epoch 661/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.3771 - val_accuracy: 0.9333\n",
      "Epoch 662/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0125 - accuracy: 0.9917 - val_loss: 1.9202 - val_accuracy: 0.6333\n",
      "Epoch 663/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0530 - accuracy: 0.9833 - val_loss: 0.5785 - val_accuracy: 0.7667\n",
      "Epoch 664/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0276 - accuracy: 0.9917 - val_loss: 0.5234 - val_accuracy: 0.7667\n",
      "Epoch 665/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 0.9917 - val_loss: 1.0482 - val_accuracy: 0.6667\n",
      "Epoch 666/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0256 - accuracy: 0.9917 - val_loss: 0.5198 - val_accuracy: 0.8000\n",
      "Epoch 667/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0142 - accuracy: 0.9917 - val_loss: 0.2882 - val_accuracy: 0.9333\n",
      "Epoch 668/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0317 - accuracy: 0.9750 - val_loss: 0.5857 - val_accuracy: 0.8000\n",
      "Epoch 669/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0286 - accuracy: 0.9833 - val_loss: 0.1371 - val_accuracy: 0.9333\n",
      "Epoch 670/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0387 - accuracy: 0.9833 - val_loss: 0.2482 - val_accuracy: 0.9333\n",
      "Epoch 671/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 0.9833 - val_loss: 0.2597 - val_accuracy: 0.9333\n",
      "Epoch 672/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0495 - accuracy: 0.9833 - val_loss: 0.6412 - val_accuracy: 0.7000\n",
      "Epoch 673/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0135 - accuracy: 0.9917 - val_loss: 0.2150 - val_accuracy: 0.9333\n",
      "Epoch 674/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0247 - accuracy: 0.9833 - val_loss: 0.1804 - val_accuracy: 0.9333\n",
      "Epoch 675/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0287 - accuracy: 0.9833 - val_loss: 0.3016 - val_accuracy: 0.9333\n",
      "Epoch 676/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0280 - accuracy: 0.9833 - val_loss: 0.2894 - val_accuracy: 0.9333\n",
      "Epoch 677/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0224 - accuracy: 0.9833 - val_loss: 0.3918 - val_accuracy: 0.9333\n",
      "Epoch 678/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.9917 - val_loss: 0.1613 - val_accuracy: 0.9333\n",
      "Epoch 679/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0252 - accuracy: 0.9917 - val_loss: 0.3486 - val_accuracy: 0.9333\n",
      "Epoch 680/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0408 - accuracy: 0.9833 - val_loss: 0.3634 - val_accuracy: 0.9333\n",
      "Epoch 681/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0185 - accuracy: 0.9917 - val_loss: 0.3051 - val_accuracy: 0.9333\n",
      "Epoch 682/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0130 - accuracy: 0.9917 - val_loss: 1.4367 - val_accuracy: 0.6333\n",
      "Epoch 683/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0272 - accuracy: 0.9833 - val_loss: 1.5357 - val_accuracy: 0.6333\n",
      "Epoch 684/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0099 - accuracy: 0.9917 - val_loss: 0.3395 - val_accuracy: 0.9333\n",
      "Epoch 685/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9917 - val_loss: 3.4113 - val_accuracy: 0.5667\n",
      "Epoch 686/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9667 - val_loss: 0.4435 - val_accuracy: 0.7667\n",
      "Epoch 687/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 0.9917 - val_loss: 0.6612 - val_accuracy: 0.7333\n",
      "Epoch 688/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0236 - accuracy: 0.9917 - val_loss: 0.3830 - val_accuracy: 0.9000\n",
      "Epoch 689/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0178 - accuracy: 0.9917 - val_loss: 0.4385 - val_accuracy: 0.8667\n",
      "Epoch 690/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0230 - accuracy: 0.9833 - val_loss: 0.3083 - val_accuracy: 0.9333\n",
      "Epoch 691/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 0.9917 - val_loss: 0.8343 - val_accuracy: 0.7667\n",
      "Epoch 692/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.1622 - val_accuracy: 0.9333\n",
      "Epoch 693/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0276 - accuracy: 0.9833 - val_loss: 0.0707 - val_accuracy: 0.9667\n",
      "Epoch 694/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9833 - val_loss: 0.6563 - val_accuracy: 0.7667\n",
      "Epoch 695/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 0.9917 - val_loss: 0.9724 - val_accuracy: 0.7000\n",
      "Epoch 696/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0127 - accuracy: 0.9917 - val_loss: 0.2884 - val_accuracy: 0.9333\n",
      "Epoch 697/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0228 - accuracy: 0.9917 - val_loss: 0.4056 - val_accuracy: 0.9333\n",
      "Epoch 698/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0178 - accuracy: 0.9917 - val_loss: 0.6122 - val_accuracy: 0.8333\n",
      "Epoch 699/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0115 - accuracy: 0.9917 - val_loss: 0.8850 - val_accuracy: 0.7667\n",
      "Epoch 700/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0488 - accuracy: 0.9833 - val_loss: 0.1890 - val_accuracy: 0.9333\n",
      "Epoch 701/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 0.9917 - val_loss: 0.4520 - val_accuracy: 0.9000\n",
      "Epoch 702/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0174 - accuracy: 0.9917 - val_loss: 0.4613 - val_accuracy: 0.9000\n",
      "Epoch 703/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 0.9917 - val_loss: 0.6275 - val_accuracy: 0.8333\n",
      "Epoch 704/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0391 - accuracy: 0.9750 - val_loss: 0.0557 - val_accuracy: 0.9667\n",
      "Epoch 705/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0209 - accuracy: 0.9917 - val_loss: 0.4135 - val_accuracy: 0.9333\n",
      "Epoch 706/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0280 - accuracy: 0.9917 - val_loss: 0.0707 - val_accuracy: 0.9667\n",
      "Epoch 707/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.4454 - val_accuracy: 0.9000\n",
      "Epoch 708/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0266 - accuracy: 0.9917 - val_loss: 2.1965 - val_accuracy: 0.6333\n",
      "Epoch 709/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0225 - accuracy: 0.9917 - val_loss: 0.4859 - val_accuracy: 0.9000\n",
      "Epoch 710/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 0.9833 - val_loss: 0.2933 - val_accuracy: 0.9333\n",
      "Epoch 711/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.6929 - val_accuracy: 0.8333\n",
      "Epoch 712/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0153 - accuracy: 0.9833 - val_loss: 0.7122 - val_accuracy: 0.8333\n",
      "Epoch 713/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0330 - accuracy: 0.9833 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 714/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9833 - val_loss: 2.2351 - val_accuracy: 0.6333\n",
      "Epoch 715/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9917 - val_loss: 1.3109 - val_accuracy: 0.7333\n",
      "Epoch 716/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0314 - accuracy: 0.9833 - val_loss: 0.0685 - val_accuracy: 0.9667\n",
      "Epoch 717/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0456 - accuracy: 0.9750 - val_loss: 0.5214 - val_accuracy: 0.8667\n",
      "Epoch 718/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0234 - accuracy: 0.9917 - val_loss: 0.2279 - val_accuracy: 0.9333\n",
      "Epoch 719/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0186 - accuracy: 0.9917 - val_loss: 1.0318 - val_accuracy: 0.7000\n",
      "Epoch 720/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0355 - accuracy: 0.9917 - val_loss: 0.7286 - val_accuracy: 0.8000\n",
      "Epoch 721/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0208 - accuracy: 0.9917 - val_loss: 0.6079 - val_accuracy: 0.8667\n",
      "Epoch 722/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.9917 - val_loss: 0.4560 - val_accuracy: 0.9333\n",
      "Epoch 723/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 0.9917 - val_loss: 2.2170 - val_accuracy: 0.6333\n",
      "Epoch 724/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0309 - accuracy: 0.9833 - val_loss: 0.5089 - val_accuracy: 0.9000\n",
      "Epoch 725/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0383 - accuracy: 0.9833 - val_loss: 0.4011 - val_accuracy: 0.9000\n",
      "Epoch 726/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0324 - accuracy: 0.9833 - val_loss: 0.9316 - val_accuracy: 0.6333\n",
      "Epoch 727/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0286 - accuracy: 0.9833 - val_loss: 0.4761 - val_accuracy: 0.7667\n",
      "Epoch 728/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.9917 - val_loss: 0.6050 - val_accuracy: 0.7667\n",
      "Epoch 729/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 0.9917 - val_loss: 0.2582 - val_accuracy: 0.9333\n",
      "Epoch 730/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.6988 - val_accuracy: 0.7667\n",
      "Epoch 731/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0193 - accuracy: 0.9833 - val_loss: 0.2778 - val_accuracy: 0.9333\n",
      "Epoch 732/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0321 - accuracy: 0.9833 - val_loss: 0.1946 - val_accuracy: 0.9333\n",
      "Epoch 733/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 0.9917 - val_loss: 1.1057 - val_accuracy: 0.7000\n",
      "Epoch 734/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0198 - accuracy: 0.9833 - val_loss: 0.6368 - val_accuracy: 0.8000\n",
      "Epoch 735/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9917 - val_loss: 2.1096 - val_accuracy: 0.6333\n",
      "Epoch 736/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.4281 - accuracy: 0.9500 - val_loss: 0.5039 - val_accuracy: 0.7333\n",
      "Epoch 737/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 0.4759 - val_accuracy: 0.8000\n",
      "Epoch 738/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0348 - accuracy: 0.9833 - val_loss: 0.3441 - val_accuracy: 0.8333\n",
      "Epoch 739/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0187 - accuracy: 0.9917 - val_loss: 0.2041 - val_accuracy: 0.9333\n",
      "Epoch 740/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0295 - accuracy: 0.9833 - val_loss: 0.6677 - val_accuracy: 0.7333\n",
      "Epoch 741/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 0.9917 - val_loss: 0.3162 - val_accuracy: 0.8667\n",
      "Epoch 742/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0209 - accuracy: 0.9917 - val_loss: 0.3356 - val_accuracy: 0.8667\n",
      "Epoch 743/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 0.9917 - val_loss: 0.5167 - val_accuracy: 0.8333\n",
      "Epoch 744/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0271 - accuracy: 0.9833 - val_loss: 0.4544 - val_accuracy: 0.8333\n",
      "Epoch 745/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9917 - val_loss: 0.4766 - val_accuracy: 0.8333\n",
      "Epoch 746/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0293 - accuracy: 0.9833 - val_loss: 0.5191 - val_accuracy: 0.8333\n",
      "Epoch 747/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9917 - val_loss: 0.7479 - val_accuracy: 0.8000\n",
      "Epoch 748/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0171 - accuracy: 0.9917 - val_loss: 0.9213 - val_accuracy: 0.7333\n",
      "Epoch 749/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0539 - accuracy: 0.9750 - val_loss: 0.0573 - val_accuracy: 0.9667\n",
      "Epoch 750/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0343 - accuracy: 0.9917 - val_loss: 0.4001 - val_accuracy: 0.8333\n",
      "Epoch 751/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 0.9917 - val_loss: 0.3458 - val_accuracy: 0.8667\n",
      "Epoch 752/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 0.9917 - val_loss: 0.5815 - val_accuracy: 0.8000\n",
      "Epoch 753/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 0.6283 - val_accuracy: 0.8000\n",
      "Epoch 754/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0179 - accuracy: 0.9917 - val_loss: 0.4511 - val_accuracy: 0.8333\n",
      "Epoch 755/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0142 - accuracy: 0.9917 - val_loss: 0.9709 - val_accuracy: 0.7000\n",
      "Epoch 756/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0233 - accuracy: 0.9833 - val_loss: 0.4531 - val_accuracy: 0.8333\n",
      "Epoch 757/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9917 - val_loss: 1.5623 - val_accuracy: 0.6333\n",
      "Epoch 758/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0247 - accuracy: 0.9833 - val_loss: 0.1679 - val_accuracy: 0.9333\n",
      "Epoch 759/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0182 - accuracy: 0.9917 - val_loss: 1.4830 - val_accuracy: 0.6333\n",
      "Epoch 760/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.3272 - val_accuracy: 0.9333\n",
      "Epoch 761/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0219 - accuracy: 0.9917 - val_loss: 5.3908 - val_accuracy: 0.4333\n",
      "Epoch 762/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0367 - accuracy: 0.9917 - val_loss: 0.2870 - val_accuracy: 0.9333\n",
      "Epoch 763/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0219 - accuracy: 0.9917 - val_loss: 0.8965 - val_accuracy: 0.7667\n",
      "Epoch 764/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0222 - accuracy: 0.9917 - val_loss: 0.4489 - val_accuracy: 0.8667\n",
      "Epoch 765/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9917 - val_loss: 0.1751 - val_accuracy: 0.9333\n",
      "Epoch 766/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0297 - accuracy: 0.9750 - val_loss: 0.3944 - val_accuracy: 0.8667\n",
      "Epoch 767/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0255 - accuracy: 0.9833 - val_loss: 0.4737 - val_accuracy: 0.8667\n",
      "Epoch 768/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0204 - accuracy: 0.9833 - val_loss: 0.1907 - val_accuracy: 0.9333\n",
      "Epoch 769/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0197 - accuracy: 0.9833 - val_loss: 0.3771 - val_accuracy: 0.9000\n",
      "Epoch 770/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0221 - accuracy: 0.9750 - val_loss: 0.3337 - val_accuracy: 0.9000\n",
      "Epoch 771/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.7477 - val_accuracy: 0.8667\n",
      "Epoch 772/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0120 - accuracy: 0.9917 - val_loss: 3.9926 - val_accuracy: 0.6000\n",
      "Epoch 773/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9667 - val_loss: 1.0284 - val_accuracy: 0.7333\n",
      "Epoch 774/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0189 - accuracy: 0.9833 - val_loss: 1.0646 - val_accuracy: 0.7000\n",
      "Epoch 775/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0177 - accuracy: 0.9917 - val_loss: 0.2732 - val_accuracy: 0.9333\n",
      "Epoch 776/1000\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.9833 - val_loss: 0.1896 - val_accuracy: 0.9333\n",
      "Epoch 777/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 2.1532 - val_accuracy: 0.6333\n",
      "Epoch 778/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0338 - accuracy: 0.9750 - val_loss: 0.6921 - val_accuracy: 0.8000\n",
      "Epoch 779/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 1.0713 - val_accuracy: 0.7333\n",
      "Epoch 780/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0187 - accuracy: 0.9833 - val_loss: 0.6358 - val_accuracy: 0.8667\n",
      "Epoch 781/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 1.0619 - val_accuracy: 0.7667\n",
      "Epoch 782/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9750 - val_loss: 1.3667 - val_accuracy: 0.7333\n",
      "Epoch 783/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.5740 - val_accuracy: 0.9000\n",
      "Epoch 784/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.9917 - val_loss: 0.4304 - val_accuracy: 0.9333\n",
      "Epoch 785/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0222 - accuracy: 0.9917 - val_loss: 0.6209 - val_accuracy: 0.9000\n",
      "Epoch 786/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0343 - accuracy: 0.9750 - val_loss: 0.0406 - val_accuracy: 0.9667\n",
      "Epoch 787/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 0.9917 - val_loss: 0.6933 - val_accuracy: 0.8000\n",
      "Epoch 788/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 0.9917 - val_loss: 1.6665 - val_accuracy: 0.6333\n",
      "Epoch 789/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.4496 - val_accuracy: 0.9333\n",
      "Epoch 790/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.9833 - val_loss: 0.7059 - val_accuracy: 0.8333\n",
      "Epoch 791/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0174 - accuracy: 0.9917 - val_loss: 0.6772 - val_accuracy: 0.8333\n",
      "Epoch 792/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0169 - accuracy: 0.9833 - val_loss: 0.5391 - val_accuracy: 0.9000\n",
      "Epoch 793/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0293 - accuracy: 0.9833 - val_loss: 0.0410 - val_accuracy: 0.9667\n",
      "Epoch 794/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0328 - accuracy: 0.9833 - val_loss: 0.3404 - val_accuracy: 0.9333\n",
      "Epoch 795/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 0.9917 - val_loss: 1.5876 - val_accuracy: 0.6333\n",
      "Epoch 796/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0292 - accuracy: 0.9750 - val_loss: 0.8022 - val_accuracy: 0.7667\n",
      "Epoch 797/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0145 - accuracy: 0.9917 - val_loss: 0.7857 - val_accuracy: 0.7667\n",
      "Epoch 798/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0234 - accuracy: 0.9917 - val_loss: 3.1542 - val_accuracy: 0.6000\n",
      "Epoch 799/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9500 - val_loss: 0.7631 - val_accuracy: 0.6333\n",
      "Epoch 800/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0144 - accuracy: 0.9917 - val_loss: 0.1559 - val_accuracy: 0.9333\n",
      "Epoch 801/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0239 - accuracy: 0.9917 - val_loss: 0.5730 - val_accuracy: 0.7667\n",
      "Epoch 802/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0189 - accuracy: 0.9833 - val_loss: 0.2617 - val_accuracy: 0.9333\n",
      "Epoch 803/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0230 - accuracy: 0.9833 - val_loss: 1.8462 - val_accuracy: 0.6333\n",
      "Epoch 804/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0275 - accuracy: 0.9750 - val_loss: 0.8786 - val_accuracy: 0.7667\n",
      "Epoch 805/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0157 - accuracy: 0.9917 - val_loss: 2.3581 - val_accuracy: 0.6333\n",
      "Epoch 806/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0342 - accuracy: 0.9750 - val_loss: 0.8130 - val_accuracy: 0.7667\n",
      "Epoch 807/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 1.2025 - val_accuracy: 0.7333\n",
      "Epoch 808/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0295 - accuracy: 0.9833 - val_loss: 0.9019 - val_accuracy: 0.7333\n",
      "Epoch 809/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9833 - val_loss: 0.1139 - val_accuracy: 0.9333\n",
      "Epoch 810/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.5471 - val_accuracy: 0.7333\n",
      "Epoch 811/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0260 - accuracy: 0.9833 - val_loss: 0.3666 - val_accuracy: 0.8000\n",
      "Epoch 812/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0222 - accuracy: 0.9833 - val_loss: 0.4190 - val_accuracy: 0.8000\n",
      "Epoch 813/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0306 - accuracy: 0.9833 - val_loss: 0.3267 - val_accuracy: 0.9000\n",
      "Epoch 814/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0120 - accuracy: 0.9917 - val_loss: 1.7566 - val_accuracy: 0.6333\n",
      "Epoch 815/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 0.9917 - val_loss: 0.3842 - val_accuracy: 0.9000\n",
      "Epoch 816/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0199 - accuracy: 0.9833 - val_loss: 0.1376 - val_accuracy: 0.9333\n",
      "Epoch 817/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 0.3996 - val_accuracy: 0.9000\n",
      "Epoch 818/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9833 - val_loss: 1.0143 - val_accuracy: 0.6333\n",
      "Epoch 819/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 0.9917 - val_loss: 0.1937 - val_accuracy: 0.9333\n",
      "Epoch 820/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0206 - accuracy: 0.9917 - val_loss: 0.4127 - val_accuracy: 0.8667\n",
      "Epoch 821/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0158 - accuracy: 0.9917 - val_loss: 0.5140 - val_accuracy: 0.8000\n",
      "Epoch 822/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0154 - accuracy: 0.9917 - val_loss: 0.6604 - val_accuracy: 0.7667\n",
      "Epoch 823/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0221 - accuracy: 0.9833 - val_loss: 0.2339 - val_accuracy: 0.9333\n",
      "Epoch 824/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0107 - accuracy: 0.9917 - val_loss: 0.6200 - val_accuracy: 0.8000\n",
      "Epoch 825/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 0.6218 - val_accuracy: 0.8000\n",
      "Epoch 826/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0087 - accuracy: 0.9917 - val_loss: 0.9451 - val_accuracy: 0.7667\n",
      "Epoch 827/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0219 - accuracy: 0.9917 - val_loss: 0.2006 - val_accuracy: 0.9333\n",
      "Epoch 828/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0500 - accuracy: 0.9833 - val_loss: 0.2459 - val_accuracy: 0.9000\n",
      "Epoch 829/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0168 - accuracy: 0.9917 - val_loss: 0.3121 - val_accuracy: 0.8000\n",
      "Epoch 830/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.1354 - val_accuracy: 0.9333\n",
      "Epoch 831/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9833 - val_loss: 0.5939 - val_accuracy: 0.7667\n",
      "Epoch 832/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0427 - accuracy: 0.9750 - val_loss: 0.5994 - val_accuracy: 0.7333\n",
      "Epoch 833/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.3754 - val_accuracy: 0.8333\n",
      "Epoch 834/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0168 - accuracy: 0.9833 - val_loss: 0.3710 - val_accuracy: 0.8333\n",
      "Epoch 835/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.1239 - val_accuracy: 0.6333\n",
      "Epoch 836/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0121 - accuracy: 0.9917 - val_loss: 0.1917 - val_accuracy: 0.9333\n",
      "Epoch 837/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0475 - accuracy: 0.9833 - val_loss: 0.6065 - val_accuracy: 0.9333\n",
      "Epoch 838/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9667 - val_loss: 0.3301 - val_accuracy: 0.9333\n",
      "Epoch 839/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.7754 - val_accuracy: 0.7667\n",
      "Epoch 840/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.7703 - val_accuracy: 0.8000\n",
      "Epoch 841/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0080 - accuracy: 0.9917 - val_loss: 3.6305 - val_accuracy: 0.6000\n",
      "Epoch 842/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.9833 - val_loss: 0.8080 - val_accuracy: 0.8000\n",
      "Epoch 843/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9917 - val_loss: 0.1650 - val_accuracy: 0.9333\n",
      "Epoch 844/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 0.4131 - val_accuracy: 0.8000\n",
      "Epoch 845/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 0.9917 - val_loss: 0.5941 - val_accuracy: 0.7667\n",
      "Epoch 846/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0114 - accuracy: 0.9917 - val_loss: 0.8687 - val_accuracy: 0.7333\n",
      "Epoch 847/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0348 - accuracy: 0.9833 - val_loss: 0.8111 - val_accuracy: 0.7333\n",
      "Epoch 848/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0122 - accuracy: 0.9917 - val_loss: 0.6828 - val_accuracy: 0.7667\n",
      "Epoch 849/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0125 - accuracy: 0.9917 - val_loss: 0.7794 - val_accuracy: 0.7667\n",
      "Epoch 850/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0194 - accuracy: 0.9917 - val_loss: 2.2418 - val_accuracy: 0.6333\n",
      "Epoch 851/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0256 - accuracy: 0.9917 - val_loss: 0.3058 - val_accuracy: 0.8667\n",
      "Epoch 852/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0220 - accuracy: 0.9833 - val_loss: 0.5051 - val_accuracy: 0.7667\n",
      "Epoch 853/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0132 - accuracy: 0.9917 - val_loss: 0.4262 - val_accuracy: 0.8333\n",
      "Epoch 854/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0107 - accuracy: 0.9917 - val_loss: 0.5674 - val_accuracy: 0.8000\n",
      "Epoch 855/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 0.9917 - val_loss: 0.8464 - val_accuracy: 0.8000\n",
      "Epoch 856/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0251 - accuracy: 0.9833 - val_loss: 0.3098 - val_accuracy: 0.9333\n",
      "Epoch 857/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 0.9917 - val_loss: 0.7206 - val_accuracy: 0.7667\n",
      "Epoch 858/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0278 - accuracy: 0.9833 - val_loss: 1.0902 - val_accuracy: 0.7333\n",
      "Epoch 859/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0158 - accuracy: 0.9917 - val_loss: 1.6311 - val_accuracy: 0.6333\n",
      "Epoch 860/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9917 - val_loss: 0.7143 - val_accuracy: 0.8000\n",
      "Epoch 861/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0126 - accuracy: 0.9917 - val_loss: 1.8151 - val_accuracy: 0.6667\n",
      "Epoch 862/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0156 - accuracy: 0.9833 - val_loss: 0.7336 - val_accuracy: 0.8000\n",
      "Epoch 863/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.6714 - val_accuracy: 0.7333\n",
      "Epoch 864/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.9917 - val_loss: 0.2846 - val_accuracy: 0.8333\n",
      "Epoch 865/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0176 - accuracy: 0.9833 - val_loss: 0.4265 - val_accuracy: 0.8000\n",
      "Epoch 866/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0390 - accuracy: 0.9750 - val_loss: 0.6814 - val_accuracy: 0.7333\n",
      "Epoch 867/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0142 - accuracy: 0.9917 - val_loss: 0.8624 - val_accuracy: 0.7333\n",
      "Epoch 868/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.6871 - val_accuracy: 0.7667\n",
      "Epoch 869/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0124 - accuracy: 0.9917 - val_loss: 0.5706 - val_accuracy: 0.8333\n",
      "Epoch 870/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0219 - accuracy: 0.9833 - val_loss: 0.1304 - val_accuracy: 0.9333\n",
      "Epoch 871/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 0.9833 - val_loss: 0.5148 - val_accuracy: 0.8667\n",
      "Epoch 872/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.3853 - val_accuracy: 0.9333\n",
      "Epoch 873/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.6680 - val_accuracy: 0.6333\n",
      "Epoch 874/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9750 - val_loss: 0.9072 - val_accuracy: 0.7667\n",
      "Epoch 875/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0420 - accuracy: 0.9833 - val_loss: 0.4767 - val_accuracy: 0.9000\n",
      "Epoch 876/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0152 - accuracy: 0.9917 - val_loss: 0.6612 - val_accuracy: 0.8000\n",
      "Epoch 877/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9833 - val_loss: 0.8570 - val_accuracy: 0.8000\n",
      "Epoch 878/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0170 - accuracy: 0.9833 - val_loss: 0.5850 - val_accuracy: 0.9333\n",
      "Epoch 879/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 0.9833 - val_loss: 0.5374 - val_accuracy: 0.9333\n",
      "Epoch 880/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0155 - accuracy: 0.9917 - val_loss: 0.5812 - val_accuracy: 0.9000\n",
      "Epoch 881/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 0.9917 - val_loss: 0.6821 - val_accuracy: 0.9333\n",
      "Epoch 882/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0305 - accuracy: 0.9833 - val_loss: 0.0579 - val_accuracy: 0.9667\n",
      "Epoch 883/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0343 - accuracy: 0.9833 - val_loss: 0.4079 - val_accuracy: 0.7667\n",
      "Epoch 884/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9917 - val_loss: 0.4970 - val_accuracy: 0.7667\n",
      "Epoch 885/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0147 - accuracy: 0.9917 - val_loss: 3.2945 - val_accuracy: 0.6000\n",
      "Epoch 886/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.9833 - val_loss: 0.6005 - val_accuracy: 0.8000\n",
      "Epoch 887/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0107 - accuracy: 0.9917 - val_loss: 1.4390 - val_accuracy: 0.6333\n",
      "Epoch 888/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0157 - accuracy: 0.9833 - val_loss: 1.9550 - val_accuracy: 0.6333\n",
      "Epoch 889/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0326 - accuracy: 0.9833 - val_loss: 0.4428 - val_accuracy: 0.8667\n",
      "Epoch 890/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0125 - accuracy: 0.9917 - val_loss: 1.1427 - val_accuracy: 0.6333\n",
      "Epoch 891/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0150 - accuracy: 0.9917 - val_loss: 0.5336 - val_accuracy: 0.8667\n",
      "Epoch 892/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 0.9833 - val_loss: 1.6448 - val_accuracy: 0.6333\n",
      "Epoch 893/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 1.1615 - val_accuracy: 0.7333\n",
      "Epoch 894/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 0.9833 - val_loss: 0.6394 - val_accuracy: 0.8000\n",
      "Epoch 895/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0428 - accuracy: 0.9833 - val_loss: 0.0975 - val_accuracy: 0.9667\n",
      "Epoch 896/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0345 - accuracy: 0.9750 - val_loss: 0.5807 - val_accuracy: 0.7333\n",
      "Epoch 897/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.2378 - val_accuracy: 0.9333\n",
      "Epoch 898/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.5563 - val_accuracy: 0.7667\n",
      "Epoch 899/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0108 - accuracy: 0.9917 - val_loss: 0.4901 - val_accuracy: 0.8000\n",
      "Epoch 900/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0141 - accuracy: 0.9833 - val_loss: 0.3088 - val_accuracy: 0.9333\n",
      "Epoch 901/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0378 - accuracy: 0.9833 - val_loss: 0.3115 - val_accuracy: 0.8333\n",
      "Epoch 902/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0196 - accuracy: 0.9917 - val_loss: 0.1576 - val_accuracy: 0.9333\n",
      "Epoch 903/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0503 - accuracy: 0.9750 - val_loss: 0.4402 - val_accuracy: 0.8000\n",
      "Epoch 904/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0139 - accuracy: 0.9917 - val_loss: 0.8330 - val_accuracy: 0.7333\n",
      "Epoch 905/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0185 - accuracy: 0.9917 - val_loss: 1.1082 - val_accuracy: 0.6667\n",
      "Epoch 906/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0169 - accuracy: 0.9917 - val_loss: 0.8147 - val_accuracy: 0.8000\n",
      "Epoch 907/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0157 - accuracy: 0.9917 - val_loss: 1.6251 - val_accuracy: 0.6333\n",
      "Epoch 908/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 0.9833 - val_loss: 1.6262 - val_accuracy: 0.6333\n",
      "Epoch 909/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0120 - accuracy: 0.9917 - val_loss: 0.5791 - val_accuracy: 0.9333\n",
      "Epoch 910/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0175 - accuracy: 0.9917 - val_loss: 0.6468 - val_accuracy: 0.9000\n",
      "Epoch 911/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.6295 - val_accuracy: 0.7333\n",
      "Epoch 912/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0482 - accuracy: 0.9917 - val_loss: 1.7128 - val_accuracy: 0.7000\n",
      "Epoch 913/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.6869 - val_accuracy: 0.9000\n",
      "Epoch 914/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0158 - accuracy: 0.9917 - val_loss: 0.6395 - val_accuracy: 0.9333\n",
      "Epoch 915/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0276 - accuracy: 0.9833 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 916/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0429 - accuracy: 0.9750 - val_loss: 2.1656 - val_accuracy: 0.6333\n",
      "Epoch 917/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.7742 - val_accuracy: 0.7667\n",
      "Epoch 918/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 0.7332 - val_accuracy: 0.8000\n",
      "Epoch 919/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0154 - accuracy: 0.9917 - val_loss: 1.1197 - val_accuracy: 0.7667\n",
      "Epoch 920/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0168 - accuracy: 0.9917 - val_loss: 0.3301 - val_accuracy: 0.9333\n",
      "Epoch 921/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0099 - accuracy: 0.9917 - val_loss: 1.6812 - val_accuracy: 0.6333\n",
      "Epoch 922/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0323 - accuracy: 0.9833 - val_loss: 1.5391 - val_accuracy: 0.7000\n",
      "Epoch 923/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.9750 - val_loss: 0.7175 - val_accuracy: 0.8000\n",
      "Epoch 924/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.5451 - val_accuracy: 0.9000\n",
      "Epoch 925/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0165 - accuracy: 0.9917 - val_loss: 0.5065 - val_accuracy: 0.9000\n",
      "Epoch 926/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 0.9917 - val_loss: 0.8739 - val_accuracy: 0.8000\n",
      "Epoch 927/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0124 - accuracy: 0.9917 - val_loss: 1.6133 - val_accuracy: 0.7000\n",
      "Epoch 928/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0302 - accuracy: 0.9917 - val_loss: 3.3703 - val_accuracy: 0.6333\n",
      "Epoch 929/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0227 - accuracy: 0.9917 - val_loss: 0.9415 - val_accuracy: 0.8667\n",
      "Epoch 930/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9583 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
      "Epoch 931/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0373 - accuracy: 0.9917 - val_loss: 0.2952 - val_accuracy: 0.8667\n",
      "Epoch 932/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0208 - accuracy: 0.9917 - val_loss: 0.9397 - val_accuracy: 0.7667\n",
      "Epoch 933/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0304 - accuracy: 0.9833 - val_loss: 0.7087 - val_accuracy: 0.7667\n",
      "Epoch 934/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.5115 - val_accuracy: 0.8667\n",
      "Epoch 935/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0152 - accuracy: 0.9917 - val_loss: 0.5293 - val_accuracy: 0.8667\n",
      "Epoch 936/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0385 - accuracy: 0.9750 - val_loss: 0.0695 - val_accuracy: 0.9667\n",
      "Epoch 937/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0269 - accuracy: 0.9833 - val_loss: 0.6312 - val_accuracy: 0.7667\n",
      "Epoch 938/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.4605 - val_accuracy: 0.8667\n",
      "Epoch 939/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.9833 - val_loss: 0.5144 - val_accuracy: 0.8667\n",
      "Epoch 940/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0215 - accuracy: 0.9833 - val_loss: 0.4222 - val_accuracy: 0.8667\n",
      "Epoch 941/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0158 - accuracy: 0.9917 - val_loss: 1.4497 - val_accuracy: 0.6333\n",
      "Epoch 942/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0384 - accuracy: 0.9917 - val_loss: 0.2914 - val_accuracy: 0.8667\n",
      "Epoch 943/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0170 - accuracy: 0.9917 - val_loss: 0.2847 - val_accuracy: 0.8667\n",
      "Epoch 944/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0187 - accuracy: 0.9917 - val_loss: 0.9990 - val_accuracy: 0.6333\n",
      "Epoch 945/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0178 - accuracy: 0.9833 - val_loss: 0.3518 - val_accuracy: 0.8667\n",
      "Epoch 946/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0178 - accuracy: 0.9833 - val_loss: 0.4814 - val_accuracy: 0.8333\n",
      "Epoch 947/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0199 - accuracy: 0.9917 - val_loss: 1.0145 - val_accuracy: 0.6333\n",
      "Epoch 948/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 0.9917 - val_loss: 0.3965 - val_accuracy: 0.8667\n",
      "Epoch 949/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0149 - accuracy: 0.9917 - val_loss: 0.4811 - val_accuracy: 0.9000\n",
      "Epoch 950/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0086 - accuracy: 0.9917 - val_loss: 1.2649 - val_accuracy: 0.7333\n",
      "Epoch 951/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0342 - accuracy: 0.9833 - val_loss: 0.6221 - val_accuracy: 0.7667\n",
      "Epoch 952/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0242 - accuracy: 0.9833 - val_loss: 1.1338 - val_accuracy: 0.6333\n",
      "Epoch 953/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.3551 - val_accuracy: 0.9333\n",
      "Epoch 954/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0198 - accuracy: 0.9833 - val_loss: 0.6677 - val_accuracy: 0.8667\n",
      "Epoch 955/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.6379 - val_accuracy: 0.8667\n",
      "Epoch 956/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0125 - accuracy: 0.9917 - val_loss: 1.1936 - val_accuracy: 0.7667\n",
      "Epoch 957/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9750 - val_loss: 0.0657 - val_accuracy: 0.9667\n",
      "Epoch 958/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.9750 - val_loss: 0.2983 - val_accuracy: 0.8667\n",
      "Epoch 959/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.2836 - val_accuracy: 0.9000\n",
      "Epoch 960/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0175 - accuracy: 0.9917 - val_loss: 0.4260 - val_accuracy: 0.8667\n",
      "Epoch 961/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 0.9917 - val_loss: 0.5484 - val_accuracy: 0.7667\n",
      "Epoch 962/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0150 - accuracy: 0.9917 - val_loss: 0.8530 - val_accuracy: 0.7667\n",
      "Epoch 963/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0193 - accuracy: 0.9917 - val_loss: 0.8439 - val_accuracy: 0.7667\n",
      "Epoch 964/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 0.9917 - val_loss: 0.7023 - val_accuracy: 0.7667\n",
      "Epoch 965/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0203 - accuracy: 0.9833 - val_loss: 0.6994 - val_accuracy: 0.7667\n",
      "Epoch 966/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0226 - accuracy: 0.9833 - val_loss: 0.9407 - val_accuracy: 0.7667\n",
      "Epoch 967/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 0.9917 - val_loss: 0.4104 - val_accuracy: 0.9000\n",
      "Epoch 968/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0156 - accuracy: 0.9917 - val_loss: 1.1941 - val_accuracy: 0.6333\n",
      "Epoch 969/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0179 - accuracy: 0.9833 - val_loss: 0.8658 - val_accuracy: 0.7667\n",
      "Epoch 970/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0173 - accuracy: 0.9917 - val_loss: 0.4441 - val_accuracy: 0.9333\n",
      "Epoch 971/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0136 - accuracy: 0.9917 - val_loss: 2.6504 - val_accuracy: 0.6333\n",
      "Epoch 972/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0378 - accuracy: 0.9833 - val_loss: 1.0417 - val_accuracy: 0.6667\n",
      "Epoch 973/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.6235 - val_accuracy: 0.8667\n",
      "Epoch 974/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 0.9917 - val_loss: 0.8735 - val_accuracy: 0.7667\n",
      "Epoch 975/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0173 - accuracy: 0.9917 - val_loss: 0.7626 - val_accuracy: 0.7667\n",
      "Epoch 976/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9917 - val_loss: 1.6703 - val_accuracy: 0.6333\n",
      "Epoch 977/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0118 - accuracy: 0.9917 - val_loss: 0.5454 - val_accuracy: 0.9000\n",
      "Epoch 978/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0155 - accuracy: 0.9917 - val_loss: 0.3951 - val_accuracy: 0.9333\n",
      "Epoch 979/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9667 - val_loss: 1.2980 - val_accuracy: 0.6333\n",
      "Epoch 980/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0212 - accuracy: 0.9917 - val_loss: 0.6099 - val_accuracy: 0.8000\n",
      "Epoch 981/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 0.9917 - val_loss: 0.5810 - val_accuracy: 0.8000\n",
      "Epoch 982/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 0.9917 - val_loss: 0.3781 - val_accuracy: 0.9000\n",
      "Epoch 983/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.7238 - val_accuracy: 0.6333\n",
      "Epoch 984/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.2253 - val_accuracy: 0.9000\n",
      "Epoch 985/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 0.9917 - val_loss: 0.5212 - val_accuracy: 0.7667\n",
      "Epoch 986/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.2275 - val_accuracy: 0.9000\n",
      "Epoch 987/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9750 - val_loss: 0.4528 - val_accuracy: 0.7667\n",
      "Epoch 988/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9917 - val_loss: 0.3850 - val_accuracy: 0.8667\n",
      "Epoch 989/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.4045 - val_accuracy: 0.8667\n",
      "Epoch 990/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 0.9917 - val_loss: 0.6404 - val_accuracy: 0.7667\n",
      "Epoch 991/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0159 - accuracy: 0.9917 - val_loss: 0.5246 - val_accuracy: 0.8000\n",
      "Epoch 992/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.3075 - val_accuracy: 0.9000\n",
      "Epoch 993/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0114 - accuracy: 0.9917 - val_loss: 0.6213 - val_accuracy: 0.7667\n",
      "Epoch 994/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.3787 - val_accuracy: 0.8667\n",
      "Epoch 995/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0205 - accuracy: 0.9833 - val_loss: 0.4848 - val_accuracy: 0.8667\n",
      "Epoch 996/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0138 - accuracy: 0.9833 - val_loss: 0.3697 - val_accuracy: 0.9000\n",
      "Epoch 997/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0086 - accuracy: 0.9917 - val_loss: 0.7919 - val_accuracy: 0.8667\n",
      "Epoch 998/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 0.9833 - val_loss: 0.1100 - val_accuracy: 0.9667\n",
      "Epoch 999/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9583 - val_loss: 0.1231 - val_accuracy: 0.9667\n",
      "Epoch 1000/1000\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 0.9917 - val_loss: 0.7212 - val_accuracy: 0.7667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2bb9241feb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model\n",
    "\n",
    "model.fit(data,target_new,epochs=1000,validation_split=0.2,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.xlabel('epoch #')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history.history['accuracy'])\n",
    "plt.xlabel('epoch #')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: FFNN_iris.model\\assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "\n",
    "model.save('FFNN_iris.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only weights & bias\n",
    "\n",
    "model.save_weights('FFNN_iris_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
